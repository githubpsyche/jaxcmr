{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97074909-54d5-4fe2-acf5-79b6ff7255f3",
   "metadata": {},
   "source": [
    "#  power_scale vs simple power demo\n",
    " This notebook demonstrates the `power_scale` function (using a log-sum-exp trick to normalize and exponentiate) compared to a simple power transformation. It also explores how `power_scale` behaves for large scale values, approaching a one-hot argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae6c87-cf73-4a10-b8c9-44c971885d2b",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011f09dd-900c-4e20-b8ac-1f795fdd2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "\n",
    "def power_scale(value: jnp.ndarray, scale: float) -> jnp.ndarray:\n",
    "    \"\"\"Returns value scaled by the exponent factor using logsumexp trick.\"\"\"\n",
    "    log_activation = jnp.log(value)\n",
    "    return lax.cond(\n",
    "        jnp.logical_and(jnp.any(value != 0), scale != 1),\n",
    "        lambda _: jnp.exp(scale * (log_activation - jnp.max(log_activation))),\n",
    "        lambda _: value,\n",
    "        None,\n",
    "    )\n",
    "\n",
    "def simple_power(value: jnp.ndarray, scale: float) -> jnp.ndarray:\n",
    "    \"\"\"Returns value raised to the specified power.\"\"\"\n",
    "    return value ** scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cfb00-8411-4228-b3d8-9c846d94299e",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f99ff9f-c08a-42e6-8990-1e66d62a4b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1. 2. 3. 4.]\n",
      "power_scale: [0.0625 0.25   0.5625 1.    ]\n",
      "simple_power: [ 1.  4.  9. 16.]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "vector = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "scale = 2.0\n",
    "\n",
    "ps = power_scale(vector, scale)\n",
    "sp = simple_power(vector, scale)\n",
    "\n",
    "print(\"Original:\", vector)\n",
    "print(\"power_scale:\", ps)\n",
    "print(\"simple_power:\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d910928-fbb2-4abd-9e4e-aac4768e7815",
   "metadata": {},
   "source": [
    "# Advantage over simple power\n",
    " The `power_scale` method ensures that the maximum entry is always scaled to 1 and preserves relative ratios under exponentiation, avoiding numerical underflow/overflow issues and preserving stable normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733cb71-d79a-4326-93ae-aa180dfc5f7e",
   "metadata": {},
   "source": [
    "# High-scale behavior and one-hot approximation\n",
    " As `scale` becomes very large, `power_scale` drives all entries except the maximum toward zero, effectively approximating a one-hot encoding at the position of the argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c22426-832c-4257-a645-3672df99eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale=1 -> [1. 2. 5. 3.]\n",
      "scale=5 -> [3.1999993e-04 1.0239999e-02 1.0000000e+00 7.7759996e-02]\n",
      "scale=20 -> [1.0485754e-14 1.0995110e-08 1.0000000e+00 3.6561578e-05]\n",
      "scale=100 -> [0.000000e+00 0.000000e+00 1.000000e+00 6.533167e-23]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "vector = jnp.array([1.0, 2.0, 5.0, 3.0])\n",
    "for s in [1, 5, 20, 100]:\n",
    "    ps = power_scale(vector, s)\n",
    "    print(f\"scale={s} ->\", ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-then-scale vs. Scale-then-mask\n",
    "This cell shows how the ordering of masking (zeroing out already-recalled items)\n",
    "and `power_scale` changes the final activations—especially with a large `scale`\n",
    "where numerical under-/overflow can bite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask → scale → norm : [0.0000000e+00 0.0000000e+00 9.9799335e-01 2.0064958e-03 7.4613226e-08]\n",
      "scale → mask → norm : [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Including the usual normalization step\n",
    "# After masking and/or scaling we typically renormalize the surviving activations\n",
    "# so they sum to 1. This makes the ordering difference even clearer.\n",
    "\n",
    "# %%\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def normalize(v):\n",
    "    s = jnp.sum(v)\n",
    "    return jnp.where(s == 0, v, v / s)\n",
    "\n",
    "def mask_then_scale_normalized(vals, recalled_mask, scale):\n",
    "    masked  = jnp.where(recalled_mask, 0.0, vals)\n",
    "    scaled  = power_scale(masked, scale)\n",
    "    return normalize(scaled)\n",
    "\n",
    "def scale_then_mask_normalized(vals, recalled_mask, scale):\n",
    "    scaled  = power_scale(vals, scale)\n",
    "    masked  = jnp.where(recalled_mask, 0.0, scaled)\n",
    "    return normalize(masked)\n",
    "\n",
    "# toy data\n",
    "activations = jnp.array([0.05, 1e8, 3.0, 2.8, 2.5])   # 1e8 is already-recalled\n",
    "recalled    = jnp.array([False, True, False, False, False])\n",
    "scale       = 90.0          # big enough to push exp() past the under-flow edge\n",
    "\n",
    "mts_norm = mask_then_scale_normalized(activations, recalled, scale)\n",
    "stm_norm = scale_then_mask_normalized(activations, recalled, scale)\n",
    "\n",
    "print(\"mask → scale → norm :\", mts_norm)\n",
    "print(\"scale → mask → norm :\", stm_norm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
