{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does Using Lax.Map instead of Jax.VMap Affect Performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm curious whether swapping from map to vmap will make my code more readable and/or performant.\n",
    "\n",
    "I currently use map in...\n",
    "\n",
    "- `predict_trials`, where I map `preidct_and_simulate_recalls` over each trial in `trials`. This is a major loss function for fitting my model.\n",
    "- `predict_transitions` calls `predict_trials`.\n",
    "- `present_and_predict_transitions` maps over present_lists and trials to call `present_and_predict_trial`.\n",
    "- `simulate_trials` maps over `rngs` to call `simulate_free_recall` for each trial.\n",
    "- `present_and_simulate_trials` adds `present_lists` to the mapping and calls `present_and_simulate_free_recall`.\n",
    "- `simulate_transitions` maps `simulate_free_recall_after_first` over `rngs` and `first_recalls`.\n",
    "- `present_and_simulate_transitions` does same as `simulate_transitions` but adds `present_lists` as a mapped argument.\n",
    "\n",
    "I'll focus my research on `predict_trials`. If I don't find a performance effect or an improvement to readability, I won't bother with the other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "# jax.config.update(\"jax_disable_jit\", True)\n",
    "from jax import numpy as jnp, lax\n",
    "from jaxcmr.helpers import load_data, generate_trial_mask\n",
    "from jaxcmr.typing import MemorySearch, MemorySearchCreateFn\n",
    "from jaxcmr.typing import Integer, Float, Array, Float_, Int_\n",
    "from typing import Mapping\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load and mask trials\n",
    "data_name = \"LohnasKahana2014\"\n",
    "data_path = f\"data/{data_name}.h5\"\n",
    "data = load_data(data_path)\n",
    "trial_query = 'data[\"list_type\"] == 1'\n",
    "trial_mask = generate_trial_mask(data, trial_query)  # type: ignore\n",
    "trials = data[\"recalls\"][trial_mask]\n",
    "max_list_length = trials.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/base_cmr_parameters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/base_cmr_parameters.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     fit_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m base_params \u001b[38;5;241m=\u001b[39m fit_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/jaxcmr/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/base_cmr_parameters.json'"
     ]
    }
   ],
   "source": [
    "with open(\"data/base_cmr_parameters.json\") as f:\n",
    "    fit_config = json.load(f)\n",
    "\n",
    "base_params = fit_config[\"fixed\"].copy()\n",
    "base_params['choice_sensitivity'] += 0.001\n",
    "bounds = fit_config[\"free\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CMR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m trial: predict_and_simulate_recalls(model, trial)[\u001b[38;5;241m1\u001b[39m], trials)\n\u001b[1;32m     20\u001b[0m _predict_trials \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(predict_trials, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 21\u001b[0m _predict_trials(\u001b[43mCMR\u001b[49m\u001b[38;5;241m.\u001b[39minit, max_list_length, trials, base_params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CMR' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_trials(\n",
    "    model_create_fn: MemorySearchCreateFn,\n",
    "    list_length: int,\n",
    "    trials: Integer[Array, \" trials recall_events\"],\n",
    "    parameters: Mapping[str, Float_],\n",
    ") -> Float[Array, \" trials recall_events\"]:\n",
    "    \"\"\"Return the simulation and outcome probabilities of multiple chains of retrieval events.\n",
    "\n",
    "    Args:\n",
    "        model_create_fn: constructor for a memory search model\n",
    "        list_length: the length of the study and recall sequences.\n",
    "        trials: the indices of the items to retrieve (1-indexed) or 0 to stop.\n",
    "        parameters: the model parameters.\n",
    "    \"\"\"\n",
    "    model = model_create_fn(list_length, parameters)\n",
    "    model = lax.fori_loop(1, list_length + 1, lambda i, m: m.experience(i), model)\n",
    "    model = model.start_retrieving()\n",
    "    return lax.map(lambda trial: predict_and_simulate_recalls(model, trial)[1], trials)\n",
    "\n",
    "_predict_trials = jax.jit(predict_trials, static_argnums=(0, 1))\n",
    "_predict_trials(CMR.init, max_list_length, trials, base_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.96 ms ± 93.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _predict_trials(CMR.init, max_list_length, trials, base_params).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.03062369, 0.20781308, 0.01182406, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.08700962, 0.21383993, 0.0047741 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.62229365, 0.6549627 , 0.42818728, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.62229365, 0.6549627 , 0.42818728, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.0830027 , 0.0185994 , 0.07063522, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.08700962, 0.48712012, 0.06968244, ..., 1.        , 1.        ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_trials(\n",
    "    model_create_fn: MemorySearchCreateFn,\n",
    "    list_length: int,\n",
    "    trials: Integer[Array, \" trials recall_events\"],\n",
    "    parameters: Mapping[str, Float_],\n",
    ") -> Float[Array, \" trials recall_events\"]:\n",
    "    \"\"\"Return the simulation and outcome probabilities of multiple chains of retrieval events.\n",
    "\n",
    "    Args:\n",
    "        model_create_fn: constructor for a memory search model\n",
    "        list_length: the length of the study and recall sequences.\n",
    "        trials: the indices of the items to retrieve (1-indexed) or 0 to stop.\n",
    "        parameters: the model parameters.\n",
    "    \"\"\"\n",
    "    model = model_create_fn(list_length, parameters)\n",
    "    model = lax.fori_loop(1, list_length + 1, lambda i, m: m.experience(i), model)\n",
    "    model = model.start_retrieving()\n",
    "    # return lax.map(lambda trial: predict_and_simulate_recalls(model, trial)[1], trials)\n",
    "    return jax.vmap(lambda trial: predict_and_simulate_recalls(model, trial)[1])(trials)\n",
    "\n",
    "_predict_trials = jax.jit(predict_trials, static_argnums=(0, 1))\n",
    "_predict_trials(CMR.init, max_list_length, trials, base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 ms ± 72.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _predict_trials(CMR.init, max_list_length, trials, base_params).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of mysterious. The vmap version is several times slower than the map version. I'm not sure why. I'll try to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I currently use vmap in my fitting code and in wrappers around my simulation functions. It's possible that I'm losing out on performance by using vmap in these places. I'll try to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Test: What about inside our fitting code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our fitting code, we currently dynamically parametrize and compress the output of `predict_trials` using `vmap` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def predict_trials(\n",
    "    model_create_fn: MemorySearchCreateFn,\n",
    "    list_length: int,\n",
    "    trials: Integer[Array, \" trials recall_events\"],\n",
    "    parameters: Mapping[str, Float_],\n",
    ") -> Float[Array, \" trials recall_events\"]:\n",
    "    \"\"\"Return the simulation and outcome probabilities of multiple chains of retrieval events.\n",
    "\n",
    "    Args:\n",
    "        model_create_fn: constructor for a memory search model\n",
    "        list_length: the length of the study and recall sequences.\n",
    "        trials: the indices of the items to retrieve (1-indexed) or 0 to stop.\n",
    "        parameters: the model parameters.\n",
    "    \"\"\"\n",
    "    model = model_create_fn(list_length, parameters)\n",
    "    model = lax.fori_loop(1, list_length + 1, lambda i, m: m.experience(i), model)\n",
    "    model = model.start_retrieving()\n",
    "    return lax.map(lambda trial: predict_and_simulate_recalls(model, trial)[1], trials)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(x):\n",
    "    params = {\n",
    "        key: x[key_index] for key_index, key in enumerate(bounds)\n",
    "    }\n",
    "    return log_likelihood(\n",
    "        predict_trials(\n",
    "            CMR.init,\n",
    "            max_list_length,\n",
    "            trials,\n",
    "            {**base_params, **params},\n",
    "        )\n",
    "    )\n",
    "\n",
    "mapped_loss_fn = jax.jit(jax.vmap(loss_fn, in_axes=(-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeat the jit compilation here because I sometimes call loss_fn directly in the notebook. This doesn't seem to affect performance even marginally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([32965.54, 32965.54, 32965.54, 32965.54, 32965.54, 32965.54,\n",
       "       32965.54, 32965.54, 32965.54, 32965.54], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.repeat(x[None, :], 10, axis=0).T\n",
    "mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 ms ± 509 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's target a version that avoids vmap..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([32965.54, 32965.54, 32965.54, 32965.54, 32965.54, 32965.54,\n",
       "       32965.54, 32965.54, 32965.54, 32965.54], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @jax.jit\n",
    "def loss_fn(x):\n",
    "    params = {\n",
    "        key: x[key_index] for key_index, key in enumerate(bounds)\n",
    "    }\n",
    "    return log_likelihood(\n",
    "        predict_trials(\n",
    "            CMR.init,\n",
    "            max_list_length,\n",
    "            trials,\n",
    "            {**base_params, **params},\n",
    "        )\n",
    "    )\n",
    "\n",
    "@jax.jit\n",
    "def mapped_loss_fn(x):\n",
    "    # use lax.map instead of vmap\n",
    "    return lax.map(loss_fn, x)\n",
    "\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.repeat(x[None, :], 10, axis=0)\n",
    "mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.8 ms ± 163 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, the mappped version is faster than the vmap version. Why didn't I catch this sooner?\n",
    "But I need it to work with the transposed x. For now, I'll just transpose outside the mapped function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([32965.54, 32965.54, 32965.54, 32965.54, 32965.54, 32965.54,\n",
       "       32965.54, 32965.54, 32965.54, 32965.54], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def mapped_loss_fn(x):\n",
    "    # use lax.map instead of vmap\n",
    "    return lax.map(loss_fn, x.T)\n",
    "\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.repeat(x[None, :], 10, axis=0).T\n",
    "mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.8 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds a couple milliseconds to the runtime. I should try to avoid this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize by Extracting Parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I optimize further? \n",
    "One possibility is that I could configure the parameters outside the mapped function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding_drift_rate': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'delay_drift_rate': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'start_drift_rate': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'recall_drift_rate': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'shared_support': [2.220446049250313e-16, 100.0],\n",
       " 'item_support': [2.220446049250313e-16, 100.0],\n",
       " 'learning_rate': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'primacy_scale': [2.220446049250313e-16, 100.0],\n",
       " 'primacy_decay': [2.220446049250313e-16, 100.0],\n",
       " 'stop_probability_scale': [2.220446049250313e-16, 0.9999999999999998],\n",
       " 'stop_probability_growth': [2.220446049250313e-16, 10.0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([32965.54, 32965.54, 32965.54, 32965.54, 32965.54, 32965.54,\n",
       "       32965.54, 32965.54, 32965.54, 32965.54], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(params):\n",
    "    return log_likelihood(\n",
    "        predict_trials(\n",
    "            CMR.init,\n",
    "            max_list_length,\n",
    "            trials,\n",
    "            {**base_params, **params},\n",
    "        )\n",
    "    )\n",
    "\n",
    "@jax.jit\n",
    "def mapped_loss_fn(x):\n",
    "    params = {\n",
    "        key: x[key_index] for key_index, key in enumerate(bounds)\n",
    "    }\n",
    "    return lax.map(loss_fn, params)\n",
    "\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.repeat(x[None, :], 10, axis=0).T\n",
    "\n",
    "mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.5 ms ± 608 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This beats my previous best time somewhat without neglecting the transpose. Can we go further? I could try to avoid creating the dictionary in the mapped function.\n",
    "\n",
    "I would need base_params to be an array with repeated values across the batch dimension. I can do this during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding_drift_rate': Array([0.80163276, 0.80163276, 0.80163276, 0.80163276, 0.80163276,\n",
       "        0.80163276, 0.80163276, 0.80163276, 0.80163276, 0.80163276],      dtype=float32),\n",
       " 'delay_drift_rate': Array([0.99664116, 0.99664116, 0.99664116, 0.99664116, 0.99664116,\n",
       "        0.99664116, 0.99664116, 0.99664116, 0.99664116, 0.99664116],      dtype=float32),\n",
       " 'start_drift_rate': Array([0.05112313, 0.05112313, 0.05112313, 0.05112313, 0.05112313,\n",
       "        0.05112313, 0.05112313, 0.05112313, 0.05112313, 0.05112313],      dtype=float32),\n",
       " 'recall_drift_rate': Array([0.8666706, 0.8666706, 0.8666706, 0.8666706, 0.8666706, 0.8666706,\n",
       "        0.8666706, 0.8666706, 0.8666706, 0.8666706], dtype=float32),\n",
       " 'shared_support': Array([0.01612209, 0.01612209, 0.01612209, 0.01612209, 0.01612209,\n",
       "        0.01612209, 0.01612209, 0.01612209, 0.01612209, 0.01612209],      dtype=float32),\n",
       " 'item_support': Array([0.8877853, 0.8877853, 0.8877853, 0.8877853, 0.8877853, 0.8877853,\n",
       "        0.8877853, 0.8877853, 0.8877853, 0.8877853], dtype=float32),\n",
       " 'learning_rate': Array([0.10455606, 0.10455606, 0.10455606, 0.10455606, 0.10455606,\n",
       "        0.10455606, 0.10455606, 0.10455606, 0.10455606, 0.10455606],      dtype=float32),\n",
       " 'primacy_scale': Array([33.57092, 33.57092, 33.57092, 33.57092, 33.57092, 33.57092,\n",
       "        33.57092, 33.57092, 33.57092, 33.57092], dtype=float32),\n",
       " 'primacy_decay': Array([1.5709189, 1.5709189, 1.5709189, 1.5709189, 1.5709189, 1.5709189,\n",
       "        1.5709189, 1.5709189, 1.5709189, 1.5709189], dtype=float32),\n",
       " 'stop_probability_scale': Array([0.003449, 0.003449, 0.003449, 0.003449, 0.003449, 0.003449,\n",
       "        0.003449, 0.003449, 0.003449, 0.003449], dtype=float32),\n",
       " 'stop_probability_growth': Array([0.377978, 0.377978, 0.377978, 0.377978, 0.377978, 0.377978,\n",
       "        0.377978, 0.377978, 0.377978, 0.377978], dtype=float32),\n",
       " 'choice_sensitivity': Array([1.001, 1.001, 1.001, 1.001, 1.001, 1.001, 1.001, 1.001, 1.001,\n",
       "        1.001], dtype=float32),\n",
       " 'mcf_trace_sensitivity': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " 'mfc_trace_sensitivity': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " 'mfc_choice_sensitivity': Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " 'semantic_scale': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'semantic_choice_sensitivity': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_base_params = {key: jnp.array([base_params[key]] * 10) for key in base_params}\n",
    "batch_base_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([32965.54, 32965.54, 32965.54, 32965.54, 32965.54, 32965.54,\n",
       "       32965.54, 32965.54, 32965.54, 32965.54], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(params):\n",
    "    return log_likelihood(\n",
    "        predict_trials(\n",
    "            CMR.init,\n",
    "            max_list_length,\n",
    "            trials,\n",
    "            params,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def mapped_loss_fn(x):\n",
    "    params = {\n",
    "        key: x[key_index] if key in bounds else batch_base_params[key]\n",
    "        for key_index, key in enumerate(batch_base_params)\n",
    "    }\n",
    "    return lax.map(loss_fn, params)\n",
    "\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.array([base_params[key] for key in bounds])\n",
    "x = jnp.repeat(x[None, :], 10, axis=0).T\n",
    "\n",
    "mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.4 ms ± 654 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mapped_loss_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to help. Maybe because the `batch_base_params` object is bigger? Anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last thing I want to check is whether log-likelihood really needs to apply log before sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.03062369, 0.20781308, 0.01182406, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.08700962, 0.21383993, 0.0047741 , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.62229365, 0.6549627 , 0.42818728, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.62229365, 0.6549627 , 0.42818728, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.0830027 , 0.0185994 , 0.07063522, ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.08700962, 0.48712012, 0.06968244, ..., 1.        , 1.        ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihoods = _predict_trials(CMR.init, max_list_length, trials, base_params)\n",
    "\n",
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-32965.54, dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.sum(jnp.log(likelihoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(9.315411, dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.log(jnp.sum(likelihoods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kay, it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results\n",
    "Let's confirm I can improve fits by using map instead of vmap by actually updating my fitting code and re-running the fitting process.\n",
    "\n",
    "Current fits return an average fitness of 590.41 with a runtime of 7:27 under .001 tolerance.\n",
    "\n",
    "New fits return an average fitness of 590.41 with a runtime of 4:50 under .001 tolerance.\n",
    "\n",
    "So I saved 2:37 by switching to map. Not bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
