{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.DISABLE_JIT = True\n",
    "from numba import int32, njit\n",
    "from numba.experimental import jitclass\n",
    "from typing import Type\n",
    "import numpy as np\n",
    "from psifr.stats import percentile_rank\n",
    "from compmempy.helpers.transforming_arrays import njit_apply_along_axis\n",
    "from compmempy.helpers.handling_data import item_to_study_positions, apply_by_subject\n",
    "from compmempy.helpers.loading_data import to_numba_typed_dict\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepetitionLagCRP:\n",
    "    def __init__(\n",
    "        self, presentations: np.ndarray, max_repetitions: int = 2, min_lag: int = 4\n",
    "    ):\n",
    "        \"Pre-allocate arrays for lag-CRP tabulations.\"\n",
    "        list_length = np.max(np.sum(presentations != 0, axis=1))\n",
    "        self.lag_range = list_length - 1\n",
    "        self.min_lag = min_lag\n",
    "        self.max_repetitions = max_repetitions\n",
    "        self.actual_lag_transitions = np.zeros(\n",
    "            (max_repetitions, self.lag_range * 2 + 1), dtype=np.int32\n",
    "        )\n",
    "        self.possible_lag_transitions = np.zeros(\n",
    "            (max_repetitions, self.lag_range * 2 + 1), dtype=np.int32\n",
    "        )\n",
    "\n",
    "    def should_tabulate(\n",
    "        self,\n",
    "        prev_study_positions: np.ndarray,\n",
    "    ) -> bool:\n",
    "        \"\"\"Only consider transitions from item with at least two spaced-out study positions\"\"\"\n",
    "        return len(prev_study_positions) > 1 and prev_study_positions[\n",
    "            -1\n",
    "        ] - prev_study_positions[-2] >= (self.min_lag + 1)\n",
    "\n",
    "    def tabulate_lags(\n",
    "        self,\n",
    "        previous_item: int,\n",
    "        current_item: int,\n",
    "        possible_items: np.ndarray,\n",
    "        item_study_positions: list[np.ndarray],\n",
    "    ):\n",
    "        \"Tabulate actual and possible serial lags of current from previous item.\"\n",
    "\n",
    "        possible_lags = np.zeros(\n",
    "            (self.max_repetitions, self.lag_range * 2 + 1), dtype=np.bool_\n",
    "        )\n",
    "        actual_lags = np.zeros(\n",
    "            (self.max_repetitions, self.lag_range * 2 + 1), dtype=np.bool_\n",
    "        )\n",
    "\n",
    "        prev_study_positions = item_study_positions[previous_item - 1]\n",
    "        current_study_positions = item_study_positions[current_item - 1]\n",
    "        for repetition_index, prev_study_position in enumerate(prev_study_positions):\n",
    "            for current_study_position in current_study_positions:\n",
    "                lag = current_study_position - prev_study_position\n",
    "                actual_lags[repetition_index, lag + self.lag_range] = True\n",
    "\n",
    "            for item in possible_items:\n",
    "                possible_study_positions = item_study_positions[item - 1]\n",
    "                for possible_study_position in possible_study_positions:\n",
    "                    lag = possible_study_position - prev_study_position\n",
    "                    possible_lags[repetition_index, lag + self.lag_range] = True\n",
    "\n",
    "        self.actual_lag_transitions += actual_lags\n",
    "        self.possible_lag_transitions += possible_lags\n",
    "\n",
    "    def tabulate_over_transitions(\n",
    "        self, trials: np.ndarray, presentations: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"Tabulate actual and possible lag transitions over a set of trials\"\n",
    "\n",
    "        terminus = np.sum(trials != 0, axis=1)\n",
    "\n",
    "        for trial_index in range(len(trials)):\n",
    "            presentation = presentations[trial_index]\n",
    "            item_count = np.max(presentation)\n",
    "            possible_items = np.arange(item_count) + 1\n",
    "            item_study_positions = njit_apply_along_axis(\n",
    "                item_to_study_positions, possible_items, presentation\n",
    "            )\n",
    "            previous_item = 0\n",
    "\n",
    "            for recall_index in range(terminus[trial_index]):\n",
    "                current_item = presentation[trials[trial_index, recall_index] - 1]\n",
    "                if recall_index > 0 and self.should_tabulate(\n",
    "                    item_study_positions[previous_item - 1],\n",
    "                ):\n",
    "                    self.tabulate_lags(\n",
    "                        previous_item,\n",
    "                        current_item,\n",
    "                        possible_items,\n",
    "                        item_study_positions,\n",
    "                    )\n",
    "\n",
    "                previous_item = current_item\n",
    "                possible_items = possible_items[possible_items != previous_item]\n",
    "\n",
    "        return self.actual_lag_transitions / self.possible_lag_transitions\n",
    "\n",
    "\n",
    "repetition_crp_spec = [\n",
    "    (\"lag_range\", int32),\n",
    "    (\"max_repetitions\", int32),\n",
    "    (\"min_lag\", int32),\n",
    "    (\"actual_lag_transitions\", int32[:, ::1]),\n",
    "    (\"possible_lag_transitions\", int32[:, ::1]),\n",
    "]\n",
    "\n",
    "numba_RepetitionLagCRP: Type[RepetitionLagCRP] = jitclass(repetition_crp_spec)(\n",
    "    RepetitionLagCRP\n",
    ")  # type: ignore\n",
    "\n",
    "\n",
    "@njit\n",
    "def repcrp(\n",
    "    trials: np.ndarray,\n",
    "    presentations: np.ndarray,\n",
    "    list_length: int,\n",
    "    max_repetitions: int = 2,\n",
    "    min_lag: int = 4,\n",
    ") -> np.ndarray:\n",
    "    \"Apply the lag-CRP to a set of trials where each item has a single serial position.\"\n",
    "    return numba_RepetitionLagCRP(\n",
    "        presentations, max_repetitions, min_lag\n",
    "    ).tabulate_over_transitions(trials, presentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"LohnasKahana2014\"\n",
    "data_path = \"data/LohnasKahana2014.h5\"\n",
    "data_query = \"data['list_type'] >= 3\"\n",
    "\n",
    "data = to_numba_typed_dict(\n",
    "    {key: np.array(value) for key, value in load_data(data_path).items()}\n",
    ")\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "list_length = data[\"listLength\"][0, 0]\n",
    "\n",
    "max_repetitions = 2\n",
    "min_lag = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/m1dxq2r12b77t0c09mty9kn00000gn/T/ipykernel_64204/1954947309.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.actual_lag_transitions / self.possible_lag_transitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[       nan,        nan,        nan, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [       nan,        nan,        nan, ...,        nan,\n",
       "                nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan,        nan, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.25      , 0.        , ...,        nan,\n",
       "                nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan,        nan, ..., 0.        ,\n",
       "                nan,        nan],\n",
       "        [0.        , 0.        , 0.        , ...,        nan,\n",
       "                nan,        nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[       nan,        nan,        nan, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.16666667, 0.        , ...,        nan,\n",
       "                nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan,        nan, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ...,        nan,\n",
       "                nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan,        nan, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.66666667, ...,        nan,\n",
       "                nan,        nan]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(\n",
    "    apply_by_subject(\n",
    "        data,\n",
    "        trial_mask,\n",
    "        repcrp,\n",
    "        max_repetitions,\n",
    "        min_lag,\n",
    "    )\n",
    ")#[:, :, list_length]\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 5.15935420116019\n",
      "Two-tailed P-value: 1.0671446175294915e-05\n",
      "There is a statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.24652416043918732\n",
      "Mean Second Sample: 0.14889906025083952\n",
      "Standard Error First Sample: 0.020744339470079483\n",
      "Standard Error Second Sample: 0.007618757463565077\n",
      "Mean Difference: 0.09762510018834776\n",
      "Standard Error Difference: 0.01864968860435012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Ensure the data has exactly two columns\n",
    "if data.shape[1] != 2:\n",
    "    raise ValueError(\"Data must have exactly two columns\")\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 1]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"Two-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\n",
    "        \"There is a statistically significant difference between the two paired samples at the 5% significance level.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"There is no statistically significant difference between the two paired samples at the 5% significance level.\"\n",
    "    )\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of CMR Fitted to Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| | | Cond34LohnasKahana2014 BaseCMR Model Fitting |\n",
       "|---|---|---|\n",
       "| fitness | mean | 817.31 +/- 70.82 |\n",
       "| | std | 203.20 |\n",
       "| primacy scale | mean | 11.76 +/- 8.89 |\n",
       "| | std | 25.52 |\n",
       "| mfc choice sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| semantic scale | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| shared support | mean | 8.95 +/- 5.74 |\n",
       "| | std | 16.47 |\n",
       "| semantic choice sensitivity | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| choice sensitivity | mean | 29.08 +/- 12.52 |\n",
       "| | std | 35.92 |\n",
       "| primacy decay | mean | 20.74 +/- 10.79 |\n",
       "| | std | 30.97 |\n",
       "| stop probability scale | mean | 0.02 +/- 0.01 |\n",
       "| | std | 0.02 |\n",
       "| stop probability growth | mean | 0.23 +/- 0.03 |\n",
       "| | std | 0.10 |\n",
       "| mfc trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| learning rate | mean | 0.47 +/- 0.08 |\n",
       "| | std | 0.23 |\n",
       "| mcf trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| item support | mean | 14.62 +/- 7.60 |\n",
       "| | std | 21.82 |\n",
       "| encoding drift rate | mean | 0.71 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| recall drift rate | mean | 0.91 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| start drift rate | mean | 0.61 +/- 0.11 |\n",
       "| | std | 0.32 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jaxcmr_research.helpers.hdf5 import simulate_h5_from_h5\n",
    "from jax import random\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data\n",
    "from jaxcmr_research.helpers.misc import summarize_parameters, import_from_string\n",
    "import numpy as np\n",
    "from jaxcmr_research.helpers.array import compute_similarity_matrix\n",
    "from jax import numpy as jnp\n",
    "import json\n",
    "from IPython.display import Markdown  # type: ignore\n",
    "\n",
    "data_name = \"Cond34LohnasKahana2014\"\n",
    "data_path = \"data/LohnasKahana2014.h5\"\n",
    "data_query = \"data['list_type'] >= 3\"\n",
    "connection_path = \"data/peers-all-mpnet-base-v2.npy\"\n",
    "experiment_count = 100\n",
    "seed = 0\n",
    "fit_result_path = (\n",
    "    \"notebooks/Model_Fitting/Cond34LohnasKahana2014_BaseCMR_Model_Fitting.json\"\n",
    ")\n",
    "\n",
    "\n",
    "data = load_data(data_path)\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "embeddings = np.load(connection_path)\n",
    "connections = compute_similarity_matrix(embeddings)  # unused here\n",
    "model_factory_path = \"jaxcmr_research.cmr.BaseCMRFactory\"\n",
    "model_factory = import_from_string(model_factory_path)\n",
    "with open(fit_result_path, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    if \"subject\" not in results[\"fits\"]:\n",
    "        results[\"fits\"][\"subject\"] = results[\"subject\"]\n",
    "\n",
    "\n",
    "Markdown(summarize_parameters([results], None, include_std=True, include_ci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(seed)\n",
    "rng, rng_iter = random.split(rng)\n",
    "sim = simulate_h5_from_h5(\n",
    "    model_factory=model_factory,\n",
    "    dataset=data,\n",
    "    connections=connections,\n",
    "    parameters={key: jnp.array(val) for key, val in results[\"fits\"].items()},\n",
    "    trial_mask=trial_mask,\n",
    "    experiment_count=experiment_count,\n",
    "    rng=rng_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/m1dxq2r12b77t0c09mty9kn00000gn/T/ipykernel_64204/1954947309.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.actual_lag_transitions / self.possible_lag_transitions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.37691602, 0.24585006],\n",
       "       [0.20507914, 0.15685939],\n",
       "       [0.2324383 , 0.2078476 ],\n",
       "       [0.14615893, 0.12893448],\n",
       "       [0.1418526 , 0.12987667],\n",
       "       [0.26139264, 0.23840021],\n",
       "       [0.13619954, 0.12609457],\n",
       "       [0.19366251, 0.18171823],\n",
       "       [0.25613003, 0.21843415],\n",
       "       [0.1783601 , 0.18394196],\n",
       "       [0.38746607, 0.25781398],\n",
       "       [0.1863354 , 0.16498757],\n",
       "       [0.19351464, 0.16460851],\n",
       "       [0.30263329, 0.22005194],\n",
       "       [0.17774365, 0.14188103],\n",
       "       [0.29895324, 0.2251898 ],\n",
       "       [0.34814036, 0.2730383 ],\n",
       "       [0.28106852, 0.23035675],\n",
       "       [0.13874346, 0.131178  ],\n",
       "       [0.16557792, 0.15006693],\n",
       "       [0.23768206, 0.20293017],\n",
       "       [0.16209428, 0.17712692],\n",
       "       [0.05846103, 0.06214786],\n",
       "       [0.23163217, 0.19284526],\n",
       "       [0.14168378, 0.12849003],\n",
       "       [0.11863967, 0.10735954],\n",
       "       [0.37306226, 0.26053499],\n",
       "       [0.13778931, 0.14711447],\n",
       "       [0.13638824, 0.13370119],\n",
       "       [0.25534163, 0.23407895],\n",
       "       [0.13148372, 0.10448708],\n",
       "       [0.25597184, 0.23151674],\n",
       "       [0.20300648, 0.18503302],\n",
       "       [0.12683859, 0.11615066],\n",
       "       [0.14312977, 0.14549045]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(\n",
    "    apply_by_subject(\n",
    "        to_numba_typed_dict({key: np.array(value) for key, value in sim.items()}),\n",
    "        generate_trial_mask(sim, data_query),\n",
    "        repcrp,\n",
    "        max_repetitions,\n",
    "        min_lag,\n",
    "    )\n",
    ")[:, :, list_length]\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 5.147517335829471\n",
      "Two-tailed P-value: 1.1055947734682667e-05\n",
      "There is a statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.2091877481178868\n",
      "Mean Second Sample: 0.17731821360867364\n",
      "Standard Error First Sample: 0.0136460584415815\n",
      "Standard Error Second Sample: 0.008723339539187561\n",
      "Mean Difference: 0.03186953450921316\n",
      "Standard Error Difference: 0.006102156456909528\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Ensure the data has exactly two columns\n",
    "if data.shape[1] != 2:\n",
    "    raise ValueError(\"Data must have exactly two columns\")\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 1]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"Two-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\n",
    "        \"There is a statistically significant difference between the two paired samples at the 5% significance level.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"There is no statistically significant difference between the two paired samples at the 5% significance level.\"\n",
    "    )\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxcmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
