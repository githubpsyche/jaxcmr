Repetition both strengthens memories and links them to an evolving temporal context that reflects recent experience.
According to retrieved-context theory (RCT), this evolution is item-based: encountering an item reactivates and blends contextual features already linked to it into the ongoing state. 
Consequently, when an item repeats, its earlier contexts reactivate and blend with the present, linking encounters through overlapping features.

The Context Maintenance and Retrieval (CMR) model implements this account and captures many memory benchmarks, yet tests of its repetition mechanism remain limited and use imperfect controls.
We re-examined published free- and serial-recall datasets with item repetitions, matching repetition lists to controls on study positions.
Transitions between items neighboring different occurrences of the same word showed no boost, and transitions after a repeated word were biased toward neighbors of its first occurrence.
In serial recall, participants reliably advanced from repeated words to their correct next neighbor rather than to another occurrence's neighbor.

Likelihood-based fits show that by blending the reinstated contexts of both presentations, study‑phase retrieval in CMR produces associative interference: cues for either occurrence activate both list regions. 
That interference predicts balanced cross-occurrence transitions -- opposite the selective first‑neighbor bias, baseline cross‑neighbor rates, and consistent forward chaining we observed.
Assigning distinct episode‑specific contexts and allowing them to compete for reinstatement during retrieval captured all effects and improved fits.
These findings challenge a core RCT assumption and suggest that distinctive, non-overlapping contexts configure repetition memory.

---

Repetition both strengthens memories and links them to an evolving temporal context that reflects recent experience.
According to retrieved-context theory (RCT), this evolution is item-based: encountering an item reactivates and blends its earlier contexts with the present, linking encounters through overlapping features.
The Context Maintenance and Retrieval (CMR) model implements this account and captures many memory benchmarks, yet tests of its repetition mechanism remain limited in scope and use imperfect controls.

We re-analysed six free- and serial-recall datasets, using position-matched controls where applicable, and identified three new empirical constraints that models must satisfy:
(i) no surplus transitions between neighbors of different occurrences,
(ii) a selective bias toward first-occurrence neighbors after retrieving a repeated item in free recall, and
(iii) preserved forward chaining in serial recall.
Standard CMR fails all three because study‑phase retrieval blends contexts, producing associative interference that should yield balanced cross‑occurrence transitions.

We introduce Instance‑CMR, an instance‑based implementation of RCT that stores distinct episode traces and lets them compete for reinstatement.
With no extra free parameters, Instance‑CMR captures the full benchmark set and the three new constraints, improving sequence likelihood fits in every dataset.
These findings overturn the “context blending” assumption and show that repetition memory is configured by episode‑specific, non‑overlapping contexts rather than a single composite trace.