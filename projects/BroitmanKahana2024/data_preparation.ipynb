{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8231297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved EMBAM dataset with 2604 trials to 'combined_subjects.h5'\n",
      "  • subject: (2604, 1)\n",
      "  • session: (2604, 1)\n",
      "  • listLength: (2604, 1)\n",
      "  • pres_itemnos: (2604, 18)\n",
      "  • recalls: (2604, 12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# === Configuration toggles ===\n",
    "BIDS_ROOT = \".\"                   # root of your BIDS dataset\n",
    "OUTPUT_FILE = \"combined_subjects.h5\"\n",
    "FILTER_6_SESSION_SUBJECTS = False  # only include subjects with ≥ 6 beh sessions\n",
    "DROP_INTRUSIONS = True            # skip recalls of items not in study list\n",
    "DROP_REPEATED_RECALLS = True      # skip repeated recalls within trial\n",
    "\n",
    "# === Helper functions ===\n",
    "\n",
    "def process_beh_file(\n",
    "    tsv_path: str,\n",
    "    subject_idx: int,\n",
    "    session_idx: int\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Read one BIDS-beh.tsv and return per-trial dicts:\n",
    "      {\n",
    "        'subject': int,\n",
    "        'session': int,\n",
    "        'studied_items': List[str],\n",
    "        'recalled_items': List[str]\n",
    "      }\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    starts = df.index[df['trial_type'] == 'TRIAL_START'].tolist()\n",
    "    if not starts:\n",
    "        raise ValueError(f\"No TRIAL_START in {tsv_path}\")\n",
    "    \n",
    "    trials = []\n",
    "    for i, st in enumerate(starts):\n",
    "        ed = starts[i+1] if i+1 < len(starts) else len(df)\n",
    "        tdf = df.iloc[st:ed]\n",
    "        # 1) STUDY list\n",
    "        study = tdf[tdf['trial_type']==\"WORD\"].sort_values('serialpos')\n",
    "        if study.empty:\n",
    "            continue\n",
    "        studied_items = study['item_name'].astype(str).tolist()\n",
    "        # 2) RAW recall\n",
    "        rec = tdf[tdf['trial_type']==\"REC_WORD\"]\n",
    "        raw_recalled = rec['item_name'].astype(str).tolist()\n",
    "        # 3) Filter intrusions & repeats\n",
    "        seen = set()\n",
    "        recalled_items = []\n",
    "        for itm in raw_recalled:\n",
    "            if DROP_INTRUSIONS and itm not in studied_items:\n",
    "                continue\n",
    "            if DROP_REPEATED_RECALLS and itm in seen:\n",
    "                continue\n",
    "            seen.add(itm)\n",
    "            recalled_items.append(itm)\n",
    "        trials.append({\n",
    "            'subject':       subject_idx,\n",
    "            'session':       session_idx,\n",
    "            'studied_items': studied_items,\n",
    "            'recalled_items':recalled_items\n",
    "        })\n",
    "    return trials\n",
    "\n",
    "def encode_pres_itemnos(studied_items: List[str]) -> List[int]:\n",
    "    \"\"\"1-based within-list indices, reuse on repeats.\"\"\"\n",
    "    first_idx = {}\n",
    "    out = []\n",
    "    ctr = 0\n",
    "    for itm in studied_items:\n",
    "        if itm not in first_idx:\n",
    "            ctr += 1\n",
    "            first_idx[itm] = ctr\n",
    "        out.append(first_idx[itm])\n",
    "    return out\n",
    "\n",
    "def encode_recalls(\n",
    "    studied_items: List[str],\n",
    "    recalled_items: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\"Map recalls to first study-position (1-based).\"\"\"\n",
    "    out = []\n",
    "    for itm in recalled_items:\n",
    "        try:\n",
    "            pos = studied_items.index(itm) + 1\n",
    "            out.append(pos)\n",
    "        except ValueError:\n",
    "            # intrusion not in list, skip\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# === 1) Discover all beh.tsv paths ===\n",
    "pattern = os.path.join(BIDS_ROOT, \"sub-*\", \"ses-*\", \"beh\", \"*_beh.tsv\")\n",
    "all_paths = sorted(glob.glob(pattern))\n",
    "\n",
    "# === 2) Group by subject & session ===\n",
    "subjects = {}\n",
    "for p in all_paths:\n",
    "    parts = p.split(os.sep)\n",
    "    subj = parts[1].replace(\"sub-\", \"\")\n",
    "    ses  = int(parts[2].replace(\"ses-\", \"\"))\n",
    "    subjects.setdefault(subj, []).append((ses, p))\n",
    "\n",
    "# === 3) Optionally filter to subjects with ≥6 sessions ===\n",
    "if FILTER_6_SESSION_SUBJECTS:\n",
    "    subjects = {s: svr for s, svr in subjects.items() if len(svr) >= 6}\n",
    "\n",
    "# === 4) Process every file ===\n",
    "all_trials: List[Dict[str,Any]] = []\n",
    "for sub_i, (subj_label, sess_list) in enumerate(subjects.items(), start=1):\n",
    "    for ses_i, path in sorted(sess_list, key=lambda x: x[0]):\n",
    "        all_trials.extend(process_beh_file(path, sub_i, ses_i))\n",
    "\n",
    "# === 5) Determine max list & recall lengths ===\n",
    "max_study   = max(len(t['studied_items']) for t in all_trials)\n",
    "max_recall  = max(len(t['recalled_items']) for t in all_trials)\n",
    "N_trials    = len(all_trials)\n",
    "\n",
    "# === 6) Allocate arrays ===\n",
    "subject_arr    = np.zeros((N_trials, 1), dtype=int)\n",
    "session_arr    = np.zeros((N_trials, 1), dtype=int)\n",
    "listLength_arr = np.zeros((N_trials, 1), dtype=int)\n",
    "pres_itemnos   = np.zeros((N_trials, max_study), dtype=int)\n",
    "recalls_arr    = np.zeros((N_trials, max_recall), dtype=int)\n",
    "\n",
    "# === 7) Fill arrays ===\n",
    "for i, trial in enumerate(all_trials):\n",
    "    study   = trial['studied_items']\n",
    "    recall  = trial['recalled_items']\n",
    "    subject_arr   [i, 0] = trial['subject']\n",
    "    session_arr   [i, 0] = trial['session']\n",
    "    listLength_arr[i, 0] = len(study)\n",
    "    pres_itemnos  [i, :len(study)]   = encode_pres_itemnos(study)\n",
    "    recalls_arr   [i, :len(recall)]  = encode_recalls(study, recall)\n",
    "\n",
    "# === 8) Save to HDF5 ===\n",
    "with h5py.File(OUTPUT_FILE, 'w') as f:\n",
    "    for name, data in {\n",
    "        \"subject\": subject_arr,\n",
    "        \"session\": session_arr,\n",
    "        \"listLength\": listLength_arr,\n",
    "        \"pres_itemnos\": pres_itemnos,\n",
    "        \"recalls\": recalls_arr\n",
    "    }.items():\n",
    "        f.create_dataset(name, data=data, compression=\"gzip\")\n",
    "\n",
    "print(f\"Saved EMBAM dataset with {N_trials} trials to '{OUTPUT_FILE}'\")\n",
    "for name, arr in [\n",
    "    (\"subject\", subject_arr),\n",
    "    (\"session\", session_arr),\n",
    "    (\"listLength\", listLength_arr),\n",
    "    (\"pres_itemnos\", pres_itemnos),\n",
    "    (\"recalls\", recalls_arr)\n",
    "]:\n",
    "    print(f\"  • {name}: {arr.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxcmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
