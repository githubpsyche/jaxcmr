# Introduction {.unnumbered}

Episodic memory -- our ability to recall events anchored to a unique time and place [@tulving1972episodic] -- must satisfy two opposing demands in order to flexibly guide behavior [@yassa2011pattern;@schlichting2015memory].
It must integrate related experiences that occur at different times so a single cue can bridge temporal gaps, yet it must also differentiate those experiences so one moment's details are not confused with those of another.memo
Failure on either front undermines adaptive behavior: without integration, we cannot generalize; without differentiation, we confuse the past.

A visit to the neighborhood bakery illustrates the challenge.
On Monday you step inside, inhale warm cinnamon, and buy a fresh loaf. 
Ten days later you return and are greeted by the aroma of espresso and a new cashier. 
Weeks later, a friend's mention of "the bakery down the street" retrieves both visits. 
The cue integrates the episodes -- both the loaf and the espresso come to mind -- yet you still know which purchase happened when.
Memory links the related episodes without collapsing their distinctive details.

This dissertation examines these dynamics in a list-learning paradigm across free and serial recall.
Participants study a single list in which a target word -- e.g., *canoe* -- appears twice, once early (position 5) and once later (position 15).
Each occurrence has its own temporal context: different neighbors flank the early and late *canoe*.
In serial recall, the correct output is unambiguous: *canoe* must be spoken twice, each time followed by the correct neighbor from its study position.
Frequently substituting a neighbor tied to the other occurrence would indicate that the two study moments were not kept fully apart in memory [@henson1996unchained; @kahana2000interresponse].
In free recall, order is unconstrained, but transitions remain diagnostic: frequent transitions from *canoe* or a first‑occurrence neighbor to the second occurrence's neighbor (and vice versa) suggest that both occurrence contexts were reinstated [@lohnas2014retrieved].
Across tasks, such transitions provide a behavioral portrait of how memory stores and reinstates contextual details tied to each occurrence.

Retrieved‑context theory (RCT) explains recall as the interaction of a drifting temporal context with item‑driven reinstatement [@howard2002distributed; @kahana2020computational].
Each study or recall event blends associated features into a context state that evolves over time, leaving a recency‑weighted record.
At study, states of this dynamic temporal context are associated with items in memory;
at retrieval, it serves as the primary cue, prioritizing items studied in similar contexts.
The theory's central claim is that reinstating the context associated with an item at retrieval allows the system to bridge the past and present.

Retrieved‑context theory principally addresses the temporal organization of memory search, the tendency for items to be recalled in study order.
Because each recalled item reinstates context from its own study position, the cue shifts toward the next serial position, naturally generating the forward‑skewed lag‑contiguity effect and the strong immediate‑recency advantage in short lists [@kahana1996associative; @howard2002distributed].
Implementations add control mechanisms for how recall is initiated, progresses, and terminates [@murdock1962serial; @polyn2009context], and related variants adapt the approach to serial-order memory [@logan2018automatic; @logan2021serial; @lohnas2024retrieved].

In the most prominent implementation of RCT, the Context Maintenance and Retrieval model (CMR) [@howard2002distributed; @polyn2009context], item-context associations are stored in composite weight matrices that map items to a single blended representation of contextual features across their occurrences.
When an item is retrieved, the model looks up and reinstates that composite with no additional selection step.
A contrasting instance-based strategy [@hintzman1984minerva] stores a separate trace for every study event, pairing the item with its occurrence-specific context.
Within that framework, reinstatement is configurable along a continuum: it may reinstate a fixed, item-level composite (recovering classic CMR behavior), construct a cue-weighted mixture across occurrence-specific contexts at retrieval, or -- in the limiting case -- select the single best-matching occurrence.
So whereas CMR's composite architecture converts one reinstatement approach into an architectural commitment, the instance-based framework's additional flexibility makes it an attractive testbed for exploring the nuances of context reinstatement.

Instance-based approaches have been effective in domains from category learning [@turner2019toward; @nosofsky2002exemplar; @stanton2002comparisons] to episodic and semantic memory [@cox2020similarity; @hintzman1988judgments; @jamieson2018instance; @shiffrin1997model].
However, aside from the Context Retrieval and Updating (CRU) model for serial-order memory [@logan2018automatic;@logan2021serial], they have played a limited role in retrieved-context modeling of episodic memory search.
The under-representation appears to reflect modeling tradition rather than a theoretical barrier; nothing in RCT prevents applying the item-driven update rule to a growing set of traces.
Work across domains also shows conditions under which composite and instance formulations become equivalent rather than distinct [@turner2019toward; @kelly2017memory; @ramsauer2020hopfield; @anderson1995introduction].

This dissertation examines and leverages the flexibility of instance-based architectures to sharpen the architectural and algorithmic stakes for retrieved-context theory:

In **Chapter 1**, I formalize Instance-CMR, an instance-based reformulation of CMR that stores each item–context pairing as a separate trace.
By collapsing those traces, the model reproduces CMR's behavior exactly, demonstrating that RCT's core claims do not depend on composite storage.
The chapter establishes architecture as an adjustable modeling choice rather than a theoretical boundary within RCT.

In **Chapter 2**, I compare CMR with the Context Retrieval and Updating (CRU) model, an instance-based account of serial recall.
A factorial model-selection analysis across free- and serial-recall datasets shows that the models' divergent successes stem not from storage format but from process choices like item confusability, context-drift rules, primacy/recency mechanisms, and retrieval-termination criteria.
The exercise clarifies the unique contributions of each model and yields a common suite of benchmarks and model features across task domains that the final chapter exploits.

In **Chapter 3**, I test an assumption shared by most retrieved-context models: that presentation and retrieval of an item reinstates a composite of the contexts associated with the item's earlier occurrences.
Because composite reinstatement blends contexts across occurrences, this rule predicts that recalling one occurrence of *canoe* should subsequently bias recall toward neighbors from all occurrences, a signature of associative interference that I show is absent in six published datasets.
In Instance-CMR, allowing repetitions to reinstate distinct contextual features and letting individual traces compete for reinstatement eliminates the mismatch.
This change improves fits to key benchmarks without adding parameters.
The chapter re-evaluates established understanding of how retrieved-context theory addresses repetition effects, showing how selective reinstatement reconciles integration with specificity and resolves a long-standing empirical puzzle.

Taken together, these chapters separate commitments from conveniences in retrieved-context accounts of recall.
They show that the core regularities of free and serial recall do not require storage-side blending; instead, the levers that matter most are the rules that steer recall.
For repetitions, the datasets analyzed are inconsistent with a composite-reinstatement assumption and are better explained when occurrences each reinstate unique contextual features and recall is dominated by the best-matching occurrence. 
The result is a refined account of repetition in free and serial recall that reconciles integration with specificity and offers a configurable framework and shared benchmarks for future tests.
