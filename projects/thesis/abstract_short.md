Retrieved‑context theory (RCT) explains episodic memory as a search guided by a continually evolving contextual state. Most implementations, exemplified by the Context Maintenance and Retrieval model (CMR), accumulate item–context learning into composite stores that blur event boundaries. This dissertation shows that RCT’s commitments are architecturally portable, that predictive success hinges on algorithmic choices rather than representational format, and that a common assumption about repetitions mischaracterizes contextual reinstatement. Chapter 1 introduces Instance‑CMR (ICMR), an instance‑based reformulation that stores each item–context pairing as a separate trace. When its parameters collapse traces into a composite input, ICMR reproduces CMR behavior, erasing architecture as a theoretical boundary. Chapter 2 exploits this equivalence to compare CMR, tuned for free recall, with the Context Retrieval and Updating model (CRU), an instance‑based serial recall model. A factorial model‑selection analysis across free‑ and serial‑recall datasets attributes divergent successes to algorithmic factors—item confusability, context drift, primacy/recency mechanisms, and termination criteria—yielding shared benchmarks and features across tasks. Chapter 3 tests the assumption that repeated items reinstate overlapping features, creating interference across occurrences. Re‑analyses of six datasets reveal no measurable interference, contradicting CMR and CRU. Within ICMR, allowing episode‑specific reinstatement and competition among traces removes the interference while preserving key fits, without adding parameters. Together, the chapters show that episodic memory balances integration and specificity by storing distinct traces and adjudicating among them at retrieval, preserving RCT’s strengths while overcoming limitations of composite implementations. These results motivate a unified, task‑general framework for modeling free and serial recall performance across diverse paradigms.