# An Instance-Based Retrieved-Context Model of Memory Search

Computational models of free recall explain how people recover a sequence of studied items without an external cue.
Within this literature, retrieved‑context theory (RCT) -- formalized in the Context Maintenance and Retrieval (CMR) model -- has achieved broad empirical coverage, capturing phenomena such as primacy and recency effects, lag‑contiguity, semantic clustering, and several population differences [@howard2002distributed; @sederberg2008context; @polyn2009context; @kahana2020computational; @healey2019contiguity].
Here I ask whether the architectural choice between composite matrices and trace stores matters for a retrieved-context model's ability to explain free recall.

Almost every RCT implementation adopts a connectionist architecture: learning updates a fixed-size weight matrix that binds item features to a drifting temporal context, so information from many episodes is blended at storage.
Retrieval multiplies a cue by that matrix and spreads activation along the shared weights [@hebb1949organization].
Because blending occurs during learning, the resulting memory is usually event-blending: it tends to reinstate a composite of associations rather than a specific prior episode.

A complementary tradition in cognitive modeling assumes that the memory system stores one trace per event. 
In these instance-based architectures, a cue activates each trace in proportion to similarity and a similarity-weighted aggregation (an 'echo') forms the model's response [@hintzman1984minerva].
When trace activations are exponentiated by a high trace‑sensitivity parameter, retrieval can focus on a single trace, yielding an event‑preserving memory that recalls one trace with minimal interference from other traces.
Although architecture and retrieval regime are not independent, they are conceptually distinct: a connectionist network fed orthogonal inputs can behave event‑preservingly, and an instance system with nearly identical traces can blend events. 
Instance models have proved useful in modeling category learning [@turner2019toward; @nosofsky2002exemplar; @stanton2002comparisons], recognition memory [@shiffrin1997model; @hintzman1988judgments], and semantic tasks such as word-sense disambiguation [@jamieson2018instance].
They win out in these domains when the task requires retrieving a specific prior episode, such as classifying an exemplar that shares rare features with a previous training item [e.g., @nosofsky2002exemplar; @jamieson2018instance].
Yet other studies have emphasized similarities between these frameworks across areas [e.g., @anderson1995introduction; @turner2019toward;@kelly2017memory; @ramsauer2020hopfield].

Despite these exchanges, comparatively little work has directly compared connectionist and instance architectures in the context of free recall.
To address this gap, I focus on two questions: 
(i) Does storing or retrieving event-preserving traces rather than blended weights change quantitative fits to serial‑position curves, lag-contiguity effects, or other phenomena; and
(ii) Are RCT's contextual computations portable across storage formats?
A positive answer to the first question would highlight an architectural ingredient previously overlooked, while a positive answer to the second would broaden RCT's reach.

Competing frameworks such as SAM [@raaijmakers1981search] and CRU [@logan2018automatic] explore different retrieval mechanisms alongside their architectural differences.
Here I focus narrowly on how storage format affects an otherwise unchanged CMR.
I introduce Instance-CMR (ICMR), a trace-based translation of CMR that leaves every mechanism unchanged except storage.
Each study event concatenates item features with the current temporal-context vector to form a new trace.
Retrieval reinstates context and cues items, but the cue now accesses a trace stack rather than a weight matrix.
When the trace-sensitivity parameter is set to a neutral value ($1.0$), ICMR is functionally equivalent to connectionist CMR, making identical predictions for all phenomena.
Freeing the parameter enables event-preserving retrieval, allowing ICMR to amplify the few traces most consistent with a cue, potentially improving the fit in conditions where recalling a particular prior episode matters.
ICMR thus provides a framework for comparing connectionist and instance-based retrieval in free recall tasks.

The remainder of the chapter first contrasts connectionist and instance traditions, emphasizing functional consequences for memory search, then details the Instance‑CMR implementation and evaluates both models on three free‑recall datasets that vary in list length and repetition pattern.  
Simulations demonstrate their functional equivalence when trace sensitivity is neutral and explore departures when selective‑trace retrieval is enabled.
My discussion explores the implications of these findings for interpreting the theoretical competition between instance and prototype memory models and the adaptability of retrieved-context theory across different cognitive modeling frameworks.
While simulations confirm that these unique properties can influence model behavior, my results show that the instance/composite distinction, while theoretically salient, does not by itself explain differences in model performance.
These findings set the stage for Chapter 2, where architecture is held fixed and algorithmic components are factorially examined.

## Relating Connectionist and Instance Memory Models

In retrieved-context models of memory search, memory representations encode associations between items and their contexts to support the retrieval of these associations using retrieval cues.
In this section, we explore and compare two main approaches for implementing these principles: connectionist models and instance models.
For our exploration of instance theory, we focus on the MINERVA-2 architecture, a well-established example of instance-based modeling [@hintzman1984minerva; @hintzman1986schema; @hintzman1988judgments].
In MINERVA-2, each learning experience is captured as a discrete memory trace, which is a vector of feature values.
All these vectors are the same length and are organized into a stack, making it easy to access and retrieve them individually:

$$
M = \begin{bmatrix}
\mathbf{m}_1 \\
\mathbf{m}_2 \\
\vdots \\
\mathbf{m}_n
\end{bmatrix}
$$ {#eq-intro0}

The model's memory is probed by a cue vector $\mathbf{cue}$, which is compared to each memory trace $m$ using a dot product similarity function, $S$.
Compared to the cosine similarity (another popular similarity mechanism for instance models), dot product is a useful similarity measure here because it does not normalize the vectors, allowing differences in learning rates to be directly reflected in trace magnitudes, simplifying model structure.
This approach measures the product of the cue and memory trace vectors, providing a straightforward measure of their similarity:

$$
S(cue, m_i) = cue \cdot m_i
$$ {#eq-intro1}

An important parameter, termed choice selectivity, influences how the activation $A_i$ from each trace diminishes based on its similarity, raising the dot product to the power $\tau$:

$$
A_i(cue, m_i) = S(cue, m_i)^\tau
$$ {#eq-intro2}

At a neutral value ($1$), activation is a linear function of similarity.
When choice selectivity is especially low (for example, $0$), the model's response is broad, activating many traces almost equally.
Conversely, a high choice selectivity value sharpens the model's response, significantly activating only the traces that are most similar to the cue.
The ability to precisely target relevant traces in response to a probe is the key feature of instance models.

The model's output, often visualized as an "echo," represents the sum of all memory traces weighted by their activation:

$$
E = \sum_i A(cue, m_i)
$$ {#eq-intro3}

This metaphor likens the model's output to a sound wave bouncing back to its source, reflecting the selective recall process of the memory system.

The Context Maintenance and Retrieval (CMR) model uses a type of memory called a linear associative memory, which follows a Hebbian learning rule [@polyn2009context].
Unlike instance models that store each memory as a separate trace, basic connectionist models like this one capture associations between items and context features in a weight matrix.
This matrix, where $n$ represents item features and $m$ context features, looks like this:

$$
W = \begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1m} \\
w_{21} & w_{22} & \cdots & w_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
w_{n1} & w_{n2} & \cdots & w_{nm}
\end{bmatrix}
$$ {#eq-intro4}

During learning, the model updates these weights using the Hebbian rule, which changes the weights based on the interaction between input and output vectors.
Each update is calculated by multiplying a learning rate $\eta$ with the input feature $x_i$ and the output feature $y_j$:

$$
\Delta w_{ij} = \eta x_i y_j
$$ {#eq-intro5}

In connectionist models, learning involves integrating all the associations directly into the weight matrix.
The model's response to a cue is a vector of activations, calculated as the dot product of the cue vector and the weight matrix, adjusted by a scaling parameter $\tau$:

$$
A = (cue \cdot W)^{\tau}
$$ {#eq-intro6}

This scaling parameter $\tau$ modifies how much each feature influences the overall response, allowing the model to emphasize certain features more than others based on their relevance to the cue.
However, this mechanism adjusts the activation of features collectively, rather than activating individual memory traces as instance models do.
Instance models uniquely retrieve specific memory traces during the recall process, based on their similarity to the cue and the selectivity parameter $\tau$.

This difference highlights two fundamental distinctions between these modeling approaches.
First, instance models organize and retrieve memory traces individually at the time of recall, whereas connectionist models integrate information during the learning phase, storing it in the weight matrix.
Second, instance models can precisely target specific memories for retrieval via trace-based activation scaling.
In contrast, connectionist linear associator networks used in CMR generate a response by combining the influences of all features learned, using the weight matrix, and cannot isolate individual memory traces under typical conditions.
Because the weight matrix aggregates over memory traces during learning in this way, linear associators are normally classified as composite memory models -- or prototype models when contrasted with instance models [@nosofsky2002exemplar; @jamieson2018instance; @anderson1995introduction].

### The Prototype Model Distinction

This distinction has important caveats.
The defining feature of an instance model is that it can retrieve records of specific experiences (instances) without interference from other experiences.
This definition is behavior-based rather than architecture-based, and many different kinds of models can realize it, even including connectionist models that implement separable memory traces [@anderson1995introduction].
For example, while simplified connectionist models lack the flexibility of instance models to separately activate individual memory traces, more complex connectionist models can be designed to achieve this functionality [@kelly2017memory; @hintzman1988judgments] using hidden layer(s) to differentiate between memory traces.

Even simple connectionist models will depend on the nature of the input-output pairs encoded by the network.
If the inputs are highly differentiated—having few or no overlapping features—then the network will operate like an instance model.
Instead, whether a linear associator network is a prototype model or an instance model depends on the nature of the input-output pairs that are encoded by the network.
If the inputs are highly separated such that they share few or no overlapping features, then the network will behave like an instance model.
This separation ensures the model can retrieve output patterns corresponding to a specific training instance without activating interfering instances.
Conversely, even when features across inputs substantially overlap, a linear associator may not behave like a prototype model, especially if the number of input-output pairs encoded is relatively small.
To act like a prototype model, it's not enough that outputs corresponding to specific training instances are inaccessible.
The memory must also exhibit a prototype effect, where a prototype -- or, average -- over training instances organizes retrieval outcomes instead of individual instances.
This distinction underscores how input and output characteristics shape the operational mode of connectionist networks.

@anderson1995introduction provides a framework for understanding how prototype- and instance-based retrieval can occur in connectionist models.
In this framework, individual inputs linked to the same output g\$ are analyzed as a sum of a prototypical component $p$ (the average vector of all inputs) and a unique component $d_i$ -- a deviation or 'noise' vector that shows how each input differs from the average.

Here's the mathematical representation:

$$
f_i = p + d_i
$$ {#eq-intro7}

The overall connectivity matrix $W$ for $n$ inputs that share the same output is then calculated as:

$$
W = \sum_i gf_i^T
$$ {#eq-intro8}

$$
W = g \sum_i (p^T + d_i^T)
$$ {#eq-intro9}

$$
W = ngp^T + g \sum_i d_i^T
$$ {#eq-intro10}

This analysis reveals that the behavior of the network primarily hinges on the total size of the noise terms $\sum_i d_i$.
If the noise is minimal, indicating high similarity among the inputs, the connectivity matrix simplifies to:

$$
W = ngp^T
$$ {#eq-intro11}

This represents the matrix of a prototype model, where the output relies on the collective average of the input features.
In effect, this model's behavior is akin to having encoded just the prototype pattern $p$, treating it as the representative of all inputs.
Conversely, if the noise component is significant, reflecting high dissimilarity among the inputs, then this noise dominates the connectivity matrix, and the model functions like an instance model.
Here, the output depends on how similar an input is to each training input's distinctive features:

$$
W = g \sum_i d_i^T
$$ {#eq-intro12}

This analysis underscores that the behavior of a linear associator model is influenced as much by the specifics of the input-output pairs it encodes as by its structural design.
Thus, comparisons between connectionist models and those explicitly based on instance theory must carefully consider these elements.

### Theoretical Competition Between Instance and Prototype Memory Models

While connectionist models can be designed to exhibit instance-like behavior, their default behavior is to collapse information across multiple experiences into a single best-fitting representation that best represents the item's associations.
Models that behave in this way are often referred to as prototype models.
In the study of memory, the debate between instance and prototype models has been a recurring theme across various fields, such as recognition, semantic memory, and category learning [@hintzman1988judgments; @nosofsky2002exemplar; @jamieson2018instance].
The key issue in this research is whether memory tasks are better supported by dynamically reactivating and combining specific experiences, or by using generalized representations that summarize common features across multiple experiences.
This section reviews examples of how how instance and prototype models can differently predict human behavior in memory tasks.

A clear illustration of these differences is seen in studies of category learning.
In typical experiments, participants are presented with examples (exemplars) that belong to one of two categories.
Each exemplar is characterized by several binary features (such as having a short or long body, a closed green eye or an open red eye, etc.), with some features strongly linked to category membership and others less so.
Participants learn these categories by observing exemplars and then try to classify new ones.

Both instance and prototype models predict how participants might categorize new exemplars based on features typical of a specific category.
However, the models diverge when dealing with exemplars that have ambiguous features regarding category membership [@stanton2002comparisons].
Prototype models usually assign such novel exemplars to categories based on their similarity to a composite representation of each category.
In contrast, instance models configured to retrieve specific instances from memory tend to categorize new exemplars based on the closest matching examples seen during training.
Across this and other comparisons [e.g., @nosofsky2002exemplar; @homa1981limitations], the consistent theme is that prototype models tend to make predictions that are more consistent with the average of experience over a label class, while instance models tend to make predictions that are more consistent with the most similar and relevant training instances.

Instance and prototype models also differ in how they handle word-sense disambiguation, a process where the meaning of a word is clarified based on context [@jamieson2018instance].
These models use orthographic cues to trigger semantic memories.
A challenge arises with homonyms—words with the same spelling but different meanings depending on the context.
Prototype models have been criticized for typically representing homonyms with a single, overarching semantic representation, despite their distinct meanings [@griffiths2007topics].

In a study by @jamieson2018instance, the effectiveness of traditional prototype models was compared to a novel instance model of semantic memory, known as ITS, in a task of word-sense disambiguation.
While prototype models generally performed well in distinguishing between common meanings of homonyms, ITS excelled at recognizing less common meanings through a dynamic activation of stored language instances.
This model's strength was particularly evident when orthographic cues were linked with distinct contextual clusters, with one cluster being much more frequent than others.
Under such conditions, prototype models often skew towards the dominant meanings, neglecting or losing access to lesser-known senses.
This discrepancy leads to a mismatch with observed human behavior, where people can often discern less frequent meanings based on nuanced cues.

This pattern underscores a consistent theme across memory studies: instance models are superior when it comes to pinpointing specific memories for retrieval, due to their flexible targeting of individual memory instances.
Despite these insights, the implications of these differences for free recall—recalling items from memory without cues—have not been thoroughly investigated.
Understanding how these models influence free recall could bridge the gap between modeling approaches and lead to integrated memory models that leverage the strengths of both instance and prototype systems.
This study aims to address this gap by comparing instance and prototype models within a unified framework, assessing their impact on the dynamics of memory search in free recall.

### Modeling Memory Search in Free Recall

Memory can be thought of as a system that links retrieval cues with information based on past experiences.
Memory search involves using this system repeatedly to find information that wasn't initially recalled.
Information retrieved using previous cues informs the construction of successive retrieval cues, supporting the search for further unrecalled information.
Because this search process involves ongoing interactions with the memory system, the fundamental ideas we have about how memory works significantly affect how the search process unfolds.
Here we review the basic principles of retrieved-context theory and the Context Maintenance and Retrieval model, and then describe how these principles can be implemented in both instance and connectionist memory models.

Research into how people recall items has helped us identify basic constraints on theories of the representations and mechanisms that underlie memory search.
Three important patterns, in particular, have been emphasized in studies of free recall [@kahana2020computational].
One such pattern is the serial position effect, which shows a U-shaped relationship between the position of an item on a study list—its serial position—and its likelihood of being recalled [@murdock1962serial].
Researchers have noted that items presented at the beginning of a list and those at the end tend to be recalled more frequently.
The enhanced recall of the first few items is known as the primacy effect, while the often stronger recall of the last few items is called the recency effect.
Primacy and recency effects demonstrate that the temporal structure of the list affects the memorability of the items within it.

This temporal structure can also be seen in the organization of responses throughout the response sequence, not just for initial and terminal items or recall positions.
Free recall task data exhibits a lag-contiguity effect where items studied at nearby serial positions tend to be recalled near one another at the retrieval phase of an experiment [@kahana1996associative].
To analyze this phenomenon, researchers calculate the conditional probability of recalling an item based on the distance, or lag, between its position and the position of the last item recalled -- the lag-CRP.
Studies consistently show that participants are more likely to transition between items that are near each other in the list, demonstrating a preference for recalling items in a temporally contiguous sequence.
Additionally, there is a forward bias in recall, where participants are more likely to recall items that follow, rather than precede, the last item recalled.

Retrieved context models have successfully explained many aspects of performance in free recall tasks [@howard2002distributed; @polyn2009context; @morton2016predictive].
These models propose that as items are encoded, a representation of the temporal context is also formed and continually updated to reflect a recency-weighted summary of past experiences.
Each item's features are linked in memory to the features of the current temporal context.
This context then guides memory search by acting as a retrieval cue that prioritizes items most associated with the current context.
To account for the recency effect, these models suggest that the temporal context at the end of the study phase closely resembles the context at the beginning of the retrieval phase, enhancing the recall of the most recently studied items.
When an item is recalled, it can also trigger the retrieval of the temporal context active when it was first learned, updating the current context and making it more likely that the next item recalled will be one that was studied nearby—this is the basis for the lag-CRP effect.
On the other hand, primacy effects are primarily attributed to the greater attention and effort devoted to the first items during the study phase, which makes them more likely to initiate the recall process.

The present work focuses on a specific implementation of the retrieved context theory, the Context Maintenance and Retrieval model [CMR, @polyn2009context; @morton2016predictive].
Briefly, CMR employs a pair of linear associator memories to encode associations between items and states of temporal context.
Temporal context here is a vector feature representing a recency-weighted history of items studied, where more recent items have a heavier influence than older ones.
The model utilizes an item-to-context memory to link item cues with features of the temporal context, and a context-to-item memory to associate contextual cues with corresponding items.
These mechanisms facilitate context reinstatement and context-driven recall.
Beyond the standard phenomena previously discussed, CMR has been applied successfully in diverse areas such as financial decision-making [@wachter2019retrieved], emotion regulation [@talmi2019retrieved], and even in studying neural signal dynamics within the medial temporal lobe [@kragel2015neural].
The model's development has also embraced integrations with theories of semantic knowledge [@morton2016predictive] and age-related cognitive changes [@healey2016four].

In the forthcoming sections, we introduce an instance-based specification of CMR.
This version encodes associations by forming memory traces for each experience that merge features of the item studied with the current state of temporal context.
Initially, like its connectionist counterpart, this model version uses dual memory representations.
However, we later expand it to a singular memory representation that incorporates temporal context, semantic, and item features, enhancing its capacity to handle semantic contiguity effects along with the standard phenomena.

By exploring an instance-based variation of CMR, we aim to discern whether the model's effectiveness stems from the connectionist approach or the broader principles of retrieved context theory.
Additionally, we seek to demonstrate how instance-based models might enhance the explanatory scope of retrieved context theory.
Showing that these principles can be adapted across different memory architectures sets the stage for future integrations of retrieved context theory with other successful memory models.
The following sections will provide a more detailed comparison of connectionist and instance-based approaches within the context of free recall, further illuminating the framework of retrieved context theory.

## Model Structure

::: {#fig-comparison layout-nrow="2"}
![Connectionist CMR](icmr_figures/connectionistcmr_diagram.png){#fig-connectionist-cmr}

![Instance CMR](icmr_figures/instancecmr_diagram.png){#fig-instance-cmr}

**Aggregation of information across memory traces happens at different time points in the two versions.** The connectionist model aggregates over traces during encoding, because there is a fixed-size set of associative weights.
The instance model creates separate traces for each event, so aggregation happens during retrieval.
:::

We base our implementations of Connectionist CMR and Instance CMR on the likelihood-based specification provided by @morton2016predictive and @kragel2015neural.
The original connectionist version of the Context Maintenance and Retrieval model (ConnectionistCMR) uses this concept by employing two types of memory networks, $M^{FC}$ and $M^{CF}$.
These networks are linear associative networks where the connection weights symbolize the forward and backward relationships between the two layers.
Essentially, these weights measure how strongly specific items and contexts are linked based on how often they occur together.
In contrast, our initial specification of a multi-trace version of CMR (InstanceCMR) also uses $M^{FC}$ and $M^{CF}$ to represent the relationships between items and contexts.
However, rather than blending these associations into a cumulative pattern of weights, each memory in Instance CMR keeps an individual trace for every experience.
This means it concatenates the states of $F$ and $C$ at the time of encoding, preserving a distinct record for each event.
To fully explain how each model is structured, we first outline the memory systems used by each model to encode and retrieve these item-context associations.
We then explore how both models manage these memories to support the process of searching for memories and performing tasks like free recall, where items must be remembered without prompts.
This comparison shows not just how the models store information but also how they use it to reconstruct past events during memory tasks.

| Symbol | Name | Description |
|-------------------|-------------------|----------------------------------|
| $C$ | temporal context | A recency-weighted average of encoded items |
| $F$ | item features | Current pattern of item feature unit activations |
| $M^{FC}$ | item-to-context memory | encoded feature-to-context associations |
| $M^{CF}$ | context-to-feature memory | encoded context-to-feature associations |
| ${\beta}_{enc}$ | encoding drift rate | Rate of context drift during item encoding |
| ${\beta}_{start}$ | start drift rate | Amount of start-list context retrieved at start of recall |
| ${\beta}_{rec}$ | recall drift rate | Rate of context drift during recall |
| ${\alpha}$ | shared support | Amount of support items initially have for one another |
| ${\delta}$ | item support | Initial pre-experimental contextual self-associations |
| ${\gamma}$ | learning rate | Amount of experimental context retrieved by a recalled item |
| ${\phi}_{s}$ | primacy scale | Scaling of primacy gradient on trace activations |
| ${\phi}_{d}$ | primacy decay | Rate of decay of primacy gradient |
| ${\theta}_{s}$ | stop probability scale | Scaling of the stop probability over output position |
| ${\theta}_{r}$ | stop probability growth | Rate of increase in stop probability over output position |
| ${\tau_{c}}$ | choice sensitivity | Exponential weighting of item supports during the recall competition |
| ${\tau_{t}}$ | trace sensitivity | Exponential weighting of trace activations during probes of $M^{CF}$ |

: Parameters and structures specifying CMR. {#tbl-cmr-parameters}

### Connectionist Memory Representations

In Connectionist CMR, the memories $M^{FC}$ and $M^{CF}$ are are structured as two weight matrices, where each element is linked to the features of items and contexts respectively.
For example, $M^{FC}_{ij}$ represents the association between the $i$th feature of an item and the $j$th context in feature-to-context memory [@fig-connectionist-cmr].

Before any experiment begins, we set up $M^{FC}$ to summarize existing associations between relevant item features and potential contextual states.
This initial configuration is guided by:

$$
M^{FC}_{pre(ij)} = \begin{cases} \begin{alignedat}{2} 1 - \gamma \text{, if } i=j \\\
          0 \text{, if } i \neq j
   \end{alignedat} \end{cases}
$$ {#eq-1}

This setup connects each item feature unit on $F$ to a unique context unit on $C$ using a parameter $\gamma$, which dictates how much pre-experimental associations influence the model's behavior.

Similarly, the associations from context to feature, managed by $M^{CF}$, are initially set as follows:

$$
M^{CF}_{pre(ij)} = \begin{cases} \begin{alignedat}{2} \delta \text{, if } i=j \\\
          \alpha \text{, if } i \neq j
       \end{alignedat} \end{cases}
$$ {#eq-2}

Here, $\delta$ adjusts the impact of pre-experimental context-to-feature associations compared to those acquired during the experiment.
The $\alpha$ parameter establishes a uniform semantic support across items, supporting each other in the recall competition.

The weights between any two units $i$ and $j$ in $M^{FC}$ or $M^{CF}$ are updated based on the Hebbian learning rule, which relies on the activity levels in the item and context layers.
Specifically, in $M^{FC}$, the learning rate of these updates is governed by the $\gamma$ parameter again:

$$
\Delta M^{FC}_{ij} = \gamma \cdot F_i \cdot C_j
$$ {#eq-3}

This formula adjusts the strength of the connection based on the simultaneous activity of item features $F_i$ and context $C_j$, with the $\gamma$ parameter controlling the learning rate.

In $M^{CF}$, a function $\phi$ enforces a primacy effect, scaling the amount of learning based on the serial position of the studied item according to:

$$ 
\phi_i = \phi_se^{-\phi_d(i-1)} + 1
$$ {#eq-4}

where $\phi_s$ and $\phi_d$ are parameters that control the strength and decay of the primacy effect, respectively.

The weight between units $i$ and $j$ in $M^{CF}$ is updated according to:

$$
\Delta M^{CF}_{ij} = \phi_i C_i F_j
$$ {#eq-5}

To retrieve contextual associations for a given item, the model computes the dot product of the item's feature vector $f$ with $M^{FC}$.

$$
\hat{C} = M^{FC} f
$$ {#eq-6}

Similarly, to retrieve item associations for a given state of context, the model computes the dot product of the context's feature vector $c$ with $M^{CF}$.

$$
\hat{F} = M^{CF} c
$$ {#eq-7}

These details describe how the connectionist CMR model encodes and retrieves associations between items and contexts in memory and enforces a primacy effect during learning to support memory search in the free recall task.

### Instance-Based Memory Representations

In contrast to the traditional connectionist implementation of CMR, our new instance-based specification of CMR (Instance CMR) represents $M^{FC}$ and $M^{CF}$ as instance-based memories.
That is, they store a distinct trace for every experience, keeping each event's unique details separate [@fig-instance-cmr].
Initially, like the traditional model, Instance CMR uses two different memory matrices to capture the associations between items and contexts ($M^{FC}$ and $M^{CF}$).
This differentiation helps to account for differences in how $M^{FC}$ and $M^{CF}$ represent pre-experimental associations between item and context features.
Later on, we will explore how merging these into a single matrix that also incorporates semantic features can streamline this process.

In the current specification, $M^{FC}$ and $M^{CF}$ are represented as two multi-trace memories.
Each trace is represented as a vector that concatenates feature vectors representing the states of $F$ and $C$.
Each trace combines feature vectors from the item layer ($F$) and the context layer ($C$), forming a complete snapshot of the encoding event.
These vectors are arranged in matrices where each row corresponds to a different experience, and each column aligns with a particular feature or context unit.

To ensure a clear comparison between the traditional composite and the new multi-trace architectures, we initialize $M^{FC}$ and $M^{CF}$ in Instance CMR to mirror the pre-experimental associations found in Connectionist CMR. For $M^{FC}$, this means setting up the memory with a trace for each feature in $F$ that connects to a specific context unit in $C$.
Initially, all features in these traces are set to $0$, except for the ones directly linked, which are set to $1$ and $1 - \gamma$ respectively.

Similarly, for $M^{CF}$, each context unit in $C$ starts connected primarily to one feature in $F$ but maintains a weaker link to all other features.
Here, context units in the traces are set to $0$ except for the primary connection, which is marked as $1$.
Item features in these traces are mostly set to $\alpha$, except for the directly associated feature, which is initialized at $\delta$.
This setup provides a structured foundation from which both memories can evolve as they encode new experiences and interactions during the experimental phase.

In the Instance CMR model, instead of updating a single weight between units for each new experience, we add a new trace that captures the state of both the item features $F$ and the context features $C$ at the time of encoding.
Differences in trace strengths ("learning rates" in connectionist memories) are taken into account at retrieval rather than encoding.
So in $M^{FC}$ and $M^{CF}$ matrices, each experience simply contributes a new trace that concatenates the item and context feature vectors:

$$
M^{FC}_{exp_{i}} = [f_i, c_i]
$$ {#eq-8}

$$
M^{CF}_{exp_{i}} = [c_i, f_i]
$$ {#eq-9}

Retrieving associations from these multi-trace memories involves a couple of steps.
First, the probe vectors for $M^{FC}$ consist only of $F$ features, formatted as $[f_{probe}, 0]$, and for $M^{CF}$, they consist only of context features, similarly formatted.
These vectors are concatenated with a zero vector to align with the dimensionality of the traces in $M^{FC}$ and $M^{CF}$ .
The retrieval process then calculates a weighted sum across both pre-experimental and experimental traces.
The weight of each trace in this sum is determined by the dot product between the probe vector and the trace, as well as the trace's learning strength, which corresponds to the learning rates in the connectionist models.

In the $M^{FC}$ matrix, the parameter $\gamma$ scales the association between the item features and the contextual features within these new traces; it's set to 1 for pre-experimental traces:

$$
[\hat{C} ...] = \sum_{i=1}^{n} \gamma M^{FC}_{i} [f, 0]
$$ {#eq-10}

Similarly, in the $M^{CF}$ matrix, the parameter $\phi_{i}$ scales the association of contextual features with the item features for each new experience, where $i$ is the index of an encoded trace and $\phi_i$ is held to 1 for pre-experimental traces:

$$
[\hat{F} ...] = \sum_{i=1}^{n} (\phi_i M^{CF}_{i} [c, 0] ]^{\tau_{t}}
$$ {#eq-11}

For experimental traces, the equation defining $\phi_i$ is the same as described above for Connectionist CMR. However, the parameter $\tau_t$ is unique to the instance model.
In $M^{CF}$ (but not in $M^{FC}$ ), the parameter $\tau_t$ is used to exponentiate the dot product between the probe vector and each trace.
This method enhances the difference in activation between traces, allowing the most relevant or best-supported traces to stand out more prominently during the retrieval process.
This feature, characteristic of instance models, accentuates the selectivity of memory retrieval at the trace level.
Since connectionist memories do not support trace-level activation modulation in the same way, when enforcing equivalence to Connectionist CMR, $\tau_{t}$ is set to $1$ to effectively disable this mechanism and ensure that the model's behavior is consistent with the connectionist specification.

### Memory Dynamics During the Free Recall Task

Remaining aspects of the model are shared between Connectionist and Instance CMR.

#### Encoding Phase

When an item $i$ is presented for study, its feature representation $f_i$ activates within the item layer $F$, and its contextual associations are retrieved from $M^{FC}$, regardless of whether it's the composite or multi-trace version.
This retrieval modifies the current state of the context layer $C$.
The input to the context from this retrieval is defined by:

$$
c^{IN}_{i} = \hat{C}
$$ {#eq-12}

Here, $\hat{C}$ represents the context vector retrieved from $M^{FC}$.
This vector is then normalized to maintain a consistent scale, specifically so its length equals $1$.

The context state is updated based on this input:

$$
c_i = \rho_i c_{i-1} + \beta_{enc} c_{i}^{IN}
$$ {#eq-13}

In this formula, $\beta_{enc}$ controls how much the context changes--or drifts--with each new item presented, reflecting the influence of the new contextual input.
The parameter $\rho$ is used to ensure the length of $c_i$ remains normalized at $1$.
It is calculated as follows:

$$
\rho_i = \sqrt{1 + \beta^2\left[\left(c_{i-1} \cdot c^{IN}_i\right)^2 - 1\right]} - \beta\left(c_{i-1} \cdot c^{IN}_i\right)
$$ {#eq-14}

This calculation adjusts $\rho_i$ to balance the contribution of the previous context state $c_{i-1}$ and the new input $c^{IN}_i$, ensuring the overall context vector doesn't grow too large or become too small.

At this point, the model captures and solidifies the association between the current states of the context and item features in both $M^{FC}$ and $M^{CF}$.
This dynamic ensures that each new experience slightly alters the context, which in turn influences how subsequent items are encoded and later retrieved.

#### Retrieval Phase

To account for the primacy effect in the model, we posit that between the encoding and retrieval phases, the content of the context layer $C$ drifts back towards its state prior to the experiment.
We model this drift and set the initial state of context at the start of retrieval as follows, with $\rho$ calculated as previously specified:

$$
c_{start} = \rho_{N+1}c_N + \beta_{start}c_0
$$ {#eq-15}

During each recall attempt, this current state of context serves as the cue to $M^{CF}$, attempting to retrieve a studied item and producing an activation vector A on the item layer $F$:

$$
A = \hat{F}
$$ {#eq-155}

To ensure all items have a chance of being recalled, each is given a baseline activation of $10^{-7}$.
The likelihood of ending the recall session, or the probability of stopping the recall without retrieving any more items, depends on the output position $j$ and is given by:

$$
P(stop, j) = \theta_s e^{j\theta_r}
$$ {#eq-16}

Here, $\theta_s$ and $\theta_r$ govern the initial stopping probability and the rate at which this probability increases, respectively, which is modeled as an exponential function.
If recall continues, the probability $P(i)$ of recalling a specific item $i$ primarily depends on its relative activation strength:

$$
P(i) = (1-P(stop))\frac{A^{\tau_{c}}_i}{\sum_{k}^{N}A^{\tau_{c}}_k}
$$ {#eq-17}

The parameter $\tau_{c}$ enhances the contrast in activation strengths between items: a high $\tau_{c}$ value amplifies differences, making the most activated item even more likely to be recalled, whereas a lower $\tau_{c}$ value diminishes these differences, equalizing recall probabilities across items with varying activations.
This mechanism can be implemented for either instance-based or connectionist memories, even on top of pre-existing trace-based activation scaling.

If an item is recalled, it is reactivated on $F$, and its contextual associations are retrieved to update the context.
This new context input is:

$$
c^{IN}_{i} = \hat{C}
$$ {#eq-18}

Context is then updated based on this input using $\beta_{rec}$ (used during retrieval rather than encoding) to adjust for the current phase of the task, setting the stage for the next recall attempt.
This iterative process continues until no further items are recalled, effectively concluding the recall session.

## Simulation 1: Functional Equivalence Between Connectionist and Instance CMR

We have established two versions of the Context Maintenance and Retrieval model: Connectionist CMR and Instance CMR that both instantiate the principles of retrieved context theory to explain memory search in free recall tasks.
But are the models truly equivalent, or are there subtle differences in their behavior that could influence their performance in different scenarios?
While we hold the trace-based activation scaling parameter $\tau_{t}$ constant at 1, the answer to this question is yes.
The reason why is that the two models use the same feature and contextual representations, and perform the same mathematical operations upon them to encode and retrieve associations: The pre-scaled activation of an output feature in either model is determined by the dot product similarity between probe vectors and output features' associated input features, weighted by each trace's respective learning rate.
Here we present simulation analyses of the functional equivalence between Connectionist CMR and Instance CMR, demonstrating how both models encode and retrieve the same associations despite their structural differences.
Establishing the equivalence of Instance CMR and Connectionist CMR under these conditions allows us to explore how trace-based activation scaling might enhance the model's performance in free recall tasks.

To clarify the roles of memory structures in the simulation of CMR and underscore how these models encode and retrieve the same associations despite their structural differences, we provide a functional analysis of the connectivity structure of $M^{FC}$ and $M^{CF}$ in both models that measures associations between item and context features irrespective of how these associations are represented within a memory architecture.
To set up this analysis, we use parameters fitted to the subset of the PEERS dataset as reported by [@healey2014memory] as described in the next section, but with $\tau_{t}$ set to 1.
Then we simulate encoding of an arbitrary study list of size 8 with each model to construct the memory structures $M^{FC}$ and $M^{CF}$ that would organize retrieval in a free recall task.

To compare the connectivity matrices of Connectionist CMR and Instance CMR, we individually activate each input unit and measure the resulting activation in the output layer for each possible input unit, activating one unit at a time.
The resulting vector represents the connectivity between the activated input unit and all output units.
Stacking these vectors together, we obtain the full latent connectivity matrix for each memory structure.
For connectionist $M^{FC}$ and $M^{CF}$, this extracted structure is identical to the weight matrices used in the model.
But for instance-based $M^{FC}$ and $M^{CF}$, this structure is effectively a conversion of the multi-trace memory into a composite memory, where each row summarizes associations between an input unit and all output units across all study events.

Along with producing latent $M^{FC}$ and $M^{CF}$ connectivity matrices, we also analyze their combined influence on the dynamics of memory search in free recall tasks by extracting a latent $M^{FF}$ matrix that passes $F$ input vectors through $M^{FC}$ and then passes the resulting context vectors through $M^{CF}$ to measure the strength of associations between item features across all study events.
This matrix reflects the interplay between $M^{FC}$ and $M^{CF}$ in guiding memory search -- how recalling an item will bias the retrieval of subsequent items based on the context reinstated by the recalled item via $M^{FC}$ and then used as a retrieval cue via $M^{CF}$.
In @fig-matrix, we illustrate that across Connectionist CMR and Instance CMR, these three latent connectivity structures are identical, despite the differences in how they store and retrieve associations.

To further illustrate how the memory structures of Connectionist CMR and Instance CMR support the temporal organization of memory search, we analyze extracted latent connectivity matrices from both models in a lag-connectivity analysis.
In latent $M^{FC}$ representations, this analysis measures the connection strength between each considered item's $F$ representations with each other item's pre-experimentally associated context units $C$ as a function of the other items' serial lag from the considered item.
The connection weight between $F_i$ and $C_j$ for all items $i$ and $j$ is calculated for each lag $j-i$ and plotted as a function of this lag.
For example, if item $i$ is considered, the lag-connectivity analysis measures the strength of the connection between $F_i$ and $C_j$ as a function of the lag $j-i$.
A converse analysis is performed for $M^{CF}$, measuring the connection strength between each considered context unit $C_i$ with each other item's pre-experimentally associated feature units $F$ as a function of the other items' serial lag from the considered item.

Finally, to measure the combined influence of $M^{FC}$ and $M^{CF}$ on memory search, we perform a lag-connectivity analysis on the latent $M^{FF}$ matrix, measuring the connection strength between each considered item's $F$ representations with each other item's $F$ representations as a function of the other items' serial lag from the considered item.
This analysis, visualized in @fig-lag-connectivity, provides a visual representation of how each of $M^{FC}$ and $M^{CF}$ supports the lag-contiguity effect in free recall tasks, with $M^{FC}$ primarily supporting transitions to backward neighbors of the last recalled item and $M^{CF}$ supporting transitions to forward neighbors.
The analysis applied to $M^{FF}$ demonstrates how the combined influence of $M^{FC}$ and $M^{CF}$ instantiates a bidirectional but forward-biased lag-connectivity structure that supports the assymetrical lag-contiguity effect in free recall tasks.
These analyses together demonstrate that the latent connectivity structures of Connectionist CMR and Instance CMR are functionally equivalent, supporting the same dynamics of memory search in free recall tasks.

::: {#fig-matrix layout="[[1,1], [1,1,1]]"}
![](icmr_figures/icmr_mfc.png)

![](icmr_figures/icmr_mcf.png)

![](icmr_figures/latent_mfc.png)

![](icmr_figures/latent_mcf.png)

![](icmr_figures/latent_mff.png)

**Comparison of latent connectivity matrices between Connectionist CMR and Instance CMR**. Top Row: Instance CMR's $M^{FC}$ (left), and $M^{CF}$ (right) matrices.
Each row represents a memory trace associating item and context features.
Bottom Row: Extracted latent connectivity matrices from either CMR variant's $M^{FC}$ (left), and $M^{CF}$ (middle) memories as well as the combined influence of $M^{FC}$ and $M^{CF}$ on memory search in the $M^{FF}$ matrix (right).
These are identical between the Instance and Connectionist models and also identical to Connectionist CMR's $M^{FC}$ and $M^{CF}$ weight matrices.
:::

::: {#fig-lag-connectivity layout="[[1,1,1]]"}
![](icmr_figures/mfc_lag_connectivity.png)

![](icmr_figures/mcf_lag_connectivity.png)

![](icmr_figures/mff_lag_connectivity.png)

**Lag-connectivity analysis of latent connectivity matrices** from $M^{FC}$ (left), $M^{CF}$ (middle), and $M^{FF}$ (right) in both models.
These analyses show how each memory structure supports the lag-contiguity effect in free recall tasks.
:::

## Simulation 2: Performance of Connectionist and Instance CMR on Free Recall Datasets

As shown, in our specification, Connectionist and Instance CMR models are functionally equivalent when the trace-based activation scaling parameter $\tau_{t}$ is set to 1.
This setup ensures that with identical parameter configurations, both models should yield the same results in free recall tasks.
This equivalence is maintained by using consistent association encodings between item and context features, identical learning rates, primacy scaling parameters across experiences, and by disabling the trace-based activation scaling in the instance model.
To demonstrate how both models equivalently handle the dynamics of memory search in free recall, we simulate their performance on a selection of free recall datasets previously used to assess the Connectionist CMR, as well as exploring how Instance CMR performs when $\tau_{t}$ is varied during model fitting.
This will help determine the potential benefits of trace-based activation scaling in these tasks.

### Datasets

For our initial comparisons, we utilize a segment of the PEERS dataset as reported by [@healey2014memory].
In this subset, each of 126 participants, aged 17 to 30, encountered 112 trials where they studied lists of 16 unique words, followed by immediate recall.
The words in each trial were unique and selected for low similarity, randomly drawn from the Toronto Word Pool [@friendly1982toronto], which includes high-frequency nouns, adjectives, and verbs.

One significant aspect of the CMR model is its ability to demonstrate the relative scale-invariance of serial position effects, regardless of list length.
As documented by [@murdock1962serial], variations in list length did not affect the primacy or the shape of the recency effects observed, though they did influence the overall probability of recalling initially encoded items and the list-list asymptote.
To assess whether our multi-trace and composite implementations of CMR can replicate these findings, we utilize a subset of the original behavioral data from [@murdock1962serial] where 15 subjects completed 240 trials with list lengths varying between 20, 30, and 40 words.

Additionally, past research [@lohnas2014retrieved] has highlighted CMR's capacity to model various repetition effects observed in free recall experiments.
N otably, recall sequences often show a spacing effect, where the likelihood of recalling an item increases with the lag between its repeated presentations in a study list.
[@lohnas2014retrieved] demonstrated that CMR could account for this through two mechanisms: contextual variability and study-phase retrieval.
In the study-phase retrieval, when an item appears multiple times, recalling its prior instances and their contexts enriches the associations made with the current presentation.
We test our model implementations against the dataset from [@lohnas2014retrieved], where 35 subjects recalled 48 lists over four sessions.
Each session included unique words, with controlled semantic relatedness below a threshold of .55 according to WAS [@steyvers2005word], and featured four types of list structures:

1.  **Control lists**: Each item appeared only once.
2.  **Pure massed lists**: Items appeared twice consecutively (e.g., 1, 1, 2, 2).
3.  **Pure spaced lists**: Items appeared twice with variable spacings from 1 to 8 positions.
4.  **Mixed lists**: Contained once-presented, massed, and spaced items, with each spacing amount distributed equally.

Through these simulations, we aim to further understand how different memory model architectures influence recall dynamics and whether certain features, like trace-based activation scaling, enhance the model's predictive accuracy and applicability across varied recall scenarios.

### Likelihood-based model optimization and comparison

To determine how accurately each model captures the sequence of items recalled in our experiments, we employ a likelihood-based model comparison technique, as suggested by @kragel2015neural.
This method evaluates model variants based on their precision in predicting the exact order of recalled items.
According to this method, repeated items and intrusions (responses naming items not presented in the list) are excluded from participants' recall sequences.

Here's how it works: For each trial, after simulating the encoding of each item as presented in the study list, the model begins by predicting the likelihood of the first item recalled by the participant.
Subsequently, the model simulates the retrieval of this item, updates its state, and uses this updated context to predict the next recall event—this could either be the retrieval of another item or the termination of the recall session.
This process continues until no further items are recalled.
The probability assigned by the model to each recall event, conditional on prior events in the sequence, is recorded, log-transformed, and summed to derive the log-likelihood for the entire sequence.
Across multiple trials in a dataset, these sequence log-likelihoods are summed to calculate the total log-likelihood for the dataset under a given model and parameter set.
Higher log-likelihoods indicate a model's superior capability to account for the observed data.

Optimizing the parameters for each model to maximize the likelihood of the observed data is achieved using the differential evolution technique [@storn1997differential], implemented in the scipy Python library.
This method involves maintaining a population of potential parameter configurations.
In each iteration, the algorithm mutates each member of this population by combining them with other members in a stochastic manner.
If the new configuration offers an improvement, it replaces the older one; otherwise, it is discarded.
This iterative process helps in progressively refining the parameters until they converge on a configuration that maximizes the observed dataset's log-likelihood.
We conduct optimization at the individual subject level and compare models by examining the distribution of log-likelihood values across subjects, which helps in identifying the most effective model configuration for explaining recall dynamics.

### Summary Statistic Simulation

In our evaluations of model performance, we utilize a set of summary statistics to characterize recall sequences.
To calculate these statistics for the models, we first simulate recall sequences based on the specified model mechanics.
For each unique study list in our dataset, we simulate 1000 trials.
Each trial involves simulating the encoding of each item into memory followed by the stochastic simulation of free recall, which continues according to the calculated probabilities for each recall attempt until the session ends.
The summary statistics for each model are then computed across all these simulations.

Our analysis primarily focuses on four well-documented regularities in free recall tasks that significantly influence model evaluation:

1.  **Serial Position Effect**: This effect is crucial for understanding how the order of items in a study list affects their likelihood of recall. We calculate and visualize the retrieval rate for each item based on its position in the study list. The rates for items presented early in the list help quantify the primacy effect, while the retrieval rates for later items illustrate the recency effect.
2.  **Probability of First Recall**: This statistic measures the rate at which items are retrieved first across recall sequences. This metric helps identify the items that are most likely to initiate the recall process.
3.  **Lag-Contiguity Effect**: This phenomenon, where items close together in the list order tend to be recalled in close sequence, is another focal point of our analysis. To assess this, we employ lag-based conditional response probability (lag-CRP) analyses. The "lag" here refers to the distance between two items in the study list. We calculate the probability of a participant or model making a transition to recall an item at a specific lag, given that such a transition is possible. A high degree of temporal contiguity is indicated by frequent transitions to nearby items and infrequent transitions to items further apart.
4.  **Spacing Effect**: The spacing effect describes how the intervals between item presentations within a study list influence recall probability. We plot the probability of recalling an item based on its lag from the last presentation (or whether it is a novel item). This analysis helps us understand how the model's memory dynamics are influenced by the spacing between repeated items.

We plot simulated data against observed data to evaluate how well the models capture these regularities across different datasets.

### Results

#### Healey & Kahana (2014)

{{< include icmr_tables/HealyKahana2014_Model_Comparison_parameters.md >}}

:   Confidence intervals of parameters fit to data from @healey2014memory, computed across subjects.
    Column 1: Connectionist CMR follows the specification in @morton2016predictive.
    Column 2: Instance CMR with $\tau_{t}$ set to 1.
    Column 3: Instance CMR with $\tau_{t}$ optimized during fitting but $\tau_{c}$ set to 1.
    Column 4: Instance CMR with both $\tau_{t}$ and $\tau_{c}$ optimized during fitting.
    {#tbl-healey tbl-colwidths="\[66, 34\]"}

::: {#fig-healey2014memory layout="[[1,1,1], [1,1,1], [1,1,1], [1,1,1]]"}
![](icmr_figures/HealyKahana2014_ConnectionistCMR_Model_Fitting_crp-1.png)

![](icmr_figures/HealyKahana2014_ConnectionistCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/HealyKahana2014_ConnectionistCMR_Model_Fitting_spc-1.png)

![](icmr_figures/HealyKahana2014_InstanceCMR_Model_Fitting_crp-1.png)

![](icmr_figures/HealyKahana2014_InstanceCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/HealyKahana2014_InstanceCMR_Model_Fitting_spc-1.png)

![](icmr_figures/HealyKahana2014_TraceScalingCMR_Model_Fitting_crp-1.png)

![](icmr_figures/HealyKahana2014_TraceScalingCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/HealyKahana2014_TraceScalingCMR_Model_Fitting_spc-1.png)

![](icmr_figures/HealyKahana2014_MultiScalingCMR_Model_Fitting_crp-1.png)

![](icmr_figures/HealyKahana2014_MultiScalingCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/HealyKahana2014_MultiScalingCMR_Model_Fitting_spc-1.png)

Summary statistic fits to @healey2014memory.
Top: Connectionist CMR. Second Row: Instance CMR, $\tau_{t}$ set to 1.
Third Row: Trace Scaling CMR -- Instance CMR, $\tau_{c}$ set to 1 and $\tau_{t}$ optimized during fitting.
Fourth Row: Multi Scaling CMR -- Instance CMR, both $\tau_{t}$ and $\tau_{c}$ optimized during fitting.
Left: conditional response probability as a function of lag.
Middle: probability of starting recall by serial position.
Right: recall probability by serial position.
:::

{{< pagebreak >}}

#### Murdock (1962)

{{< include icmr_tables/Murdock1962_Model_Comparison_parameters.md >}}

:   Confidence intervals of parameters fit to data from @murdock1962serial, computed across subjects.
    Column 1: Connectionist CMR follows the specification in @morton2016predictive.
    Column 2: Instance CMR with trace activation scaling turned off.
    Column 3: Instance CMR with only trace activation scaling and no item activation scaling.
    Fourth model Column 4: Instance CMR with both trace and item activation scaling parameters freed for fitting.
    {#tbl-murdock tbl-colwidths="\[66, 34\]"}

::: {#fig-murdock1962memory20 layout="[[1,1,1], [1,1,1], [1,1,1], [1,1,1]]"}
![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL20_crp-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL20_pnr-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL20_spc-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL20_crp-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL20_pnr-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL20_spc-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL20_crp-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL20_pnr-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL20_spc-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL20_crp-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL20_pnr-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL20_spc-1.png)

Summary statistic fits to @murdock1962serial where list length = 20.
Top: Connectionist CMR. Second Row: Instance CMR, $\tau_{t}$ set to 1.
Third Row: Trace Scaling CMR -- Instance CMR, $\tau_{c}$ set to 1 and $\tau_{t}$ optimized during fitting.
Fourth Row: Multi Scaling CMR -- Instance CMR, both $\tau_{t}$ and $\tau_c$ optimized during fitting.
Left: conditional response probability as a function of lag.
Middle: probability of starting recall with each serial position.
Right: recall probability as a function of serial position.
:::

::: {#fig-murdock1962memory30 layout="[[1,1,1], [1,1,1], [1,1,1], [1,1,1]]"}
![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL30_crp-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL30_pnr-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL30_spc-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL30_crp-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL30_pnr-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL30_spc-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL30_crp-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL30_pnr-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL30_spc-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL30_crp-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL30_pnr-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL30_spc-1.png)

Summary statistic fits to @murdock1962serial where list length = 30.
Top: Connectionist CMR. Second Row: Instance CMR, $\tau_{t}$ set to 1.
Third Row: Trace Scaling CMR -- Instance CMR, $\tau_{c}$ set to 1 and $\tau_{t}$ optimized during fitting.
Fourth Row: Multi Scaling CMR -- Instance CMR, both $\tau_t$ and $\tau_c$ freed for fitting.
Left: conditional response probability as a function of lag.
Middle: probability of starting recall with each serial position.
Right: recall probability as a function of serial position.
:::

::: {#fig-murdock1962memory40 layout="[[1,1,1], [1,1,1], [1,1,1], [1,1,1]]"}
![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL40_crp-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL40_pnr-1.png)

![](icmr_figures/Murdock1962_ConnectionistCMR_Model_Fitting_LL40_spc-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL40_crp-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL40_pnr-1.png)

![](icmr_figures/Murdock1962_InstanceCMR_Model_Fitting_LL40_spc-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL40_crp-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL40_pnr-1.png)

![](icmr_figures/Murdock1962_TraceScalingCMR_Model_Fitting_LL40_spc-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL40_crp-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL40_pnr-1.png)

![](icmr_figures/Murdock1962_MultiScalingCMR_Model_Fitting_LL40_spc-1.png)

Summary statistic fits to @murdock1962serial where list length = 40.
Top: Connectionist CMR. Second Row: Instance CMR, $\tau_{t}$ set to 1.
Third Row: Trace Scaling CMR -- Instance CMR, $\tau_{c}$ set to 1 and $\tau_{t}$ optimized during fitting.
Fourth Row: Multi Scaling CMR -- Instance CMR, both $\tau_t$ and $\tau_c$ freed for fitting.
Left: conditional response probability as a function of lag.
Middle: probability of starting recall with each serial position.
Right: recall probability as a function of serial position.
:::

{{< pagebreak >}}

#### Lohnas & Kahana (2014)

{{< include icmr_tables/LohnasKahana2014_Model_Comparison_parameters.md >}}

:   Confidence intervals of parameters fit to data from @lohnas2014retrieved, computed across subjects.
    Column 1: Connectionist CMR follows the specification in @morton2016predictive.
    Second Row: Instance CMR, with $\tau_{t}$ set to 1.
    Third Row: Trace Scaling CMR -- Instance CMR with $\tau_{t}$ set to 1 and $\tau_{c}$ optimized during fitting.
    Fourth model Column 4: Instance CMR with both trace and item activation scaling parameters freed for fitting.
    {#tbl-lohnas tbl-colwidths="\[66, 34\]"}

::: {#fig-lohnas2014memory layout="[[1,1,1], [1,1,1], [1,1,1], [1,1,1]]"}
![](icmr_figures/LohnasKahana2014_ConnectionistCMR_Model_Fitting_crp-1.png)

![](icmr_figures/LohnasKahana2014_ConnectionistCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/LohnasKahana2014_ConnectionistCMR_Model_Fitting_spc-1.png)

![](icmr_figures/LohnasKahana2014_InstanceCMR_Model_Fitting_crp-1.png)

![](icmr_figures/LohnasKahana2014_InstanceCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/LohnasKahana2014_InstanceCMR_Model_Fitting_spc-1.png)

![](icmr_figures/LohnasKahana2014_TraceScalingCMR_Model_Fitting_crp-1.png)

![](icmr_figures/LohnasKahana2014_TraceScalingCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/LohnasKahana2014_TraceScalingCMR_Model_Fitting_spc-1.png)

![](icmr_figures/LohnasKahana2014_MultiScalingCMR_Model_Fitting_crp-1.png)

![](icmr_figures/LohnasKahana2014_MultiScalingCMR_Model_Fitting_pfr-1.png)

![](icmr_figures/LohnasKahana2014_MultiScalingCMR_Model_Fitting_spc-1.png)

Summary statistic fits to @lohnas2014retrieved.
Top: Connectionist CMR. Second Row: Instance CMR, $\tau_{t}$ set to 1.
Third Row: Trace Scaling CMR -- Instance CMR, $\tau_{c}$ set to 1 and $\tau_{t}$ optimized during fitting.
Fourth Row: Multi Scaling CMR -- Instance CMR, both $\tau_t$ and $\tau_c$ freed for fitting.
Left: conditional response probability as a function of lag.
Middle: probability of starting recall with each serial position.
Right: recall probability as a function of serial position.
:::

@tbl-healey, @tbl-murdock, and @tbl-lohnas present the confidence intervals of the parameters fit to the data across subjects for each model variant.
The tables also present the mean and confidence intervals of the log-likelihood values for each model variant.
@fig-healey2014memory, @fig-murdock1962memory20, and @fig-murdock1962memory30 illustrate the summary statistic fits for each model variant.
We additionally performed paired t-tests to compare the log-likelihood values of the models across subjects, using a significance level of $0.05$ and Connectionist CMR as the reference model.
Alternative models Instance-CMR, Trace Scaling CMR, and Multi Scaling CMR were compared to Connectionist CMR to determine if subject-level log-likelihoods were significantly lower than the reference model (lower scores indicate better fit), with outcome p-values \~0.0176, \~0.0232, and \~0.0008, respectively.
Performing the same tests for the data from @murdock1962serial yielded p-values 0.9125, 0.9414, and 0.6252 for the three alternative models, respectively.
For the data from @lohnas2014retrieved, the p-values were 0.8638, 0.1015, and 0.4374 for the three alternative models, respectively.
These results suggest that the Multi Scaling CMR model provides the best fit to the data across all three datasets, with the Instance CMR model performing similarly to the Connectionist CMR model.
However, even the best-fitting model, Multi Scaling CMR, only marginally outperforms Connectionist CMR in terms of log-likelihood values and capture of summary statistics across all datasets.

These several figures help illustrate a single point: the functional equivalence between Connectionist and Instance CMR models.
When the trace-based activation scaling parameter $\tau_{t}$ is set to 1, both models yield the same results in free recall tasks, with differences in results only attributable to the stochastic nature of the simulations and fitting process.
Even when $\tau_{t}$ is varied during model fitting, either instead of or in addition to the item activation scaling parameter $\tau_{c}$, model performance is only marginally different.
The challenges that the original Connectionist CMR faces in capturing the dynamics of memory search in free recall tasks are shared by Instance CMR, and are not clearly resolved by the latter model.
For example, both models overrate the strength of the lag-contiguity effect in the dataset from @murdock1962serial, and lack the ability to capture the plateau-like shape of the probability of first recall curve in data both from @murdock1962serial and @lohnas2014retrieved.
These results suggest that the instance-based specification of CMR does not provide a clear advantage over the connectionist specification in terms of capturing the dynamics of memory search in free recall tasks -- at least not without additional modifications to the model or novel task conditions.

## Discussion

In light of a collection of results across task domains distinguishing between the performance of prototype- and instance-based models of memory-based behavior, we searched for evidence of a similar distinction with respect to the free recall task paradigm.
To do this, we specified and compared the original connectionist implementation of an established model of task performance based in retrieved context theory (Connectionist CMR) against a parallel instance-based variant (Instance CMR) across diverse experimental conditions, including a dataset featuring variable study list lengths across trials and a dataset featuring item repetitions within trials and variable item repetition spacing between trials.
While our results identify some marginal differences in model performance across datasets, they do not provide clear evidence of a performance distinction between instance- and instance-based models in the free recall task paradigm.
The model variants accounted for human performance on the free recall task with similar effectiveness in each dataset considered.
In addition, the results of our semantic extension of Connectionist and Instance CMR models will further underscore the portability of retrieved context theory across model architectures.
We show that the concatenation of memory traces in a single memory structure is equivalent to a linear combination of the separate retrieval of traces from two memory structures, identify the mathematical basis for this equivalence, and provide a foundation for future integrations of CMR with other cognitive models.

Altogether these results demonstrate that the success of the theoretical commitments made by the Context Maintenance and Retrieval (CMR) model and models like it are likely not especially dependent on the architectures in which they are implemented.
Instead, the main insights of retrieved context theories (at least as formalized by CMR) are highly portable, and can likely be esconced within any reasonable model architecture where memory search via temporal contextual representations might prove valuable.
Establishing the portability of these successful theoretical principles across modeling approaches helps advance the historical effort in cognitive science to develop "a general account of memory and its processes in a working computational system to produce a common explanation of behavior rather than a set of lab-specific and domain-specific theories for different behaviors" [@newell1973you; @jamieson2018instance].

An instance-based specification of CMR does not provide a clear advantage over the connectionist specification in terms of capturing the dynamics of memory search in free recall tasks -- at least not without additional modifications to the model or novel task conditions.
But a few features unique to the instance-based specification of CMR may still prove valuable in future research.
Throughout this work, we emphasized one of these: the instance-based specification of CMR is more easily integrated with other instance-based models of memory, potentially supporting lines of research that seek to combine the strengths of different models to provide a more comprehensive account of memory performance across different tasks.

Another feature of instance-based memory models that may prove valuable in future research is their interpretability, the degree to which the model's parameters and internal representations can be understood and reasoned about [@gao2023interpretability].
Because instance models do not compress information about the history of an item into a single vector, they can be more easily interpreted than connectionist models.
Each trace is represented separately and its weighting in the activation function is also calculated separately, making the contribution of each trace to the model's output transparent in its internal structure.
These features may make instance-based models favorable for research that seeks to diagnose and address the limitations of existing models, or to develop new models that can capture the dynamics of memory search in free recall tasks more effectively.

The most emphasized advantage of instance models in the literature though is their ability to capture the dynamics of memory search in free recall tasks more effectively than connectionist models via its use of trace-based activation scaling in domains such as category learning and semantic memory [@nosofsky2002exemplar; @jamieson2018instance].
Why does this advantage not appear when comparing Connectionist and Instance CMR in the free recall task?

One key reason is that Connectionist CMR has some features that allow it to exhibit instance-memory-like behavior in some aspects of its dynamics.
First, the item representations on its $F$ layer are orthonormal -- each item's representation is unique and does not overlap with any other item's representation.
This means that even without the trace-based activation scaling unique to instance models, the model can still selectively activate the representation of a single item in the $F$ layer without activating the representations of other items, making Instance $M^{FC}$ and Connectionist $M^{FC}$ equivalent for all values of $\tau_{t}$.
Similarities between $\tau_{t}$ in the instance-based model and $\tau_{c}$ in the connectionist model also contribute to the similarity in model performance.
Despite some fundamental differences, both activation scaling mechanisms help to focus the model's attention on the most relevant traces in memory, perhaps enough to in a broad range of cases obscure the differences in model performance that might be expected from the different architectures.

A more fundamental issue for comparison between Instance and Connectionist CMR is that the free recall task and design of CMR might not instantiate a clear instance vs prototype model distinction that has set up previous successes of instance models in other domains.
In these other domains, experiments were configured to support clear contrasts between prototype-based decision-making where the central tendency of a class drives responses, and instance-based decision-making where the the similarity of a probe to a specific study instance drives responses.
The free recall task is designed to test the ability of a model to retrieve items from memory in a specific order, and the dynamics of memory search in this task may not be as sensitive to the differences between instance and prototype models as other tasks.
However, future variations of the task that introduce category structure as a major factor in recall, or that require participants to make decisions based on the category membership, might provide a clearer test of the distinction between instance and prototype models in the domain of free recall.
These results highlight that the advantage of instance- over prototype- models in accounting for memory performance is highly contingent on the task and conditions under which models are tested.