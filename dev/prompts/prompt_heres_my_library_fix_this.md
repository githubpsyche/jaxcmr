Here's my current jaxcmr library:

└── jaxcmr
    ├── __init__.py
    ├── _modidx.py
    ├── cmr.py
    ├── context.py
    ├── crp.py
    ├── experimental
        ├── __init__.py
        ├── array.py
        ├── correct_likelihood.py
        ├── decision_probability.py
        ├── plotting.py
        └── repetition.py
    ├── fitting.py
    ├── helpers.py
    ├── instance_memory.py
    ├── likelihood.py
    ├── linear_memory.py
    ├── math.py
    ├── pnr.py
    ├── repcrp.py
    ├── simulation.py
    ├── spc.py
    ├── srac.py
    ├── state_analysis.py
    ├── summarize.py
    └── typing.py


/jaxcmr/__init__.py:
--------------------------------------------------------------------------------
1 | __version__ = "0.0.1"
2 | 


--------------------------------------------------------------------------------
/jaxcmr/_modidx.py:
--------------------------------------------------------------------------------
 1 | # Autogenerated by nbdev
 2 | 
 3 | d = { 'settings': { 'branch': 'main',
 4 |                 'doc_baseurl': '/jaxcmr',
 5 |                 'doc_host': 'https://githubpsyche.github.io',
 6 |                 'git_url': 'https://github.com/githubpsyche/jaxcmr',
 7 |                 'lib_path': 'jaxcmr'},
 8 |   'syms': { 'jaxcmr.cmr': {},
 9 |             'jaxcmr.context': { 'jaxcmr.context.TemporalContext': ('context.html#temporalcontext', 'jaxcmr/context.py'),
10 |                                 'jaxcmr.context.TemporalContext.__init__': ('context.html#temporalcontext.__init__', 'jaxcmr/context.py'),
11 |                                 'jaxcmr.context.TemporalContext._repr_markdown_': ( 'context.html#temporalcontext._repr_markdown_',
12 |                                                                                     'jaxcmr/context.py'),
13 |                                 'jaxcmr.context.TemporalContext.init': ('context.html#temporalcontext.init', 'jaxcmr/context.py'),
14 |                                 'jaxcmr.context.TemporalContext.integrate': ( 'context.html#temporalcontext.integrate',
15 |                                                                               'jaxcmr/context.py')},
16 |             'jaxcmr.crp': { 'jaxcmr.crp.SimpleTabulation': ('crp.html#simpletabulation', 'jaxcmr/crp.py'),
17 |                             'jaxcmr.crp.SimpleTabulation.__init__': ('crp.html#simpletabulation.__init__', 'jaxcmr/crp.py'),
18 |                             'jaxcmr.crp.SimpleTabulation._update': ('crp.html#simpletabulation._update', 'jaxcmr/crp.py'),
19 |                             'jaxcmr.crp.SimpleTabulation.update': ('crp.html#simpletabulation.update', 'jaxcmr/crp.py'),
20 |                             'jaxcmr.crp.Tabulation': ('crp.html#tabulation', 'jaxcmr/crp.py'),
21 |                             'jaxcmr.crp.Tabulation.__init__': ('crp.html#tabulation.__init__', 'jaxcmr/crp.py'),
22 |                             'jaxcmr.crp.Tabulation.available_lags_from': ('crp.html#tabulation.available_lags_from', 'jaxcmr/crp.py'),
23 |                             'jaxcmr.crp.Tabulation.available_recalls_after': ( 'crp.html#tabulation.available_recalls_after',
24 |                                                                                'jaxcmr/crp.py'),
25 |                             'jaxcmr.crp.Tabulation.lags_from_previous': ('crp.html#tabulation.lags_from_previous', 'jaxcmr/crp.py'),
26 |                             'jaxcmr.crp.Tabulation.tabulate': ('crp.html#tabulation.tabulate', 'jaxcmr/crp.py'),
27 |                             'jaxcmr.crp.Tabulation.tabulate_actual_lags': ('crp.html#tabulation.tabulate_actual_lags', 'jaxcmr/crp.py'),
28 |                             'jaxcmr.crp.Tabulation.tabulate_available_lags': ( 'crp.html#tabulation.tabulate_available_lags',
29 |                                                                                'jaxcmr/crp.py'),
30 |                             'jaxcmr.crp.crp': ('crp.html#crp', 'jaxcmr/crp.py'),
31 |                             'jaxcmr.crp.plot_crp': ('crp.html#plot_crp', 'jaxcmr/crp.py'),
32 |                             'jaxcmr.crp.set_false_at_index': ('crp.html#set_false_at_index', 'jaxcmr/crp.py'),
33 |                             'jaxcmr.crp.simple_crp': ('crp.html#simple_crp', 'jaxcmr/crp.py'),
34 |                             'jaxcmr.crp.simple_tabulate_trial': ('crp.html#simple_tabulate_trial', 'jaxcmr/crp.py'),
35 |                             'jaxcmr.crp.tabulate_trial': ('crp.html#tabulate_trial', 'jaxcmr/crp.py')},
36 |             'jaxcmr.experimental.array': {},
37 |             'jaxcmr.experimental.correct_likelihood': {},
38 |             'jaxcmr.experimental.decision_probability': {},
39 |             'jaxcmr.experimental.plotting': {},
40 |             'jaxcmr.experimental.repetition': {},
41 |             'jaxcmr.fitting': {},
42 |             'jaxcmr.helpers': {},
43 |             'jaxcmr.instance_memory': {},
44 |             'jaxcmr.likelihood': {},
45 |             'jaxcmr.linear_memory': {},
46 |             'jaxcmr.math': {},
47 |             'jaxcmr.pnr': { 'jaxcmr.pnr.fixed_pres_pnr': ('pnr.html#fixed_pres_pnr', 'jaxcmr/pnr.py'),
48 |                             'jaxcmr.pnr.plot_pnr': ('pnr.html#plot_pnr', 'jaxcmr/pnr.py'),
49 |                             'jaxcmr.pnr.pnr': ('pnr.html#pnr', 'jaxcmr/pnr.py')},
50 |             'jaxcmr.repcrp': { 'jaxcmr.repcrp.RepCRPTabulation': ('repcrp.html#repcrptabulation', 'jaxcmr/repcrp.py'),
51 |                                'jaxcmr.repcrp.RepCRPTabulation.__init__': ('repcrp.html#repcrptabulation.__init__', 'jaxcmr/repcrp.py'),
52 |                                'jaxcmr.repcrp.RepCRPTabulation.available_lags_from': ( 'repcrp.html#repcrptabulation.available_lags_from',
53 |                                                                                        'jaxcmr/repcrp.py'),
54 |                                'jaxcmr.repcrp.RepCRPTabulation.available_recalls_after': ( 'repcrp.html#repcrptabulation.available_recalls_after',
55 |                                                                                            'jaxcmr/repcrp.py'),
56 |                                'jaxcmr.repcrp.RepCRPTabulation.conditional_tabulate': ( 'repcrp.html#repcrptabulation.conditional_tabulate',
57 |                                                                                         'jaxcmr/repcrp.py'),
58 |                                'jaxcmr.repcrp.RepCRPTabulation.lags_from_previous': ( 'repcrp.html#repcrptabulation.lags_from_previous',
59 |                                                                                       'jaxcmr/repcrp.py'),
60 |                                'jaxcmr.repcrp.RepCRPTabulation.should_tabulate': ( 'repcrp.html#repcrptabulation.should_tabulate',
61 |                                                                                    'jaxcmr/repcrp.py'),
62 |                                'jaxcmr.repcrp.RepCRPTabulation.tabulate': ('repcrp.html#repcrptabulation.tabulate', 'jaxcmr/repcrp.py'),
63 |                                'jaxcmr.repcrp.RepCRPTabulation.tabulate_actual_lags': ( 'repcrp.html#repcrptabulation.tabulate_actual_lags',
64 |                                                                                         'jaxcmr/repcrp.py'),
65 |                                'jaxcmr.repcrp.RepCRPTabulation.tabulate_available_lags': ( 'repcrp.html#repcrptabulation.tabulate_available_lags',
66 |                                                                                            'jaxcmr/repcrp.py'),
67 |                                'jaxcmr.repcrp.plot_difference_rep_crp': ('repcrp.html#plot_difference_rep_crp', 'jaxcmr/repcrp.py'),
68 |                                'jaxcmr.repcrp.plot_first_rep_crp': ('repcrp.html#plot_first_rep_crp', 'jaxcmr/repcrp.py'),
69 |                                'jaxcmr.repcrp.plot_rep_crp': ('repcrp.html#plot_rep_crp', 'jaxcmr/repcrp.py'),
70 |                                'jaxcmr.repcrp.plot_second_rep_crp': ('repcrp.html#plot_second_rep_crp', 'jaxcmr/repcrp.py'),
71 |                                'jaxcmr.repcrp.repcrp': ('repcrp.html#repcrp', 'jaxcmr/repcrp.py'),
72 |                                'jaxcmr.repcrp.set_false_at_index': ('repcrp.html#set_false_at_index', 'jaxcmr/repcrp.py'),
73 |                                'jaxcmr.repcrp.tabulate_trial': ('repcrp.html#tabulate_trial', 'jaxcmr/repcrp.py')},
74 |             'jaxcmr.simulation': {},
75 |             'jaxcmr.spc': { 'jaxcmr.spc.fixed_pres_spc': ('spc.html#fixed_pres_spc', 'jaxcmr/spc.py'),
76 |                             'jaxcmr.spc.plot_spc': ('spc.html#plot_spc', 'jaxcmr/spc.py'),
77 |                             'jaxcmr.spc.spc': ('spc.html#spc', 'jaxcmr/spc.py')},
78 |             'jaxcmr.srac': { 'jaxcmr.srac.fixed_pres_srac': ('srac.html#fixed_pres_srac', 'jaxcmr/srac.py'),
79 |                              'jaxcmr.srac.plot_srac': ('srac.html#plot_srac', 'jaxcmr/srac.py'),
80 |                              'jaxcmr.srac.srac': ('srac.html#srac', 'jaxcmr/srac.py')},
81 |             'jaxcmr.state_analysis': {},
82 |             'jaxcmr.summarize': {},
83 |             'jaxcmr.typing': {}}}
84 | 


--------------------------------------------------------------------------------
/jaxcmr/cmr.py:
--------------------------------------------------------------------------------
  1 | from typing import Mapping, Optional
  2 | 
  3 | import numpy as np
  4 | from jax import lax
  5 | from jax import numpy as jnp
  6 | from simple_pytree import Pytree
  7 | 
  8 | from jaxcmr.context import TemporalContext
  9 | from jaxcmr.instance_memory import InstanceMemory
 10 | from jaxcmr.linear_memory import LinearMemory
 11 | from jaxcmr.math import exponential_primacy_decay, exponential_stop_probability, lb
 12 | from jaxcmr.typing import (
 13 |     Array,
 14 |     Context,
 15 |     Float,
 16 |     Float_,
 17 |     Int_,
 18 |     Integer,
 19 |     Memory,
 20 |     MemorySearch,
 21 | )
 22 | 
 23 | 
 24 | class CMR(Pytree):
 25 |     """The Context Maintenance and Retrieval (CMR) model of memory search."""
 26 | 
 27 |     def __init__(
 28 |         self,
 29 |         list_length: int,
 30 |         parameters: Mapping[str, Float_],
 31 |         mfc: Memory,
 32 |         mcf: Memory,
 33 |         context: Context,
 34 |     ):
 35 |         self.encoding_drift_rate = parameters["encoding_drift_rate"]
 36 |         self.start_drift_rate = parameters["start_drift_rate"]
 37 |         self.recall_drift_rate = parameters["recall_drift_rate"]
 38 |         self.shared_support = parameters["shared_support"]
 39 |         self.item_support = parameters["item_support"]
 40 |         self.primacy_scale = parameters["primacy_scale"]
 41 |         self.primacy_decay = parameters["primacy_decay"]
 42 |         self.mfc_learning_rate = parameters["learning_rate"]
 43 |         self.stop_probability_scale = parameters["stop_probability_scale"]
 44 |         self.stop_probability_growth = parameters["stop_probability_growth"]
 45 |         self.mcf_sensitivity = parameters["choice_sensitivity"]
 46 |         self.item_count = list_length
 47 |         self.items = jnp.eye(self.item_count)
 48 |         self._stop_probability = exponential_stop_probability(
 49 |             self.stop_probability_scale,
 50 |             self.stop_probability_growth,
 51 |             jnp.arange(self.item_count),
 52 |         )
 53 |         self._mcf_learning_rate = exponential_primacy_decay(
 54 |             jnp.arange(list_length), self.primacy_scale, self.primacy_decay
 55 |         )
 56 |         self.context = context
 57 |         self.mfc = mfc
 58 |         self.mcf = mcf
 59 |         self.recalls = jnp.zeros(self.item_count, dtype=int)
 60 |         self.recallable = jnp.zeros(self.item_count, dtype=bool)
 61 |         self.is_active = jnp.array(True)
 62 |         self.recall_total = jnp.array(0, dtype=int)
 63 |         self.study_index = jnp.array(0, dtype=int)
 64 | 
 65 |     @property
 66 |     def mcf_learning_rate(self) -> Float[Array, ""]:
 67 |         """The learning rate for the MCF memory under its current state."""
 68 |         return self._mcf_learning_rate[self.study_index]
 69 | 
 70 |     def experience_item(self, item_index: Int_) -> "CMR":
 71 |         """Return the model after experiencing item with the specified index.
 72 | 
 73 |         Args:
 74 |             item_index: the index of the item to experience. 0-indexed.
 75 |         """
 76 |         item = self.items[item_index]
 77 |         context_input = self.mfc.probe(item)
 78 |         new_context = self.context.integrate(context_input, self.encoding_drift_rate)
 79 |         return self.replace(
 80 |             context=new_context,
 81 |             mfc=self.mfc.associate(item, new_context.state, self.mfc_learning_rate),
 82 |             mcf=self.mcf.associate(new_context.state, item, self.mcf_learning_rate),
 83 |             recallable=self.recallable.at[item_index].set(True),
 84 |             study_index=self.study_index + 1,
 85 |         )
 86 | 
 87 |     def experience(self, choice: Int_) -> "CMR":
 88 |         """Returns model after simulating the specified study event.
 89 | 
 90 |         Args:
 91 |             choice: the index of the item to experience (1-indexed). 0 is ignored.
 92 |         """
 93 |         return lax.cond(
 94 |             choice == 0,
 95 |             lambda: self,
 96 |             lambda: self.experience_item(choice - 1),
 97 |         )
 98 | 
 99 |     def start_retrieving(self) -> "CMR":
100 |         """Returns model after transitioning from study to retrieval mode."""
101 |         start_input = self.context.initial_state
102 |         start_context = self.context.integrate(start_input, self.start_drift_rate)
103 |         return self.replace(context=start_context)
104 | 
105 |     def retrieve_item(self, item_index: Int_) -> "CMR":
106 |         """Return model after simulating retrieval of item with the specified index.
107 | 
108 |         Args:
109 |             choice: the index of the item to retrieve (0-indexed)
110 |         """
111 |         new_context = self.context.integrate(
112 |             self.mfc.probe(self.items[item_index]),
113 |             self.recall_drift_rate,
114 |         )
115 |         return self.replace(
116 |             context=new_context,
117 |             recalls=self.recalls.at[self.recall_total].set(item_index + 1),
118 |             recallable=self.recallable.at[item_index].set(False),
119 |             recall_total=self.recall_total + 1,
120 |         )
121 | 
122 |     def retrieve(self, choice: Int_) -> "CMR":
123 |         """Return model after simulating the specified retrieval event.
124 | 
125 |         Args:
126 |             choice: the index of the item to retrieve (1-indexed) or 0 to stop.
127 |         """
128 |         return lax.cond(
129 |             choice == 0,
130 |             lambda: self.replace(is_active=False),
131 |             lambda: self.retrieve_item(choice - 1),
132 |         )
133 | 
134 |     def activations(self) -> Float[Array, " item_count"]:
135 |         """Returns relative support for retrieval of each item given model state"""
136 |         item_activations = self.mcf.probe(self.context.state) + lb
137 |         return item_activations * self.recallable  # mask recalled items
138 | 
139 |     def stop_probability(self) -> Float[Array, ""]:
140 |         """Returns probability of stopping retrieval given model state"""
141 |         total_recallable = jnp.sum(self.recallable)
142 |         return lax.cond(
143 |             total_recallable == 0,
144 |             true_fun=lambda: 1.0,
145 |             false_fun=lambda: lax.cond(
146 |                 self.is_active,
147 |                 true_fun=lambda: jnp.minimum(
148 |                     1.0 - (lb * total_recallable),
149 |                     self._stop_probability[self.recall_total],
150 |                 ),
151 |                 false_fun=lambda: 1.0,
152 |             ),
153 |         )
154 | 
155 |     def item_probability(self, item_index: Int_) -> Float[Array, ""]:
156 |         """Return the probability of retrieval of an item at the specified index.
157 | 
158 |         Assumes that some items are recallable, with at least the minimum recall probability.
159 | 
160 |         Args:
161 |             item_index: the index of the item to retrieve.
162 |         """
163 |         p_continue = 1 - self.stop_probability()
164 |         item_activations = self.activations()
165 |         return p_continue * (item_activations[item_index] / jnp.sum(item_activations))
166 | 
167 |     def outcome_probability(self, choice: Int_) -> Float[Array, ""]:
168 |         """Return probability of the specified retrieval event.
169 | 
170 |         Args:
171 |             choice: the index of the item to retrieve (1-indexed) or 0 to stop.
172 |         """
173 |         p_stop = self.stop_probability()
174 |         return lax.cond(
175 |             choice == 0,
176 |             lambda: p_stop,
177 |             lambda: lax.cond(
178 |                 p_stop == 1.0,
179 |                 lambda: 0.0,
180 |                 lambda: self.item_probability(choice - 1),
181 |             ),
182 |         )
183 | 
184 |     def outcome_probabilities(self) -> Float[Array, " recall_outcomes"]:
185 |         """Return the outcome probabilities of all recall events."""
186 |         p_stop = self.stop_probability()
187 |         item_activation = self.activations()
188 |         item_activation_sum = jnp.sum(item_activation)
189 |         return jnp.hstack(
190 |             (
191 |                 p_stop,
192 |                 (
193 |                     (1 - p_stop)
194 |                     * item_activation
195 |                     / lax.select(item_activation_sum == 0, 1.0, item_activation_sum)
196 |                 ),
197 |             )
198 |         )
199 | 
200 | 
201 | def BaseCMR(list_length: int, parameters: Mapping[str, Float_]) -> CMR:
202 |     """Creates a regular CMR model with linear associative $M^{FC}$ and $M^{CF}$ memories."""
203 |     context = TemporalContext.init(list_length)
204 |     mfc = LinearMemory.init_mfc(
205 |         list_length,
206 |         context.size,
207 |         parameters["learning_rate"],
208 |         parameters.get("mfc_choice_sensitivity", 1.0),
209 |     )
210 |     mcf = LinearMemory.init_mcf(
211 |         list_length,
212 |         context.size,
213 |         parameters["item_support"],
214 |         parameters["shared_support"],
215 |         parameters["choice_sensitivity"],
216 |     )
217 |     return CMR(list_length, parameters, mfc, mcf, context)
218 | 
219 | 
220 | def InstanceCMR(list_length: int, parameters: Mapping[str, Float_]) -> CMR:
221 |     """
222 |     Creates InstanceCMR model with instance-based $M^{FC}$ and $M^{CF}$ memories.
223 | 
224 |     Equivalent to the original CMR model when `mcf_trace_sensitivity` is set to 1.0.
225 |     Usually slower than the linear version, but often more interpretable and flexible.
226 |     """
227 |     context = TemporalContext.init(list_length)
228 |     mfc = InstanceMemory.init_mfc(
229 |         list_length,
230 |         context.size,
231 |         list_length,
232 |         parameters["learning_rate"],
233 |         parameters.get("mfc_choice_sensitivity", 1.0),
234 |         parameters.get("mfc_trace_sensitivity", 1.0),
235 |     )
236 |     mcf = InstanceMemory.init_mcf(
237 |         list_length,
238 |         context.size,
239 |         list_length,
240 |         parameters["item_support"],
241 |         parameters["shared_support"],
242 |         parameters["choice_sensitivity"],
243 |         parameters["mcf_trace_sensitivity"],
244 |     )
245 |     return CMR(list_length, parameters, mfc, mcf, context)
246 | 
247 | 
248 | def MixedCMR(list_length: int, parameters: Mapping[str, Float_]) -> CMR:
249 |     """
250 |     Creates MixedCMR model with linear $M^{FC}$ and instance-based $M^{CF}$ memories.
251 | 
252 |     Equivalent to InstanceCMR but faster feature-to-context memory.
253 |     """
254 |     context = TemporalContext.init(list_length)
255 |     mfc = LinearMemory.init_mfc(
256 |         list_length,
257 |         context.size,
258 |         parameters["learning_rate"],
259 |         parameters.get("mfc_choice_sensitivity", 1.0),
260 |     )
261 |     mcf = InstanceMemory.init_mcf(
262 |         list_length,
263 |         context.size,
264 |         list_length,
265 |         parameters["item_support"],
266 |         parameters["shared_support"],
267 |         parameters["choice_sensitivity"],
268 |         parameters["mcf_trace_sensitivity"],
269 |     )
270 |     return CMR(list_length, parameters, mfc, mcf, context)
271 | 
272 | 
273 | class BaseCMRFactory:
274 |     def __init__(
275 |         self,
276 |         dataset: dict[str, Integer[Array, " trials ?"]],
277 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
278 |     ) -> None:
279 |         """Initialize the factory with the specified trials and trial data."""
280 |         self.max_list_length = np.max(dataset["listLength"]).item()
281 | 
282 |     def create_model(
283 |         self,
284 |         trial_index: Int_,
285 |         parameters: Mapping[str, Float_],
286 |     ) -> MemorySearch:
287 |         """Create a new memory search model with the specified parameters for the specified trial."""
288 |         return BaseCMR(self.max_list_length, parameters)
289 | 
290 | 
291 | class InstanceCMRFactory:
292 |     def __init__(
293 |         self,
294 |         dataset: dict[str, Integer[Array, " trials ?"]],
295 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
296 |     ) -> None:
297 |         """Initialize the factory with the specified trials and trial data."""
298 |         self.max_list_length = np.max(dataset["listLength"]).item()
299 | 
300 |     def create_model(
301 |         self,
302 |         trial_index: Int_,
303 |         parameters: Mapping[str, Float_],
304 |     ) -> MemorySearch:
305 |         """Create a new memory search model with the specified parameters for the specified trial."""
306 |         return InstanceCMR(self.max_list_length, parameters)
307 | 
308 | 
309 | class MixedCMRFactory:
310 |     def __init__(
311 |         self,
312 |         dataset: dict[str, Integer[Array, " trials ?"]],
313 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
314 |     ) -> None:
315 |         """Initialize the factory with the specified trials and trial data."""
316 |         self.max_list_length = np.max(dataset["listLength"]).item()
317 | 
318 |     def create_model(
319 |         self,
320 |         trial_index: Int_,
321 |         parameters: Mapping[str, Float_],
322 |     ) -> MemorySearch:
323 |         """Create a new memory search model with the specified parameters for the specified trial."""
324 |         return MixedCMR(self.max_list_length, parameters)
325 | 


--------------------------------------------------------------------------------
/jaxcmr/context.py:
--------------------------------------------------------------------------------
 1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/context.ipynb.
 2 | 
 3 | # %% auto 0
 4 | __all__ = ['TemporalContext']
 5 | 
 6 | # %% ../notebooks/context.ipynb 2
 7 | import base64
 8 | import io
 9 | 
10 | import matplotlib.pyplot as plt
11 | from jax import numpy as jnp
12 | from simple_pytree import Pytree
13 | 
14 | from .math import normalize_magnitude
15 | from .state_analysis import matrix_heatmap
16 | from .typing import Array, Float, Float_
17 | 
18 | 
19 | class TemporalContext(Pytree):
20 |     """Temporal context representation for memory search models."""
21 | 
22 |     def __init__(self, item_count: int, size: int):
23 |         """Create a new temporal context model.
24 | 
25 |         Args:
26 |             item_count: the number of items in the context model.
27 |             size: the size of the context representation.
28 |         """
29 |         self.size = size
30 |         self.zeros = jnp.zeros(size)
31 |         self.state = self.zeros.at[0].set(1)
32 |         self.initial_state = self.zeros.at[0].set(1)
33 |         self.next_outlist_unit = item_count + 1
34 | 
35 |     @classmethod
36 |     def init(cls, item_count: int) -> "TemporalContext":
37 |         """Standardized initialization for a new context model.
38 | 
39 |         Args:
40 |             item_count: the number of items in the context model.
41 |         """
42 |         return cls(item_count, item_count + 1)
43 | 
44 |     def integrate(
45 |         self,
46 |         context_input: Float[Array, " context_feature_units"],
47 |         drift_rate: Float_,
48 |     ) -> "TemporalContext":
49 |         """Returns context after integrating input representation, preserving unit length.
50 | 
51 |         Args:
52 |             context_input: the input representation to be integrated into the contextual state.
53 |             drift_rate: The drift rate parameter.
54 |         """
55 |         context_input = normalize_magnitude(context_input)
56 |         rho = jnp.sqrt(
57 |             1 + jnp.square(drift_rate) * (jnp.square(self.state * context_input) - 1)
58 |         ) - (drift_rate * (self.state * context_input))
59 |         return self.replace(
60 |             state=normalize_magnitude((rho * self.state) + (drift_rate * context_input))
61 |         )
62 | 
63 |     def _repr_markdown_(self):
64 |         """Returns a markdown representation of the context model."""
65 |         fig, ax = matrix_heatmap(self.state, figsize=(6, 0.6))
66 | 
67 |         ax.set_xlabel("")
68 |         ax.set_ylabel("")
69 |         ax.set_yticks([])
70 | 
71 |         # Remove colorbar safely if desired:
72 |         for coll in ax.collections:
73 |             if hasattr(coll, "colorbar") and coll.colorbar:
74 |                 coll.colorbar.remove()
75 | 
76 |         buf = io.BytesIO()
77 |         fig.savefig(buf, format="png", bbox_inches="tight")
78 |         plt.close(fig)
79 | 
80 |         encoded = base64.b64encode(buf.getvalue()).decode("utf-8")
81 |         return f'<img src="data:image/png;base64,{encoded}" />'
82 | 


--------------------------------------------------------------------------------
/jaxcmr/crp.py:
--------------------------------------------------------------------------------
  1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/crp.ipynb.
  2 | 
  3 | # %% auto 0
  4 | __all__ = ['SimpleTabulation', 'simple_tabulate_trial', 'simple_crp', 'set_false_at_index', 'Tabulation', 'tabulate_trial', 'crp',
  5 |            'plot_crp']
  6 | 
  7 | # %% ../notebooks/crp.ipynb 3
  8 | from typing import Optional, Sequence
  9 | 
 10 | from jax import jit, lax, vmap
 11 | from jax import numpy as jnp
 12 | from matplotlib import rcParams  # type: ignore
 13 | from matplotlib.axes import Axes
 14 | from simple_pytree import Pytree
 15 | 
 16 | from .experimental.plotting import init_plot, plot_data, set_plot_labels
 17 | from .experimental.repetition import all_study_positions
 18 | from .helpers import apply_by_subject
 19 | from .typing import Array, Bool, Float, Int_, Integer
 20 | 
 21 | # %% ../notebooks/crp.ipynb 5
 22 | class SimpleTabulation(Pytree):
 23 |     "A simple tabulation of transitions between items during recall of a study list."
 24 | 
 25 |     def __init__(self, list_length: int, first_recall: Int_):
 26 |         self.lag_range = list_length - 1
 27 |         self.list_length = list_length
 28 |         self.all_items = jnp.arange(1, list_length + 1, dtype=int)
 29 |         self.actual_transitions = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 30 |         self.avail_transitions = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 31 |         self.avail_items = jnp.ones(list_length, dtype=bool)
 32 |         self.avail_items = self.avail_items.at[first_recall - 1].set(False)
 33 |         self.previous_item = first_recall
 34 | 
 35 |     def _update(self, current_item: Int_) -> "SimpleTabulation":
 36 |         "Tabulate actual and possible serial lags of current from previous item."
 37 |         actual_lag = current_item - self.previous_item + self.lag_range
 38 |         all_lags = self.all_items - self.previous_item + self.lag_range
 39 | 
 40 |         return self.replace(
 41 |             previous_item=current_item,
 42 |             avail_items=self.avail_items.at[current_item - 1].set(False),
 43 |             avail_transitions=self.avail_transitions.at[all_lags].add(self.avail_items),
 44 |             actual_transitions=self.actual_transitions.at[actual_lag].add(1),
 45 |         )
 46 | 
 47 |     def update(self, choice: Int_) -> "SimpleTabulation":
 48 |         "Tabulate a transition if the choice is non-zero (i.e., a valid item)."
 49 |         return lax.cond(choice > 0, lambda: self._update(choice), lambda: self)
 50 | 
 51 | # %% ../notebooks/crp.ipynb 7
 52 | def simple_tabulate_trial(
 53 |     trial: Integer[Array, " recall_events"], list_length: int
 54 | ) -> SimpleTabulation:
 55 |     "Tabulate transitions across a single trial."
 56 |     return lax.scan(
 57 |         lambda tabulation, recall: (tabulation.update(recall), None),
 58 |         SimpleTabulation(list_length, trial[0]),
 59 |         trial[1:],
 60 |     )[0]
 61 | 
 62 | # %% ../notebooks/crp.ipynb 9
 63 | def simple_crp(
 64 |     trials: Integer[Array, "trials recall_events"], list_length: int
 65 | ) -> Float[Array, " lags"]:
 66 |     "Tabulate transitions for multiple trials."
 67 |     tabulated_trials = lax.map(lambda t: simple_tabulate_trial(t, list_length), trials)
 68 |     total_actual_transitions = jnp.sum(tabulated_trials.actual_transitions, axis=0)
 69 |     total_possible_transitions = jnp.sum(tabulated_trials.avail_transitions, axis=0)
 70 |     return total_actual_transitions / total_possible_transitions
 71 | 
 72 | 
 73 | # %% ../notebooks/crp.ipynb 12
 74 | def set_false_at_index(vec: Bool[Array, " positions"], i: Int_):
 75 |     return lax.cond(i, lambda: (vec.at[i - 1].set(False), None), lambda: (vec, None))
 76 | 
 77 | 
 78 | class Tabulation(Pytree):
 79 |     "A tabulation of transitions between items during recall of a study list."
 80 | 
 81 |     def __init__(
 82 |         self,
 83 |         presentation: Integer[Array, " study_events"],
 84 |         first_recall: Int_,
 85 |         size: int = 3,
 86 |     ):
 87 |         self.list_length = presentation.size
 88 |         self.lag_range = self.list_length - 1
 89 |         self.all_positions = jnp.arange(1, self.list_length + 1, dtype=int)
 90 |         self.base_lags = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 91 |         self.size = size
 92 |         self.item_study_positions = lax.map(
 93 |             lambda i: all_study_positions(i, presentation, size),
 94 |             self.all_positions,
 95 |         )
 96 | 
 97 |         self.actual_lags = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 98 |         self.avail_lags = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 99 | 
100 |         self.previous_positions = self.item_study_positions[first_recall - 1]
101 |         self.avail_recalls = jnp.ones(self.list_length, dtype=bool)
102 |         self.avail_recalls = self.available_recalls_after(first_recall)
103 |     
104 |     # for updating avail_recalls: study positions still available for retrieval
105 |     def available_recalls_after(self, recall: Int_) -> Bool[Array, " positions"]:
106 |         "Update the study positions available to retrieve after a transition."
107 |         study_positions = self.item_study_positions[recall - 1]
108 |         return lax.scan(set_false_at_index, self.avail_recalls, study_positions)[0]
109 | 
110 |     # for updating actual_lags: lag-transitions actually made from the previous item
111 |     def lags_from_previous(self, pos: Int_) -> Bool[Array, " positions"]:
112 |         "Identify the lag(s) from the study position(s) of the previous item."
113 | 
114 |         def f(prev):
115 |             return lax.cond(
116 |                 (pos * prev) == 0,
117 |                 lambda: self.base_lags,
118 |                 lambda: self.base_lags.at[pos - prev + self.lag_range].add(1),
119 |             )
120 | 
121 |         return lax.map(f, self.previous_positions).sum(0).astype(bool)
122 | 
123 |     def tabulate_actual_lags(self, recall: Int_) -> Integer[Array, " lags"]:
124 |         "Tabulate the actual transition after a transition."
125 |         recall_study_positions = self.item_study_positions[recall - 1]
126 |         new_lags = (
127 |             lax.map(self.lags_from_previous, recall_study_positions).sum(0).astype(bool)
128 |         )
129 |         return self.actual_lags + new_lags
130 | 
131 |     # for updating avail_lags: lag-transitions available from the previous item
132 |     def available_lags_from(self, pos: Int_) -> Bool[Array, " lags"]:
133 |         "Identify recallable lag transitions from the specified study position."
134 |         return lax.cond(
135 |             pos == 0,
136 |             lambda: self.base_lags,
137 |             lambda: self.base_lags.at[self.all_positions - pos + self.lag_range].add(
138 |                 self.avail_recalls
139 |             ),
140 |         )
141 | 
142 |     def tabulate_available_lags(self) -> Integer[Array, " lags"]:
143 |         "Tabulate available transitions after a transition."
144 |         new_lags = (
145 |             lax.map(self.available_lags_from, self.previous_positions)
146 |             .sum(0)
147 |             .astype(bool)
148 |         )
149 |         return self.avail_lags + new_lags
150 | 
151 |     # unifying tabulation of actual/avail lags, previous positions, and avail recalls
152 |     def tabulate(self, recall: Int_) -> "Tabulation":
153 |         "Tabulate actual and possible serial lags of current from previous item."
154 |         return lax.cond(
155 |             recall,
156 |             lambda: self.replace(
157 |                 previous_positions=self.item_study_positions[recall - 1],
158 |                 avail_recalls=self.available_recalls_after(recall),
159 |                 actual_lags=self.tabulate_actual_lags(recall),
160 |                 avail_lags=self.tabulate_available_lags(),
161 |             ),
162 |             lambda: self,
163 |         )
164 | 
165 | # %% ../notebooks/crp.ipynb 14
166 | def tabulate_trial(
167 |     trial: Integer[Array, " recall_events"],
168 |     presentation: Integer[Array, " study_events"],
169 |     size: int = 3,
170 | ) -> tuple[Float[Array, " lags"], Float[Array, " lags"]]:
171 |     init = Tabulation(presentation, trial[0], size)
172 |     tab = lax.fori_loop(1, trial.size, lambda i, t: t.tabulate(trial[i]), init)
173 |     return tab.actual_lags, tab.avail_lags
174 | 
175 | # %% ../notebooks/crp.ipynb 16
176 | def crp(
177 |     trials: Integer[Array, "trials recall_events"],
178 |     presentations: Integer[Array, "trials study_events"],
179 |     list_length: int,
180 |     size: int = 3,
181 | ) -> Float[Array, " lags"]:
182 |     actual, possible = vmap(tabulate_trial, in_axes=(0, 0, None))(
183 |         trials, presentations, size
184 |     )
185 |     return actual.sum(0) / possible.sum(0)
186 | 
187 | # %% ../notebooks/crp.ipynb 19
188 | def plot_crp(
189 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
190 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
191 |     max_lag: int = 5,
192 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
193 |     color_cycle: Optional[list[str]] = None,
194 |     labels: Optional[Sequence[str]] = None,
195 |     contrast_name: Optional[str] = None,
196 |     axis: Optional[Axes] = None,
197 |     size: int = 3,
198 | ) -> Axes:
199 |     """Returns Axes object with plotted prob of lag-CRP for datasets and trial masks.
200 | 
201 |     Args:
202 |         datasets: Datasets containing trial data to be plotted.
203 |         trial_masks: Masks to filter trials in datasets.
204 |         max_lag: Maximum lag to plot.
205 |         color_cycle: List of colors for plotting each dataset.
206 |         distances: Unused, included for compatibility with other plotting functions.
207 |         labels: Names for each dataset for legend, optional.
208 |         contrast_name: Name of contrast for legend labeling, optional.
209 |         axis: Existing matplotlib Axes to plot on, optional.
210 |         size: Maximum number of study positions an item can be presented at.
211 |     """
212 |     axis = init_plot(axis)
213 | 
214 |     if color_cycle is None:
215 |         color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
216 | 
217 |     if labels is None:
218 |         labels = [""] * len(datasets)
219 | 
220 |     if isinstance(datasets, dict):
221 |         datasets = [datasets]
222 | 
223 |     if isinstance(trial_masks, jnp.ndarray):
224 |         trial_masks = [trial_masks]
225 | 
226 |     lag_interval = jnp.arange(-max_lag, max_lag + 1, dtype=int)
227 | 
228 |     for data_index, data in enumerate(datasets):
229 |         lag_range = (jnp.max(data["listLength"]) - 1).item()
230 |         subject_values = apply_by_subject(
231 |             data,
232 |             trial_masks[data_index],
233 |             jit(crp, static_argnames=("size")),
234 |             size,
235 |         )
236 |         subject_values = jnp.vstack(subject_values)
237 |         subject_values = subject_values[
238 |             :, lag_range - max_lag : lag_range + max_lag + 1
239 |         ]
240 | 
241 |         color = color_cycle.pop(0)
242 |         plot_data(
243 |             axis,
244 |             lag_interval,
245 |             subject_values,
246 |             labels[data_index],
247 |             color,
248 |         )
249 | 
250 |     set_plot_labels(axis, "Lag", "Conditional Resp. Prob.", contrast_name)
251 |     return axis
252 | 
253 | 


--------------------------------------------------------------------------------
/jaxcmr/experimental/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/githubpsyche/jaxcmr/main/jaxcmr/experimental/__init__.py


--------------------------------------------------------------------------------
/jaxcmr/experimental/array.py:
--------------------------------------------------------------------------------
  1 | from typing import Any, Callable
  2 | 
  3 | import numpy as np
  4 | from jax import numpy as jnp, vmap, lax
  5 | 
  6 | from jaxcmr.typing import Array, Float, Real, Integer
  7 | from numba import types
  8 | from numba.typed import Dict
  9 | 
 10 | # from numba import njit
 11 | 
 12 | __all__ = [
 13 |     "repmat",
 14 |     "np_segment_array_by_index",
 15 |     "segment_by_index",
 16 |     "sub_connectivity",
 17 |     "subset_connectivity_matrix",
 18 |     "np_connectivity_by_index",
 19 |     "cos_sim",
 20 |     "compute_similarity_matrix",
 21 |     "segment_by_nan",
 22 |     "njit_apply_along_axis",
 23 |     "filter_repeated_recalls",
 24 | ]
 25 | 
 26 | def to_numba_typed_dict(py_dict: dict[str, np.ndarray]) -> dict[str, np.ndarray]:
 27 |     """
 28 |     Converts a Python dictionary of 2D int64 arrays to a Numba-typed dictionary.
 29 | 
 30 |     Parameters
 31 |     ----------
 32 |         py_dict (dict[str, np.ndarray]): A Python dictionary containing 2D int64 arrays.
 33 | 
 34 |     Returns
 35 |     -------
 36 |         dict[str, np.ndarray]: A Numba-typed dictionary containing 2D int32 arrays.
 37 |     """
 38 |     # Define the key and value types for the Numba typed dict
 39 |     key_type = types.unicode_type
 40 |     value_type = types.Array(types.int32, 2, "C")  # 2D int64 array
 41 | 
 42 |     # Initialize an empty Numba typed dict
 43 |     numba_dict = Dict.empty(key_type=key_type, value_type=value_type)
 44 | 
 45 |     # Populate the Numba typed dict
 46 |     for key, value in py_dict.items():
 47 |         numba_dict[key] = np.ascontiguousarray(value.astype(np.int32))
 48 | 
 49 |     return numba_dict
 50 | 
 51 | def filter_trial(trial):
 52 |     seen = set()
 53 |     filtered_trial = [
 54 |         item for item in trial if item > 0 and (item not in seen and not seen.add(item))
 55 |     ]
 56 |     # Pad with zeros to maintain the original trial length
 57 |     return filtered_trial + [0] * (len(trial) - len(filtered_trial))
 58 | 
 59 | 
 60 | def filter_repeated_recalls(recalls: jnp.ndarray):
 61 |     """Filters out repeated recalls within each trial, keeping only the first occurrence and padding with 0s.
 62 | 
 63 |     Args:
 64 |         recalls: A trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 65 | 
 66 |     Returns:
 67 |         A trial by recall position array with repeated recalls removed and padded with 0s.
 68 |     """
 69 |     # Convert to list of lists for easy manipulation
 70 |     recalls_list = recalls.tolist()
 71 | 
 72 |     return [filter_trial(trial) for trial in recalls_list]
 73 | 
 74 | 
 75 | def repmat(matrix: Real[Array, " ..."], m: int, n: int) -> Real[Array, " x y"]:
 76 |     """Return matrix replicated and tiled m x n times.
 77 | 
 78 |     Args:
 79 |         matrix: 2-D array to be replicated and tiled.
 80 |         m: Number of rows to replicate.
 81 |         n: Number of columns to replicate.
 82 |     """
 83 | 
 84 |     # Handling 1D array
 85 |     if matrix.ndim == 1:
 86 |         row_count = 1
 87 |         col_count = matrix.shape[0]
 88 |         result = jnp.zeros((m, col_count * n), dtype=matrix.dtype)
 89 |         for col_rep in range(n):
 90 |             result[:, col_rep * col_count : (col_rep + 1) * col_count] = matrix
 91 | 
 92 |     # Handling 2D array
 93 |     else:
 94 |         row_count, col_count = matrix.shape
 95 |         result = jnp.zeros((row_count * m, col_count * n), dtype=matrix.dtype)
 96 |         for row_rep in range(m):  # sourcery skip: use-itertools-product
 97 |             for col_rep in range(n):
 98 |                 result[
 99 |                     row_rep * row_count : (row_rep + 1) * row_count,
100 |                     col_rep * col_count : (col_rep + 1) * col_count,
101 |                 ] = matrix
102 | 
103 |     return result
104 | 
105 | 
106 | def np_segment_array_by_index(
107 |     data: Real[Array, " ..."], index_vector: Real[Array, " ..."]
108 | ) -> tuple[Real[Array, " ..."], Real[Array, " ..."]]:
109 |     """Returns array and unique indices by segmenting the input data array based on index vectors.
110 | 
111 |     Applies padding to handle variable sizes within groups.
112 | 
113 |     Args:
114 |         data: The multidimensional array to segment.
115 |         index_vector: Array of indices used to determine how to group elements of the data array.
116 |     """
117 |     # Find unique indices and their positions
118 |     unique_indices, inverse_index, counts = jnp.unique(
119 |         index_vector, return_inverse=True, return_counts=True
120 |     )
121 |     # Determine the maximum count of any index
122 |     max_count = jnp.max(counts)
123 | 
124 |     # Initialize the output array with zeros (for padding)
125 |     output_shape = (len(unique_indices), max_count, data.shape[1])
126 |     output_array = jnp.zeros(output_shape, dtype=data.dtype)
127 | 
128 |     # Temporary array to track insertion positions for each unique index
129 |     positions = jnp.zeros_like(unique_indices)
130 | 
131 |     # Iterate over each element in the original data array
132 |     for i in range(data.shape[0]):
133 |         idx = inverse_index[i]  # Find the group index for the current row
134 |         pos = positions[idx]  # Find the current insert position for this group
135 |         # Check if we still have space to add new items in the group
136 |         if pos < max_count:
137 |             # Insert the data row into the correct position in the output array
138 |             output_array[idx, pos] = data[i]
139 |             # Update the position tracker
140 |             positions[idx] = pos + 1
141 | 
142 |     return output_array, unique_indices
143 | 
144 | 
145 | def segment_by_index(vector, index_vector) -> tuple[list[np.ndarray], np.ndarray]:
146 |     unique_indices, first_indices = np.unique(index_vector, return_index=True)
147 |     unique_indices = unique_indices[np.argsort(first_indices)]
148 |     return [vector[index_vector == idx] for idx in unique_indices], unique_indices
149 | 
150 | 
151 | def sub_connectivity(
152 |     trial_item_ids: Integer[Array, " item_ids"], connectivity: Float[Array, " N N"]
153 | ) -> jnp.ndarray:
154 |     """Returns a connectivity matrix subset and zero-padded based on non-zero, 1-indexed trial_item_ids.
155 | 
156 |     Args:
157 |         trial_item_ids: Array of item IDs with 0 indicating padding.
158 |         connectivity: Full NxN connectivity matrix.
159 |     """
160 | 
161 |     def connection_at(i, j):
162 |         return lax.cond(
163 |             (i * j) == 0,
164 |             lambda: 0.,
165 |             lambda: connectivity[i - 1, j - 1],
166 |         )
167 | 
168 |     item_count = trial_item_ids.shape[0]
169 |     output_matrix = jnp.zeros((item_count, item_count))
170 |     return lax.fori_loop(
171 |         0,
172 |         item_count,
173 |         lambda i, matrix: lax.fori_loop(
174 |             0,
175 |             item_count,
176 |             lambda j, matrix: matrix.at[i, j].set(
177 |                 connection_at(trial_item_ids[i], trial_item_ids[j])
178 |             ),
179 |             matrix,
180 |         ),
181 |         output_matrix,
182 |     )
183 | 
184 | 
185 | def subset_connectivity_matrix(
186 |     trialwise_item_ids: Float[Array, " trial list_length"],
187 |     connectivity: Float[Array, " N N"],
188 | ) -> Float[Array, " trial list_length list_length"]:
189 |     """Return a subset of the connectivity matrix based on the specified 1-indexed item_ids.
190 | 
191 |     The output matrix is zero-padded to match the dimensions of item_ids.
192 | 
193 |     Args:
194 |         trialwise_item_ids: Trial by pres matrix of item IDs, with 0 for padding.
195 |         connectivity: Full NxN connectivity matrix.
196 |     """
197 |     return vmap(sub_connectivity, in_axes=(0, None))(trialwise_item_ids, connectivity)
198 | 
199 | 
200 | def np_connectivity_by_index(connectivity, item_ids, index_vector):
201 |     return jnp.array(
202 |         [
203 |             subset_connectivity_matrix(item_ids, connectivity)
204 |             for item_ids in np_segment_array_by_index(item_ids, index_vector)[0]
205 |         ]
206 |     )
207 | 
208 | def cos_sim(x: Float[Array, "N D"], y: Float[Array, "M D"], epsilon: float = 1e-8) -> Float[Array, "N M"]:
209 |     """
210 |     Compute cosine similarity between two sets of vectors.
211 | 
212 |     Args:
213 |         x: Array of shape (N, D) containing N D-dimensional vectors.
214 |         y: Array of shape (M, D) containing M D-dimensional vectors.
215 |         epsilon: Small value to ensure numerical stability.
216 | 
217 |     Returns:
218 |         Array of shape (N, M) containing pairwise cosine similarities.
219 |     """
220 |     # Compute norms
221 |     x_norm = jnp.linalg.norm(x, axis=1, keepdims=True)
222 |     y_norm = jnp.linalg.norm(y, axis=1, keepdims=True)
223 | 
224 |     # Normalize vectors, handling potential zero norms
225 |     x_normalized = jnp.where(x_norm > epsilon, x / x_norm, 0)
226 |     y_normalized = jnp.where(y_norm > epsilon, y / y_norm, 0)
227 | 
228 |     # Compute cosine similarity
229 |     similarities = jnp.dot(x_normalized, y_normalized.T)  # type: ignore
230 | 
231 |     # Handle potential numerical instabilities
232 |     similarities = jnp.nan_to_num(similarities, nan=0.0)
233 |     similarities = jnp.clip(similarities, -1, 1)
234 | 
235 |     return similarities
236 | 
237 | def compute_similarity_matrix(embeddings: Float[Array, " words features"]) -> Real[Array, " N N"]:
238 |     """
239 |     Returns an N x N similarity matrix from an N x Z embeddings array.
240 |     Diagonal elements are zero and all other connections are at least 0.
241 | 
242 |     Args:
243 |         embeddings: An N x Z matrix where each row represents an embedding.
244 |     """
245 |     cosine_scores = cos_sim(embeddings, embeddings)
246 | 
247 |     # Ensure all values are non-negative
248 |     cosine_scores = jnp.maximum(cosine_scores, 0)
249 | 
250 |     # Set diagonal elements to zero
251 |     cosine_scores = cosine_scores.at[jnp.diag_indices_from(cosine_scores)].set(0)
252 | 
253 |     return cosine_scores
254 | 
255 | 
256 | def segment_by_nan(vector: jnp.ndarray) -> list[tuple[int, int]]:
257 |     "Returns list of tuples segmenting the vector by its NaN values."
258 |     segments = []
259 |     start_idx = 0
260 |     for i in range(len(vector)):
261 |         if jnp.isnan(vector[i]):
262 |             segments.append((start_idx, i))
263 |             start_idx = i + 1
264 |     if start_idx < len(vector):
265 |         segments.append((start_idx, len(vector)))
266 |     return segments
267 | 
268 | 
269 | def njit_apply_along_axis(
270 |     func: Callable, array: jnp.ndarray, *args: Any
271 | ) -> list[jnp.ndarray]:
272 |     """Returns a list of results by applying a jit-compiled function along the first axis of a numpy array.
273 | 
274 |     Args:
275 |         func: Jit-compiled function to be applied. It takes an array slice and additional arguments.
276 |         array: Numpy array to be processed.
277 |         *args: Additional arguments to pass to the function.
278 |     """
279 |     return [func(array[i], *args) for i in range(len(array))]
280 | 


--------------------------------------------------------------------------------
/jaxcmr/experimental/correct_likelihood.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Okay, it looks like all I need to do is switch from using predict_and_simulate_recalls to something that takes a presentation sequence as an argument along with model and choices. Then I compute m.outcome_probability(correct) and then use choices to determine whether i'm tracking P(correct) or 1-P(correct) for that recall event. scan through all choices/presentations.
  3 | 
  4 | Initializes (or “experiences”) the study items in sequence, just as CRU would during encoding.
  5 | Starts retrieval if needed (i.e., calls model.start_retrieving()).
  6 | Iterates over the observed recall events, each of which is tied to a specific study position.
  7 | For each event, computes the model’s probability of the correct item and records either P(correct) if the participant was indeed correct or 1 – P(correct) otherwise.
  8 | Optionally updates the CRU’s internal state via something like model = model.retrieve(...), depending on how you want to replicate CRU’s post-retrieval state update.
  9 | """
 10 | 
 11 | from typing import Callable, Iterable, Mapping, Optional, Type
 12 | 
 13 | import numpy as np
 14 | from jax import jit, lax, vmap
 15 | from jax import numpy as jnp
 16 | 
 17 | from jaxcmr.helpers import all_rows_identical, log_likelihood
 18 | from jaxcmr.typing import (
 19 |     Array,
 20 |     Float,
 21 |     Float_,
 22 |     Integer,
 23 |     MemorySearch,
 24 |     MemorySearchModelFactory,
 25 | )
 26 | 
 27 | 
 28 | def predict_and_simulate_recalls(
 29 |     model: MemorySearch,
 30 |     presented: Integer[Array, " study_events"],
 31 |     choices: Integer[Array, " recall_events"],
 32 | ) -> tuple[MemorySearch, Float[Array, " recall_events"]]:
 33 |     """
 34 |     Return the updated model and the outcome probabilities of a chain of retrieval events.
 35 |     Args:
 36 |         model: the current memory search model.
 37 |         presented: the indices of the items presented (1-indexed).
 38 |         choices: the indices of the items to retrieve (1-indexed) or 0 to stop.
 39 |     """
 40 | 
 41 |     def predict(model, i):
 42 |         return (
 43 |             model.retrieve(choices[i]),
 44 |             lax.cond(
 45 |                 choices[i],
 46 |                 lambda: model.outcome_probability(presented[i]),
 47 |                 lambda: 1 - model.outcome_probability(presented[i]),
 48 |             ),
 49 |         )
 50 | 
 51 |     return lax.scan(predict, model, length=choices.size)
 52 | 
 53 | 
 54 | class MemorySearchLikelihoodFnGenerator:
 55 |     def __init__(
 56 |         self,
 57 |         model_factory: Type[MemorySearchModelFactory],
 58 |         dataset: dict[str, Integer[Array, " trials ?"]],
 59 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
 60 |     ) -> None:
 61 |         """Initialize the factory with the specified trials and trial data."""
 62 |         self.factory = model_factory(dataset, connections)
 63 |         self.create_model = self.factory.create_model
 64 | 
 65 |         # Store the presentation lists as a JAX array
 66 |         self.present_lists = jnp.array(dataset["pres_itemnos"])
 67 | 
 68 |         # Reindex the recalled items so they match the "present_lists" indexing
 69 |         trials = np.array(dataset["recalls"])
 70 |         for trial_index in range(trials.shape[0]):
 71 |             present = self.present_lists[trial_index]
 72 |             recall = trials[trial_index]
 73 |             reindexed = np.array(
 74 |                 [(present[item - 1] if item else 0) for item in recall]
 75 |             )
 76 |             trials[trial_index] = reindexed
 77 | 
 78 |         self.trials = jnp.array(trials)
 79 | 
 80 |     def init_model_for_retrieval(
 81 |         self,
 82 |         trial_index: Integer[Array, ""],
 83 |         parameters: Mapping[str, Float_],
 84 |     ) -> MemorySearch:
 85 |         """
 86 |         Create and initialize a MemorySearch model for a given trial's presentation list.
 87 |         """
 88 |         present = self.present_lists[trial_index]
 89 |         model = self.create_model(trial_index, parameters)
 90 |         model = lax.fori_loop(
 91 |             0, present.size, lambda i, m: m.experience(present[i]), model
 92 |         )
 93 |         return model.start_retrieving()
 94 | 
 95 |     def base_predict_trials(
 96 |         self,
 97 |         trial_indices: Integer[Array, " trials"],
 98 |         parameters: Mapping[str, Float_],
 99 |     ) -> Integer[Array, " trials recall_events"]:
100 |         """
101 |         Predict outcomes for each trial using a single initial model (from trial 0),
102 |         skipping re-experiencing items for each subsequent trial.
103 |         Only valid if all present-lists match.
104 |         """
105 |         model = self.init_model_for_retrieval(jnp.array(0), parameters)
106 |         return vmap(predict_and_simulate_recalls, in_axes=(None, 0, 0))(
107 |             model, self.trials[trial_indices], self.present_lists[trial_indices]
108 |         )[1]
109 | 
110 |     def present_and_predict_trials(
111 |         self,
112 |         trial_indices: Integer[Array, " trials"],
113 |         parameters: Mapping[str, Float_],
114 |     ) -> Integer[Array, " trials recall_events"]:
115 |         """
116 |         Predict outcomes for each trial by creating a new model for each trial
117 |         (re-experiencing items per trial).
118 |         """
119 | 
120 |         def present_and_predict_trial(i):
121 |             model = self.init_model_for_retrieval(i, parameters)
122 |             return predict_and_simulate_recalls(
123 |                 model, self.trials[i], self.present_lists[i]
124 |             )[1]
125 | 
126 |         return vmap(present_and_predict_trial)(trial_indices)
127 | 
128 |     def base_predict_trials_loss(
129 |         self,
130 |         trial_indices: Integer[Array, " trials"],
131 |         parameters: Mapping[str, Float_],
132 |     ) -> Float[Array, ""]:
133 |         """Return negative log-likelihood for the 'base' approach."""
134 |         return log_likelihood(self.base_predict_trials(trial_indices, parameters))
135 | 
136 |     def present_and_predict_trials_loss(
137 |         self,
138 |         trial_indices: Integer[Array, " trials"],
139 |         parameters: Mapping[str, Float_],
140 |     ) -> Float[Array, ""]:
141 |         """Return negative log-likelihood for the 'present-and-predict' approach."""
142 |         return log_likelihood(
143 |             self.present_and_predict_trials(trial_indices, parameters)
144 |         )
145 | 
146 |     def __call__(
147 |         self,
148 |         trial_indices: Integer[Array, " trials"],
149 |         base_params: Mapping[str, Float_],
150 |         free_params: Iterable[str],
151 |     ) -> Callable[[np.ndarray], Float[Array, ""]]:
152 |         """
153 |         Return a loss function that:
154 |           1. Checks if all present-lists are identical for the selected trials.
155 |           2. Chooses either the 'base' approach (single initial model) or the
156 |              'present-and-predict' approach (fresh model per trial).
157 |           3. Expects an array of parameter values (single set or multiple sets).
158 |         """
159 |         # Decide which approach to use, based on whether all present-lists match
160 |         if all_rows_identical(self.present_lists[trial_indices]):
161 |             base_loss_fn = self.base_predict_trials_loss
162 |         else:
163 |             base_loss_fn = self.present_and_predict_trials_loss
164 | 
165 |         def specialized_loss_fn(params: Mapping[str, Float_]) -> Float[Array, ""]:
166 |             """Combine base_params and dynamic params, compute negative log-likelihood."""
167 |             return base_loss_fn(trial_indices, {**base_params, **params})
168 | 
169 |         @jit
170 |         def single_param_loss(x: jnp.ndarray) -> Float[Array, ""]:
171 |             """
172 |             x is shape (n_params,) for a single set of free parameters.
173 |             """
174 |             param_dict = {key: x[i] for i, key in enumerate(free_params)}
175 |             return specialized_loss_fn(param_dict)
176 | 
177 |         @jit
178 |         def multi_param_loss(x: jnp.ndarray) -> Float[Array, " n_samples"]:
179 |             """
180 |             x is shape (n_samples, n_params) for multiple sets of free parameters.
181 |             We'll vectorize over the first axis, returning shape (n_samples,).
182 |             """
183 | 
184 |             def loss_for_one_sample(x_row: jnp.ndarray) -> Float[Array, ""]:
185 |                 param_dict = {key: x_row[i] for i, key in enumerate(free_params)}
186 |                 return specialized_loss_fn(param_dict)
187 | 
188 |             # vmap applies loss_for_one_sample across the leading dimension of x
189 |             return vmap(loss_for_one_sample, in_axes=1)(x)
190 | 
191 |         # Return a function that checks the dimensionality of x at runtime
192 |         return lambda x: multi_param_loss(x) if x.ndim > 1 else single_param_loss(x)
193 | 


--------------------------------------------------------------------------------
/jaxcmr/experimental/decision_probability.py:
--------------------------------------------------------------------------------
  1 | import jax.numpy as jnp
  2 | import numpy as np
  3 | from jax import lax, vmap
  4 | from jax.scipy.special import ndtr
  5 | from jaxtyping import Array, Bool, Float, Integer
  6 | from simple_pytree import Pytree
  7 | 
  8 | from jaxcmr.math import lb
  9 | 
 10 | Float_ = Float[Array, ""] | float | int
 11 | Int_ = Integer[Array, ""] | int
 12 | Bool_ = Bool[Array, ""] | bool
 13 | 
 14 | theta = 200.0
 15 | tmax = 400.0
 16 | n_steps = 400
 17 | t_vals = np.maximum(np.linspace(0.0, tmax, n_steps), lb)
 18 | dt = t_vals[1] - t_vals[0]
 19 | 
 20 | 
 21 | def wald_pdf(v: Float_) -> Float[Array, " timepoints"]:
 22 |     """
 23 |     Returns the Wald (Inverse Gaussian) PDF at each time point.
 24 | 
 25 |     Wald PDF:
 26 |       f(t) = theta / sqrt(2π * t^3) * exp( - (theta - v * t)^2 / (2 * t) )
 27 |     """
 28 |     numerator = theta
 29 |     denominator = jnp.sqrt(2.0 * jnp.pi * t_vals**3)
 30 |     exponent = -((theta - v * t_vals) ** 2) / (2.0 * t_vals)
 31 |     exponent = jnp.clip(exponent, a_min=None, a_max=50.0)
 32 |     return (numerator / denominator) * jnp.exp(exponent)
 33 | 
 34 | 
 35 | def wald_cdf_closed_form(v: Float_) -> Float[Array, " timepoints"]:
 36 |     t = jnp.maximum(t_vals, 1e-30)
 37 |     alpha = (v * t - theta) / jnp.sqrt(t)
 38 |     beta  = -(v * t + theta) / jnp.sqrt(t)
 39 |     exp_term = 2.0 * theta * v
 40 |     # Clip to avoid overflow/underflow
 41 |     exp_term = jnp.clip(exp_term, -60.0, 60.0)
 42 |     return ndtr(alpha) + jnp.exp(exp_term) * ndtr(beta)
 43 | 
 44 | 
 45 | def race_diffusion_precompute(
 46 |     drifts: Float[Array, " runners"],
 47 | ) -> tuple[
 48 |     Float_, Float[Array, " runners timepoints"], Float[Array, " runners timepoints"]
 49 | ]:
 50 |     """
 51 |     Precompute the time grid, PDFs, and CDFs for each runner.
 52 | 
 53 |     Returns:
 54 |         dt: The spacing between time points.
 55 |         pdfs: PDFs for each runner (shape (n_runners, n_steps)).
 56 |         cdfs: CDFs for each runner (shape (n_runners, n_steps)).
 57 |     """
 58 | 
 59 |     def single_runner_pdf_cdf(v):
 60 |         pdf_v = wald_pdf(v)
 61 |         # cdf_v = wald_cdf_from_pdf(pdf_v)
 62 |         cdf_v = wald_cdf_closed_form(v)
 63 |         return pdf_v, cdf_v
 64 | 
 65 |     pdfs, cdfs = vmap(single_runner_pdf_cdf, in_axes=(0,))(
 66 |         drifts
 67 |     )  # shape: (n_runners, n_steps)
 68 |     return dt, pdfs, cdfs
 69 | 
 70 | 
 71 | def compute_runner_probability(
 72 |     i: Int_,
 73 |     pdfs: Float[Array, "runners timepoints"],
 74 |     cdfs: Float[Array, " runners timepoints"],
 75 |     dt: Float_,
 76 | ) -> Float_:
 77 |     """
 78 |     Probability that runner i finishes first, i.e.:
 79 |       ∫ [pdfs[i,t] * ∏_{j != i} (1 - cdfs[j,t])] dt
 80 |     Using a single product over all runners' (1 - cdfs[j,t]) and dividing out the i-th factor.
 81 |     """
 82 |     # ∏_{j} [1 - cdfs[j,t]]
 83 |     all_prod = jnp.prod(1.0 - cdfs, axis=0)
 84 |     # Avoid division by zero if (1 - cdfs[i,t]) == 0
 85 |     denom = jnp.clip(1.0 - cdfs[i], 1e-30, None)
 86 |     density_i = pdfs[i] * (all_prod / denom)
 87 |     return jnp.trapezoid(density_i, dx=dt)
 88 | 
 89 | 
 90 | def compute_runner_probabilities(
 91 |     pdfs: Float[Array, "runners timepoints"],
 92 |     cdfs: Float[Array, " runners timepoints"],
 93 |     dt: Float_,
 94 | ) -> Float[Array, " runners"]:
 95 |     """
 96 |     Returns finishing-first probabilities for all runners, normalized.
 97 | 
 98 |     p_i = ∫ pdfs[i,t] * ( ∏_{j} [1 - cdfs[j,t]] ) / (1 - cdfs[i,t]) dt
 99 |     """
100 |     all_prod = jnp.prod(1.0 - cdfs, axis=0)
101 | 
102 |     def _prob_for_runner(i):
103 |         denom = jnp.clip(1.0 - cdfs[i], 1e-30, None)
104 |         density_i = pdfs[i] * (all_prod / denom)
105 |         return jnp.trapezoid(density_i, dx=dt)
106 | 
107 |     raw_probs = vmap(_prob_for_runner)(jnp.arange(pdfs.shape[0]))
108 |     total = jnp.sum(raw_probs)
109 |     return lax.cond(total > 0, lambda: raw_probs / total, lambda: raw_probs)
110 | 
111 | 
112 | class RaceDiffusionModel(Pytree):
113 |     def outcome_probability(
114 |         self,
115 |         item_index: Int_,
116 |         supports: Float[Array, " items"],
117 |     ) -> Float_:
118 |         """
119 |         Probability of selecting the specified item (index).
120 |         """
121 |         d, pdfs, cdfs = race_diffusion_precompute(supports)
122 |         return compute_runner_probability(item_index, pdfs, cdfs, d)
123 | 
124 |     def outcome_probabilities(
125 |         self,
126 |         supports: Float[Array, " items"],
127 |     ) -> Float[Array, " items"]:
128 |         """
129 |         Returns probability distribution over all items.
130 |         """
131 |         d, pdfs, cdfs = race_diffusion_precompute(supports)
132 |         return compute_runner_probabilities(pdfs, cdfs, d)
133 | 


--------------------------------------------------------------------------------
/jaxcmr/experimental/plotting.py:
--------------------------------------------------------------------------------
  1 | from typing import Optional
  2 | from matplotlib.axes import Axes
  3 | import matplotlib.pyplot as plt
  4 | 
  5 | from scipy.stats import bootstrap
  6 | from jaxcmr.experimental.array import segment_by_nan
  7 | from jax import numpy as jnp
  8 | from jaxcmr.typing import Real, Array
  9 | 
 10 | 
 11 | __all__ = [
 12 |     "init_plot",
 13 |     "plot_data",
 14 |     "calculate_errors",
 15 |     "plot_with_error_bars",
 16 |     "plot_without_error_bars",
 17 |     "set_plot_labels",
 18 | ]
 19 | 
 20 | 
 21 | def init_plot(axis: Optional[Axes] = None) -> Axes:
 22 |     """Return initialized plotting axis
 23 | 
 24 |     Args:
 25 |         axis: An existing axis to use for the plot.
 26 |     """
 27 |     if axis is None:
 28 |         plt.figure()
 29 |         axis = plt.gca()
 30 |     return axis
 31 | 
 32 | 
 33 | def calculate_errors(y_values: jnp.ndarray, y_mean: jnp.ndarray) -> jnp.ndarray:
 34 |     """Returns an array of errors calculated from the given y values and y mean, using bootstrapped confidence intervals.
 35 | 
 36 |     Args:
 37 |         y_values: Array of y values.
 38 |         y_mean: Array of y mean values.
 39 |     """
 40 |     errors = jnp.zeros((2, len(y_mean)))
 41 |     bootstrapped_confidence_intervals = bootstrap(
 42 |         (y_values,), jnp.nanmean, confidence_level=0.95
 43 |     ).confidence_interval
 44 | 
 45 |     return (
 46 |         errors.at[0]
 47 |         .set(y_mean - bootstrapped_confidence_intervals.low)
 48 |         .at[1]
 49 |         .set(bootstrapped_confidence_intervals.high - y_mean)
 50 |     )
 51 | 
 52 | 
 53 | def plot_with_error_bars(
 54 |     axis: Axes,
 55 |     x_values: Real[Array, " trial_count values"],
 56 |     y_values: Real[Array, " trial_count values"],
 57 |     y_mean: Real[Array, " values"],
 58 |     segments: list[tuple[int, int]],
 59 |     label: str,
 60 |     color: str,
 61 | ):
 62 |     """Returns a plot of a line graph with error bars on the specified axis.
 63 | 
 64 |     Args:
 65 |         axis: The axis object to plot on.
 66 |         x_values: The x-axis values.
 67 |         y_values: The y-axis values.
 68 |         y_mean: The mean y-axis values.
 69 |         segments: A list of tuples indicating start and end indices for segments.
 70 |         label: The label for the line graph.
 71 |         color: The color of the line graph.
 72 |     """
 73 |     errors = calculate_errors(y_values, y_mean)
 74 | 
 75 |     for index, (start, end) in enumerate(segments):
 76 |         axis.errorbar(
 77 |             x_values[start:end],
 78 |             y_mean[start:end] - errors[0][start:end],
 79 |             errors[:, start:end],
 80 |             label=label if index == 0 else None,
 81 |             color=color,
 82 |         )
 83 | 
 84 | 
 85 | def plot_without_error_bars(
 86 |     axis: Axes,
 87 |     x_values: Real[Array, " trial_count values"],
 88 |     y_mean: Real[Array, " values"],
 89 |     segments: list[tuple[int, int]],
 90 |     label: str,
 91 |     color: str,
 92 | ):
 93 |     """Returns a plot of a line graph without error bars on the specified axis.
 94 | 
 95 |     Args:
 96 |         axis: The axis object to plot on.
 97 |         x_values: The x-axis values.
 98 |         y_values: The y-axis values.
 99 |         y_mean: The mean y-axis values.
100 |         segments: A list of tuples indicating start and end indices for segments.
101 |         label: The label for the line graph.
102 |         color: The color of the line graph.
103 |     """
104 |     for index, (start, end) in enumerate(segments):
105 |         axis.plot(
106 |             x_values[start:end],
107 |             y_mean[start:end],
108 |             label=label if index == 0 else None,
109 |             color=color,
110 |         )
111 | 
112 | 
113 | def plot_data(
114 |     axis: Axes,
115 |     x_values: Real[Array, " trial_count values"],
116 |     y_values: Real[Array, " trial_count values"],
117 |     label: str,
118 |     color: str,
119 | ) -> Axes:
120 |     """Returns axis plotting data as line segment(s) on the provided axis.
121 | 
122 |     Args:
123 |         axis: the axis to plot on; if None, a new figure is created.
124 |         x_values: Vector of x values to plot.
125 |         crp_values: vector or matrix of y values to plot.
126 |         label: name for the plotted data.
127 |         color: color for the plotted data.
128 |     """
129 |     if y_values.ndim == 1:
130 |         y_values = y_values[jnp.newaxis, :]
131 |     y_mean = jnp.nanmean(y_values, axis=0)
132 |     segments = segment_by_nan(y_mean)
133 | 
134 |     if len(y_values) > 1:
135 |         plot_with_error_bars(axis, x_values, y_values, y_mean, segments, label, color)
136 |     else:
137 |         plot_without_error_bars(axis, x_values, y_mean, segments, label, color)
138 |     return axis
139 | 
140 | 
141 | def set_plot_labels(
142 |     axis: Axes, xlabel: str, ylabel: str, contrast_name: Optional[str] = None
143 | ) -> Axes:
144 |     """Returns an axis with modified labels and optional legend settings for a plot.
145 | 
146 |     Args:
147 |         axis: The axis to modify.
148 |         xlabel: The label for the x-axis.
149 |         ylabel: The label for the y-axis.
150 |         contrast_name: Name of the contrast for the legend, if applicable.
151 |     """
152 |     # Set x and y labels
153 |     axis.set(xlabel=xlabel, ylabel=ylabel)
154 | 
155 |     # If a contrast name is provided, set it as the legend's title
156 |     if contrast_name:
157 |         axis.legend(title=contrast_name)
158 | 
159 |     return axis
160 | 


--------------------------------------------------------------------------------
/jaxcmr/experimental/repetition.py:
--------------------------------------------------------------------------------
  1 | import numpy as np
  2 | from tqdm import tqdm
  3 | from jaxcmr.helpers import generate_trial_mask
  4 | from numba import njit
  5 | from jaxcmr.typing import Integer, Array, Int_
  6 | from jax import lax, numpy as jnp
  7 | 
  8 | 
  9 | @njit
 10 | def shuffle_matrix(rng, matrix, experiment_count):
 11 |     # Initialize an empty list to store each shuffled matrix
 12 |     shufflings = np.zeros(
 13 |         (int(experiment_count * matrix.shape[0]), matrix.shape[1]), dtype=matrix.dtype
 14 |     )
 15 | 
 16 |     for _ in range(experiment_count):
 17 |         # Copy and shuffle the matrix
 18 |         shuffled_matrix = matrix.copy()
 19 |         rng.shuffle(shuffled_matrix)
 20 |         shufflings[_ * matrix.shape[0] : (_ + 1) * matrix.shape[0]] = shuffled_matrix
 21 | 
 22 |     return shufflings
 23 | 
 24 | 
 25 | def control_dataset(
 26 |     data: dict,
 27 |     mixed_trial_query: str,
 28 |     ctrl_trial_query: str,
 29 |     control_experiment_count: int = 1000,
 30 | ):
 31 |     trial_mask = generate_trial_mask(data, mixed_trial_query)
 32 |     control_trial_mask = generate_trial_mask(data, ctrl_trial_query)
 33 |     trials = data["recalls"]
 34 |     list_length = np.max(data["listLength"][trial_mask])
 35 |     presentations = data["pres_itemnos"][:, :list_length]
 36 |     subjects = data["subject"].flatten()
 37 | 
 38 |     result_trials = []
 39 |     result_presentations = []
 40 |     result_subjects = []
 41 |     rng = np.random.default_rng(1)
 42 |     for subject_index, subject in tqdm(enumerate(np.unique(subjects))):
 43 |         subject_specific_trial_mask = np.logical_and(subjects == subject, trial_mask)
 44 |         ctrl_subject_specific_trial_mask = np.logical_and(
 45 |             subjects == subject, control_trial_mask
 46 |         )
 47 | 
 48 |         trial_count = np.sum(subject_specific_trial_mask)
 49 |         if trial_count == 0:
 50 |             continue
 51 | 
 52 |         shuffled_control_trials = shuffle_matrix(
 53 |             rng, trials[ctrl_subject_specific_trial_mask], control_experiment_count
 54 |         )
 55 |         repeated_mixed_presentations = np.tile(
 56 |             presentations[subject_specific_trial_mask],
 57 |             (control_experiment_count, 1),
 58 |         )
 59 | 
 60 |         result_trials.append(shuffled_control_trials)
 61 |         result_presentations.append(repeated_mixed_presentations)
 62 |         result_subjects += [subject] * trial_count * control_experiment_count
 63 | 
 64 |     result_trials = np.concatenate(result_trials)
 65 |     result_presentations = np.concatenate(result_presentations)
 66 | 
 67 |     return {
 68 |         "subject": np.expand_dims(result_subjects, axis=1),
 69 |         "recalls": result_trials,
 70 |         "pres_itemnos": result_presentations,
 71 |         "listLength": np.full((result_trials.shape[0], 1), list_length),
 72 |     }
 73 | 
 74 | @njit
 75 | def njit_item_to_study_positions(
 76 |         item: int,
 77 |         presentation: np.ndarray):
 78 |     """Return the one-indexed study positions of an item in a 1D presentation sequence.
 79 | 
 80 |     Args:
 81 |         item: the item index.
 82 |         presentation: the 1D presentation sequence.
 83 |         size: number of non-zero entries to return.
 84 |     """
 85 |     return (
 86 |         np.empty(0, dtype=np.int64)
 87 |         if item == 0
 88 |         else np.nonzero(presentation == item)[0] + 1
 89 |     )
 90 |     
 91 | 
 92 | def item_to_study_positions(
 93 |     item: Int_,
 94 |     presentation: Integer[Array, " list_length"],
 95 |     size: int,
 96 | ):
 97 |     """Return the one-indexed study positions of an item in a 1D presentation sequence.
 98 | 
 99 |     Args:
100 |         item: the item index.
101 |         presentation: the 1D presentation sequence.
102 |         size: number of non-zero entries to return.
103 |     """
104 |     return lax.cond(
105 |         item == 0,
106 |         lambda: jnp.zeros(size, dtype=int),
107 |         lambda: jnp.nonzero(presentation == item, size=size, fill_value=-1)[0] + 1,
108 |     )
109 | 
110 | 
111 | def all_study_positions(
112 |     study_position: Int_,
113 |     presentation: Integer[Array, " list_length"],
114 |     size: int,
115 | ):
116 |     """Return the one-indexed study positions associated with a given study position.
117 | 
118 |     Args:
119 |         study_position: the study position.
120 |         presentation: the 1D presentation sequence.
121 |         size: number of non-zero entries to return.
122 |     """
123 |     item = lax.cond(
124 |         study_position > 0,
125 |         lambda: presentation[study_position - 1],
126 |         lambda: 0,
127 |     )
128 |     return item_to_study_positions(item, presentation, size)


--------------------------------------------------------------------------------
/jaxcmr/fitting.py:
--------------------------------------------------------------------------------
  1 | import time
  2 | from typing import Any, Mapping, Optional, Type
  3 | 
  4 | import numpy as np
  5 | from jax import numpy as jnp
  6 | from scipy.optimize import differential_evolution
  7 | from tqdm import trange
  8 | 
  9 | from jaxcmr.typing import (
 10 |     Array,
 11 |     Bool,
 12 |     FitResult,
 13 |     Float_,
 14 |     Integer,
 15 |     LossFnGenerator,
 16 |     MemorySearchModelFactory,
 17 | )
 18 | 
 19 | 
 20 | def make_subject_trial_masks(
 21 |     trial_mask: Bool[Array, " trials"], subject_vector: Integer[Array, " trials"]
 22 | ):
 23 |     """Returns a list of subject-specific masks and the list of unique subjects."""
 24 |     unique_subjects = np.unique(subject_vector)
 25 |     subject_masks = [
 26 |         (subject_vector == s) & trial_mask.astype(bool) for s in unique_subjects
 27 |     ]
 28 |     return subject_masks, unique_subjects
 29 | 
 30 | 
 31 | class ScipyDE:
 32 |     """A fitting class that uses SciPy's Differential Evolution algorithm."""
 33 | 
 34 |     def __init__(
 35 |         self,
 36 |         dataset: dict[str, Integer[Array, " trials ?"]],
 37 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
 38 |         base_params: Mapping[str, Float_],
 39 |         model_factory: Type[MemorySearchModelFactory],
 40 |         loss_fn_generator: Type[LossFnGenerator],
 41 |         hyperparams: Optional[dict[str, Any]] = None,
 42 |     ):
 43 |         """
 44 |         Configure the fitting algorithm.
 45 | 
 46 |         Args:
 47 |             dataset: The dataset containing trial data (including 'subject').
 48 |             connections: Optional connectivity matrix.
 49 |             base_params: A dictionary of parameters that are held fixed.
 50 |             model_factory: Class implementing MemorySearchModelFactory.
 51 |             loss_fn_generator: Class implementing LossFnGenerator.
 52 |             hyperparams: Optional dictionary of hyperparameters for the fitting routine.
 53 |                 May include 'bounds' (dict[str, list[float]]) and other keys
 54 |                 like 'num_steps', 'pop_size', etc.
 55 |         """
 56 |         # Store essential data
 57 |         self.dataset = dataset
 58 |         self.connections = connections
 59 |         self.base_params = base_params
 60 |         self.subjects = dataset["subject"].flatten()
 61 | 
 62 |         # configure convenience features
 63 |         if hyperparams is None:
 64 |             hyperparams = {}
 65 |         self.progress_bar = hyperparams.get("progress_bar", True)
 66 |         self.display_iterations = hyperparams.get("display_iterations", False)
 67 | 
 68 |         # Extract bounds for free params; store all hyperparams for convenience
 69 |         self.free_parameter_bounds = hyperparams.get("bounds", {})
 70 |         self.bounds = np.array(list(self.free_parameter_bounds.values()))
 71 |         
 72 |         if hyperparams is None:
 73 |             hyperparams = {}
 74 |         self.all_hyperparams = {
 75 |             "bounds": self.free_parameter_bounds,
 76 |             "num_steps": hyperparams.get("num_steps", 1000),
 77 |             "pop_size": hyperparams.get("pop_size", 15),
 78 |             "relative_tolerance": hyperparams.get("relative_tolerance", 0.001),
 79 |             "cross_over_rate": hyperparams.get("cross_over_rate", 0.9),
 80 |             "diff_w": hyperparams.get("diff_w", 0.85),
 81 |             "best_of": hyperparams.get("best_of", 1),
 82 |         }
 83 | 
 84 |         self.loss_fn_generator = loss_fn_generator(model_factory, dataset, connections)
 85 | 
 86 |     def _fit_single_mask(
 87 |         self, trial_mask: Bool[Array, " trials"], subject_id: int = -1
 88 |     ) -> FitResult:
 89 |         """Returns result of fitting the model to the trials specified by the mask."""
 90 |         # Convert the mask to an array of trial indices
 91 |         trial_indices = jnp.where(trial_mask)[0]
 92 | 
 93 |         # Build a scalar loss function based on these trial indices
 94 |         loss_fn = self.loss_fn_generator(
 95 |             trial_indices, self.base_params, self.free_parameter_bounds
 96 |         )
 97 | 
 98 |         # Run differential evolution
 99 |         best_fitness = np.inf
100 |         for _ in range(self.all_hyperparams["best_of"]):
101 |             fit_result = differential_evolution(
102 |                 loss_fn,
103 |                 self.bounds,
104 |                 maxiter=self.all_hyperparams["num_steps"],
105 |                 popsize=self.all_hyperparams["pop_size"],
106 |                 vectorized=True,
107 |                 disp=self.display_iterations,
108 |                 tol=self.all_hyperparams["relative_tolerance"],
109 |                 mutation=self.all_hyperparams["diff_w"],
110 |                 recombination=self.all_hyperparams["cross_over_rate"],
111 |             )
112 |             if fit_result.fun < best_fitness:
113 |                 best_fitness = fit_result.fun
114 |                 best_fit_result = fit_result
115 | 
116 |         return {
117 |             "fixed": {k: float(v) for k, v in self.base_params.items()},
118 |             "free": {
119 |                 k: self.free_parameter_bounds[k] for k in self.free_parameter_bounds
120 |             },
121 |             "fitness": [float(best_fitness)],
122 |             "fits": {
123 |                 # For each base param, we just repeat its original value
124 |                 **{k: [float(v)] for k, v in self.base_params.items()},
125 |                 # For each free param, we store the optimizer's best value
126 |                 **{
127 |                     # Map each free param name to the best-fit value
128 |                     param_name: [float(best_fit_result.x[idx])]
129 |                     for idx, param_name in enumerate(self.free_parameter_bounds)
130 |                 },
131 |                 "subject": [subject_id],
132 |             },
133 |             # These keys will be added at the top-level fit call
134 |             "hyperparameters": {},
135 |             "fit_time": 0.0,
136 |         }
137 | 
138 |     def fit(
139 |         self, trial_mask: Bool[Array, " trials"], fit_to_subjects: bool = True
140 |     ) -> FitResult:
141 |         """Either fit the model to all subjects individually or do a single global fit."""
142 |         t0 = time.perf_counter()
143 | 
144 |         # If not per-subject, just do one global fit
145 |         if not fit_to_subjects:
146 |             result = self._fit_single_mask(trial_mask)
147 |             result["hyperparameters"] = self.all_hyperparams
148 |             result["fit_time"] = time.perf_counter() - t0
149 |             return result
150 | 
151 |         # Otherwise, gather per-subject masks
152 |         subject_trial_masks, unique_subjects = make_subject_trial_masks(
153 |             trial_mask, self.subjects
154 |         )
155 | 
156 |         # Prepare a global result structure
157 |         all_results: FitResult = {
158 |             "fixed": {k: float(v) for k, v in self.base_params.items()},
159 |             "free": {
160 |                 k: self.free_parameter_bounds[k] for k in self.free_parameter_bounds
161 |             },
162 |             "fitness": [],
163 |             "fits": {
164 |                 **{k: [] for k in self.free_parameter_bounds},
165 |                 **{k: [] for k in self.base_params},
166 |                 "subject": [],
167 |             },
168 |             "hyperparameters": self.all_hyperparams,
169 |             "fit_time": 0.0,
170 |         }
171 | 
172 |         # Optionally show progress bar
173 |         subject_range = (
174 |             trange(len(unique_subjects))
175 |             if self.progress_bar
176 |             else range(len(unique_subjects))
177 |         )
178 |         for s in subject_range:
179 |             # If no trials for this subject, skip
180 |             if np.sum(subject_trial_masks[s]) == 0:
181 |                 continue
182 | 
183 |             # Single-fit on the subject-specific mask
184 |             fit_result = self._fit_single_mask(subject_trial_masks[s], int(unique_subjects[s]))
185 |             all_results["fitness"] += fit_result["fitness"]
186 | 
187 |             # Show in tqdm progress bar
188 |             if self.progress_bar:
189 |                 subject_range.set_description( # type: ignore
190 |                     f"Subject={unique_subjects[s]}, Fitness={fit_result['fitness'][0]}"
191 |                 )
192 | 
193 |             # Accumulate fitted parameters
194 |             all_results["fits"]["subject"].append(int(unique_subjects[s]))
195 | 
196 |             # Append param values
197 |             for param_name, values_list in fit_result["fits"].items():
198 |                 # Skip 'subject' to avoid double-appending
199 |                 if param_name == "subject":
200 |                     continue
201 |                 all_results["fits"][param_name] += values_list
202 | 
203 |             # Bail out if we got a non-finite fitness
204 |             if not jnp.isfinite(fit_result["fitness"][0]):
205 |                 raise ValueError(
206 |                     f"Non-finite fitness for subject {unique_subjects[s]}", fit_result
207 |                 )
208 | 
209 |         all_results["fit_time"] = time.perf_counter() - t0
210 |         return all_results
211 | 


--------------------------------------------------------------------------------
/jaxcmr/helpers.py:
--------------------------------------------------------------------------------
  1 | import importlib
  2 | from pathlib import Path
  3 | from typing import Callable, Iterable, List, Optional, Sequence
  4 | 
  5 | import h5py
  6 | import jax.numpy as jnp
  7 | 
  8 | from jaxcmr.typing import Array, Bool, Float, Real
  9 | 
 10 | 
 11 | def all_rows_identical(arr: Real[Array, " x y"]) -> bool:
 12 |     """Return whether all rows in the 2D array are identical."""
 13 |     return jnp.all(arr == arr[0])  # type: ignore
 14 | 
 15 | 
 16 | def log_likelihood(likelihoods: Float[Array, "trial_count ..."]) -> Float[Array, ""]:
 17 |     """Return the summed log likelihood over specified likelihoods."""
 18 |     return -jnp.sum(jnp.log(likelihoods))
 19 | 
 20 | 
 21 | def import_from_string(import_string):
 22 |     """
 23 |     Import a module or function from a string.
 24 | 
 25 |     Args:
 26 |         import_string: A string in the format 'module.submodule.ClassName' or 'module.function_name'.
 27 | 
 28 |     Returns:
 29 |         The imported module or function.
 30 | 
 31 |     Raises:
 32 |         ImportError: If the import string is not valid.
 33 |     """
 34 |     module_name, function_name = import_string.rsplit(".", 1)
 35 |     module = importlib.import_module(module_name)
 36 |     return getattr(module, function_name)
 37 | 
 38 | 
 39 | def format_floats(iterable: Iterable[float], precision: int = 2) -> List[str]:
 40 |     """
 41 |     Formats a list of floats to a specified precision.
 42 | 
 43 |     Args:
 44 |         iterable: Iterable of floats to format.
 45 |         precision: Number of decimal places to format to.
 46 | 
 47 |     Returns:
 48 |         List of formatted strings.
 49 |     """
 50 |     format_str = f"{{:.{precision}f}}"
 51 |     return [format_str.format(x) for x in iterable]
 52 | 
 53 | 
 54 | def find_project_root(marker: str = ".git") -> str:
 55 |     """
 56 |     Finds the project root by traversing upwards from `start` directory
 57 |     until a directory containing `marker` is found.
 58 |     """
 59 |     start = Path.cwd()
 60 |     for path in [start, *start.parents]:
 61 |         if (path / marker).exists():
 62 |             return str(path)
 63 |     raise FileNotFoundError(f"Could not find project root containing {marker}.")
 64 | 
 65 | 
 66 | def generate_trial_mask(
 67 |     data: dict, trial_query: Optional[str]
 68 | ) -> Bool[Array, " trial_count"]:
 69 |     """Returns a boolean mask for selecting trials based on a specified query condition.
 70 | 
 71 |     Args:
 72 |         data: dict containing trial data arrays, including a "recalls" key with an array.
 73 |         trial_query: condition to evaluate, which should return a boolean array.
 74 |         If None, returns a mask that selects all trials.
 75 |     """
 76 |     if trial_query is None:
 77 |         return jnp.ones(data["recalls"].shape[0], dtype=bool)
 78 |     return eval(trial_query).flatten()
 79 | 
 80 | 
 81 | def load_data(data_path: str) -> dict[str, jnp.ndarray]:
 82 |     """
 83 |     Loads and processes an HDF5 dataset from the specified file.
 84 | 
 85 |     This function opens the HDF5 file at `data_path`, extracts all datasets stored
 86 |     under the "/data" group, transposes each array, and converts them into jnp
 87 |     arrays for further processing.
 88 | 
 89 |     Args:
 90 |         data_path: Path to the HDF5 file containing the dataset.
 91 | 
 92 |     Returns:
 93 |         A dictionary where each key corresponds to a dataset name and each value is
 94 |         a jax.numpy array containing the transposed data.
 95 |     """
 96 |     with h5py.File(data_path, "r") as f:
 97 |         result = {key: f["/data"][key][()].T for key in f["/data"].keys()}  # type: ignore
 98 |     return {key: jnp.array(value) for key, value in result.items()}
 99 | 
100 | 
101 | def save_dict_to_hdf5(data: dict, path: str):
102 |     """Save a dictionary of numpy arrays to an HDF5 file."""
103 |     with h5py.File(path, "w") as file:
104 |         data_group = file.create_group(
105 |             "data"
106 |         )  # Create a group named "data" in the HDF5 file
107 |         for key, value in data.items():
108 |             # Create each dataset within the "data" group
109 |             data_group.create_dataset(key, data=value.T)
110 | 
111 | 
112 | def find_max_list_length(
113 |     datasets: Sequence[dict[str, jnp.ndarray]],
114 |     trial_masks: Sequence[Bool[Array, " trial_count"]],
115 | ) -> int:
116 |     """Returns highest list length across multiple datasets, given trial masks.
117 | 
118 |     Args:
119 |         datasets: dataset dicts, each with a key "listLength" mapping to a numpy array.
120 |         trial_masks: Boolean numpy arrays used as masks to filter each dataset.
121 |     """
122 |     return max(
123 |         jnp.max(data["listLength"][trial_mask]).item()
124 |         for data, trial_mask in zip(datasets, trial_masks)
125 |     )
126 | 
127 | 
128 | def apply_by_subject(
129 |     data: dict[str, jnp.ndarray],
130 |     trial_mask: Bool[Array, " trial_count"],
131 |     func: Callable,
132 |     *args,
133 | ) -> list[jnp.ndarray]:
134 |     """Returns result from applying `func` to each subject's data
135 | 
136 |     Args:
137 |         data: Dataset containing recalls, presentations, etc., indexed by subject.
138 |         trial_mask: Boolean array specifying which trials to include.
139 |         func: Function to apply per subject. It should accept trials, presentations, list_length, and additional arguments.
140 |         *args: Additional positional arguments to pass to the function.
141 |     """
142 |     trials = data["recalls"]
143 |     presentations = data["pres_itemnos"]
144 |     subject_indices = data["subject"].flatten()
145 |     list_length = jnp.max(data["listLength"][trial_mask]).item()
146 |     results = []
147 | 
148 |     for subject in jnp.unique(data["subject"]):
149 |         subject_mask = jnp.logical_and(subject_indices == subject, trial_mask)
150 |         if jnp.sum(subject_mask) == 0:
151 |             continue
152 |         results.append(
153 |             func(trials[subject_mask], presentations[subject_mask], list_length, *args)
154 |         )
155 |     return results
156 | 


--------------------------------------------------------------------------------
/jaxcmr/instance_memory.py:
--------------------------------------------------------------------------------
  1 | from jax import numpy as jnp
  2 | from simple_pytree import Pytree
  3 | 
  4 | from jaxcmr.math import power_scale
  5 | from jaxcmr.typing import Array, Float, Float_, Int_
  6 | 
  7 | 
  8 | class InstanceMemory(Pytree):
  9 |     """Instance-based memory model for CMR.
 10 | 
 11 |     Attributes:
 12 |         state: the current state of the memory.
 13 |         _probe: pre-allocated probe array for memory retrieval.
 14 |         study_index: index for the next trace to be stored.
 15 |         feature_activation_scale: the scaling factor for activated output features.
 16 |         trace_activation_scale: the scaling factor for activated traces.
 17 |         output_size: the size of the output representation.
 18 |         input_size: the size of the input representation.
 19 |     """
 20 | 
 21 |     def __init__(
 22 |         self,
 23 |         state: Float[Array, " input_size output_size"],
 24 |         probe: Float[Array, " input_size+output_size"],
 25 |         study_index: Int_,
 26 |         feature_activation_scale: Float_,
 27 |         trace_activation_scale: Float_,
 28 |         input_size: int,
 29 |         output_size: int,
 30 |     ):
 31 |         self.state = state
 32 |         self.input_size = input_size
 33 |         self.output_size = output_size
 34 |         self.study_index = study_index
 35 |         self.feature_activation_scale = feature_activation_scale
 36 |         self.trace_activation_scale = trace_activation_scale
 37 |         self._probe = probe
 38 | 
 39 |     def associate(
 40 |         self,
 41 |         in_pattern: Float[Array, " input_size"],
 42 |         out_pattern: Float[Array, " output_size"],
 43 |         learning_rate: Float_,
 44 |     ) -> "InstanceMemory":
 45 |         """Return the updated memory after associating input and output patterns.
 46 | 
 47 |         Args:
 48 |             in_pattern: a feature pattern for an input.
 49 |             out_pattern: a feature pattern for an output.
 50 |             learning_rate: the learning rate parameter.
 51 |         """
 52 |         return self.replace(
 53 |             state=self.state.at[self.study_index].set(
 54 |                 jnp.concatenate((in_pattern, out_pattern * learning_rate))
 55 |             ),
 56 |             study_index=self.study_index + 1,
 57 |         )
 58 | 
 59 |     def trace_activations(self, input_pattern: Float[Array, " input_size"]):
 60 |         """Return the trace activations for the input pattern.
 61 | 
 62 |         Args:
 63 |             input_pattern: the input feature pattern.
 64 |             trace_activation_scale: the scaling factor for activated traces.
 65 |         """
 66 |         activation = jnp.dot(self.state, input_pattern)
 67 |         return power_scale(activation, self.trace_activation_scale)
 68 | 
 69 |     def probe(
 70 |         self,
 71 |         in_pattern: Float[Array, " input_size"],
 72 |     ) -> Float[Array, " output_size"]:
 73 |         """Return the output pattern associated with the input pattern in memory.
 74 | 
 75 |         Args:
 76 |             input_pattern: the input feature pattern.
 77 |         """
 78 |         t = self.trace_activations(self._probe.at[: in_pattern.size].set(in_pattern))
 79 |         a = jnp.dot(t, self.state)[in_pattern.size :]
 80 |         return power_scale(a, self.feature_activation_scale)
 81 | 
 82 |     @classmethod
 83 |     def init_mfc(
 84 |         cls,
 85 |         list_length: int,
 86 |         context_feature_count: int,
 87 |         size: int,
 88 |         learning_rate: Float_,
 89 |         feature_activation_scale: Float_,
 90 |         trace_activation_scale: Float_,
 91 |     ) -> "InstanceMemory":
 92 |         """Return a new instance-based item-to-context memory model.
 93 | 
 94 |         Initially, all items are associated with a unique context unit by `1-learning_rate`.
 95 |         We pre-allocate for `list_length` study events.
 96 | 
 97 |         To allow multiplex traces, set size to `list_length + list_length + list_length`.
 98 |         To allow out-of-list contexts, set context_feature_count to `list_length + list_length + 1`.
 99 | 
100 |         Args:
101 |             list_length: the max number of study events that could occur.
102 |             context_feature_count: the number of unique units in context.
103 |             size: maximum number of additional traces to allow to be stored in memory.
104 |             learning_rate: the learning rate parameter.
105 |             feature_activation_scale: the activation scaling factor for output features.
106 |             trace_activation_scale: the activation scaling factor for traces.
107 |         """
108 |         item_feature_count = list_length
109 |         items = jnp.eye(list_length)
110 |         contexts = jnp.eye(list_length, context_feature_count, 1) * (1 - learning_rate)
111 |         presentations = (
112 |             jnp.zeros((list_length + size, item_feature_count + context_feature_count))
113 |             .at[:list_length, :item_feature_count]
114 |             .set(items)
115 |         )
116 | 
117 |         return cls(
118 |             state=presentations.at[:list_length, item_feature_count:].set(contexts),
119 |             probe=jnp.zeros(item_feature_count + context_feature_count),
120 |             study_index=list_length,
121 |             input_size=item_feature_count,
122 |             output_size=context_feature_count,
123 |             feature_activation_scale=feature_activation_scale,
124 |             trace_activation_scale=trace_activation_scale,
125 |         )
126 | 
127 |     @classmethod
128 |     def init_mcf(
129 |         cls,
130 |         list_length: int,
131 |         context_feature_count: int,
132 |         size: int,
133 |         item_support: Float_,
134 |         shared_support: Float_,
135 |         feature_activation_scale: Float_,
136 |         trace_activation_scale: Float_,
137 |     ) -> "InstanceMemory":
138 |         """Return a new instance-based context-to-item memory model.
139 | 
140 |         Initially, in-list context units are associated with all items according to shared_support.
141 |         They are also associated with a unique item according to item_support.
142 |         Start-of-list and out-of-list context units receive no initial associations.
143 |         We pre-allocate for `list_length` study events.
144 | 
145 |         To allow multiplex traces, set size to `list_length + list_length + list_length`.
146 |         To allow out-of-list contexts, set context_feature_count to `list_length + list_length + 1`.
147 |         Otherwise, use `list_length` and `list_length + 1` respectively.
148 | 
149 |         Args:
150 |             list_length: the max number of study events that could occur.
151 |             context_feature_count: the number of unique units in context.
152 |             size: maximum number of additional traces to allow to be stored in memory.
153 |             item_support: initial association between an item and its own context feature
154 |             shared_support: initial association between an item and all other contextual features
155 |             feature_activation_scale: the activation scaling factor for output features.
156 |             trace_activation_scale: the activation scaling factor for traces.
157 |         """
158 |         item_feature_count = list_length
159 |         contexts = jnp.eye(list_length, context_feature_count, 1)
160 |         items = (
161 |             jnp.eye(list_length) * (item_support - shared_support)
162 |         ) + shared_support
163 |         presentations = (
164 |             jnp.zeros((size + list_length, context_feature_count + item_feature_count))
165 |             .at[:list_length, :context_feature_count]
166 |             .set(contexts)
167 |         )
168 |         return cls(
169 |             state=presentations.at[:list_length, context_feature_count:].set(items),
170 |             probe=jnp.zeros(item_feature_count + context_feature_count),
171 |             study_index=list_length,
172 |             input_size=context_feature_count,
173 |             output_size=item_feature_count,
174 |             feature_activation_scale=feature_activation_scale,
175 |             trace_activation_scale=trace_activation_scale,
176 |         )
177 | 


--------------------------------------------------------------------------------
/jaxcmr/likelihood.py:
--------------------------------------------------------------------------------
  1 | from typing import Callable, Iterable, Mapping, Optional, Type
  2 | 
  3 | import numpy as np
  4 | from jax import jit, lax, vmap
  5 | from jax import numpy as jnp
  6 | 
  7 | from jaxcmr.helpers import all_rows_identical, log_likelihood
  8 | from jaxcmr.typing import (
  9 |     Array,
 10 |     Float,
 11 |     Float_,
 12 |     Integer,
 13 |     MemorySearch,
 14 |     MemorySearchModelFactory,
 15 | )
 16 | 
 17 | 
 18 | def predict_and_simulate_recalls(
 19 |     model: MemorySearch, choices: Integer[Array, " recall_events"]
 20 | ) -> tuple[MemorySearch, Float[Array, " recall_events"]]:
 21 |     """
 22 |     Return the updated model and the outcome probabilities of a chain of retrieval events.
 23 |     Args:
 24 |         model: the current memory search model.
 25 |         choices: the indices of the items to retrieve (1-indexed) or 0 to stop.
 26 |     """
 27 |     return lax.scan(
 28 |         lambda m, c: (m.retrieve(c), m.outcome_probability(c)), model, choices
 29 |     )
 30 | 
 31 | 
 32 | class MemorySearchLikelihoodFnGenerator:
 33 |     def __init__(
 34 |         self,
 35 |         model_factory: Type[MemorySearchModelFactory],
 36 |         dataset: dict[str, Integer[Array, " trials ?"]],
 37 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
 38 |     ) -> None:
 39 |         """Initialize the factory with the specified trials and trial data."""
 40 |         self.factory = model_factory(dataset, connections)
 41 |         self.create_model = self.factory.create_model
 42 | 
 43 |         # Store the presentation lists as a JAX array
 44 |         self.present_lists = jnp.array(dataset["pres_itemnos"])
 45 | 
 46 |         # Reindex the recalled items so they match the "present_lists" indexing
 47 |         trials = np.array(dataset["recalls"])
 48 |         for trial_index in range(trials.shape[0]):
 49 |             present = self.present_lists[trial_index]
 50 |             recall = trials[trial_index]
 51 |             reindexed = np.array(
 52 |                 [(present[item - 1] if item else 0) for item in recall]
 53 |             )
 54 |             trials[trial_index] = reindexed
 55 | 
 56 |         self.trials = jnp.array(trials)
 57 | 
 58 |     def init_model_for_retrieval(
 59 |         self,
 60 |         trial_index: Integer[Array, ""],
 61 |         parameters: Mapping[str, Float_],
 62 |     ) -> MemorySearch:
 63 |         """
 64 |         Create and initialize a MemorySearch model for a given trial's presentation list.
 65 |         """
 66 |         present = self.present_lists[trial_index]
 67 |         model = self.create_model(trial_index, parameters)
 68 |         model = lax.fori_loop(
 69 |             0, present.size, lambda i, m: m.experience(present[i]), model
 70 |         )
 71 |         return model.start_retrieving()
 72 | 
 73 |     def base_predict_trials(
 74 |         self,
 75 |         trial_indices: Integer[Array, " trials"],
 76 |         parameters: Mapping[str, Float_],
 77 |     ) -> Integer[Array, " trials recall_events"]:
 78 |         """
 79 |         Predict outcomes for each trial using a single initial model (from trial 0),
 80 |         skipping re-experiencing items for each subsequent trial.
 81 |         Only valid if all present-lists match.
 82 |         """
 83 |         model = self.init_model_for_retrieval(trial_indices[0], parameters)
 84 |         return vmap(predict_and_simulate_recalls, in_axes=(None, 0))(
 85 |             model, self.trials[trial_indices]
 86 |         )[1]
 87 | 
 88 |     def present_and_predict_trials(
 89 |         self,
 90 |         trial_indices: Integer[Array, " trials"],
 91 |         parameters: Mapping[str, Float_],
 92 |     ) -> Integer[Array, " trials recall_events"]:
 93 |         """
 94 |         Predict outcomes for each trial by creating a new model for each trial
 95 |         (re-experiencing items per trial).
 96 |         """
 97 | 
 98 |         def present_and_predict_trial(i):
 99 |             model = self.init_model_for_retrieval(i, parameters)
100 |             return predict_and_simulate_recalls(model, self.trials[i])[1]
101 | 
102 |         return vmap(present_and_predict_trial)(trial_indices)
103 | 
104 |     def base_predict_trials_loss(
105 |         self,
106 |         trial_indices: Integer[Array, " trials"],
107 |         parameters: Mapping[str, Float_],
108 |     ) -> Float[Array, ""]:
109 |         """Return negative log-likelihood for the 'base' approach."""
110 |         return log_likelihood(self.base_predict_trials(trial_indices, parameters))
111 | 
112 |     def present_and_predict_trials_loss(
113 |         self,
114 |         trial_indices: Integer[Array, " trials"],
115 |         parameters: Mapping[str, Float_],
116 |     ) -> Float[Array, ""]:
117 |         """Return negative log-likelihood for the 'present-and-predict' approach."""
118 |         return log_likelihood(
119 |             self.present_and_predict_trials(trial_indices, parameters)
120 |         )
121 | 
122 |     def __call__(
123 |         self,
124 |         trial_indices: Integer[Array, " trials"],
125 |         base_params: Mapping[str, Float_],
126 |         free_params: Iterable[str],
127 |     ) -> Callable[[np.ndarray], Float[Array, ""]]:
128 |         """
129 |         Return a loss function that:
130 |           1. Checks if all present-lists are identical for the selected trials.
131 |           2. Chooses either the 'base' approach (single initial model) or the
132 |              'present-and-predict' approach (fresh model per trial).
133 |           3. Expects an array of parameter values (single set or multiple sets).
134 |         """
135 |         # Decide which approach to use, based on whether all present-lists match
136 |         if all_rows_identical(self.present_lists[trial_indices]):
137 |             base_loss_fn = self.base_predict_trials_loss
138 |         else:
139 |             base_loss_fn = self.present_and_predict_trials_loss
140 | 
141 |         def specialized_loss_fn(params: Mapping[str, Float_]) -> Float[Array, ""]:
142 |             """Combine base_params and dynamic params, compute negative log-likelihood."""
143 |             return base_loss_fn(trial_indices, {**base_params, **params})
144 | 
145 |         @jit
146 |         def single_param_loss(x: jnp.ndarray) -> Float[Array, ""]:
147 |             """
148 |             x is shape (n_params,) for a single set of free parameters.
149 |             """
150 |             param_dict = {key: x[i] for i, key in enumerate(free_params)}
151 |             return specialized_loss_fn(param_dict)
152 | 
153 |         @jit
154 |         def multi_param_loss(x: jnp.ndarray) -> Float[Array, " n_samples"]:
155 |             """
156 |             x is shape (n_samples, n_params) for multiple sets of free parameters.
157 |             We'll vectorize over the first axis, returning shape (n_samples,).
158 |             """
159 | 
160 |             def loss_for_one_sample(x_row: jnp.ndarray) -> Float[Array, ""]:
161 |                 param_dict = {key: x_row[i] for i, key in enumerate(free_params)}
162 |                 return specialized_loss_fn(param_dict)
163 | 
164 |             # vmap applies loss_for_one_sample across the leading dimension of x
165 |             return vmap(loss_for_one_sample, in_axes=1)(x)
166 | 
167 |         # Return a function that checks the dimensionality of x at runtime
168 |         return lambda x: multi_param_loss(x) if x.ndim > 1 else single_param_loss(x)
169 | 


--------------------------------------------------------------------------------
/jaxcmr/linear_memory.py:
--------------------------------------------------------------------------------
  1 | from jax import lax
  2 | from jax import numpy as jnp
  3 | from simple_pytree import Pytree
  4 | 
  5 | from jaxcmr.math import power_scale
  6 | from jaxcmr.typing import Array, Float, Float_
  7 | 
  8 | 
  9 | class LinearMemory(Pytree):
 10 |     """Linear associative memory model for CMR.
 11 | 
 12 |     Attributes:
 13 |         input_size: the size of the input representation.
 14 |         output_size: the size of the output representation.
 15 |         state: the current state of the memory.
 16 |     """
 17 | 
 18 |     def __init__(
 19 |         self,
 20 |         state: Float[Array, " input_size output_size"],
 21 |         activation_scale: Float_,
 22 |     ):
 23 |         self.state = state
 24 |         self.activation_scale = activation_scale
 25 |         self.input_size = self.state.shape[0]
 26 |         self.output_size = self.state.shape[1]
 27 | 
 28 |     def associate(
 29 |         self,
 30 |         in_pattern: Float[Array, " input_size"],
 31 |         out_pattern: Float[Array, " output_size"],
 32 |         learning_rate: Float_,
 33 |     ) -> "LinearMemory":
 34 |         """Return the updated memory after associating input and output patterns.
 35 | 
 36 |         Args:
 37 |             in_pattern: a feature pattern for an input.
 38 |             out_pattern: a feature pattern for an output.
 39 |             learning_rate: the learning rate parameter.
 40 |         """
 41 |         return self.replace(
 42 |             state=self.state + (learning_rate * jnp.outer(in_pattern, out_pattern))
 43 |         )
 44 | 
 45 |     def probe(
 46 |         self,
 47 |         in_pattern: Float[Array, " input_size"],
 48 |     ) -> Float[Array, " output_size"]:
 49 |         """Return the output pattern associated with the input pattern in memory.
 50 | 
 51 |         Args:
 52 |             input_pattern: the input feature pattern.
 53 |         """
 54 |         return power_scale(jnp.dot(in_pattern, self.state), self.activation_scale)
 55 | 
 56 |     @classmethod
 57 |     def init_mfc(
 58 |         cls,
 59 |         item_count: int,
 60 |         context_feature_count: int,
 61 |         learning_rate: Float_,
 62 |         activation_scale: Float_,
 63 |     ) -> "LinearMemory":
 64 |         """Return a new linear associative item-to-context memory model.
 65 | 
 66 |         Initially, all items are associated with a unique context unit by `1-learning_rate`.
 67 |         To allow out-of-list contexts, set context_feature_count to `list_length + list_length + 1`.
 68 |         Otherwise use `list_length+1`.
 69 | 
 70 |         Args:
 71 |             item_count: the number of items in the memory model.
 72 |             context_feature_count: the number of unique units in context.
 73 |             learning_rate: the learning rate parameter.
 74 |             activation_scale: the activation scaling factor.
 75 |         """
 76 |         item_feature_count = item_count
 77 |         return cls(
 78 |             jnp.eye(item_feature_count, context_feature_count, 1) * (1 - learning_rate),
 79 |             activation_scale,
 80 |         )
 81 | 
 82 |     @classmethod
 83 |     def init_mcf(
 84 |         cls,
 85 |         item_count: int,
 86 |         context_feature_count: int,
 87 |         item_support: Float_,
 88 |         shared_support: Float_,
 89 |         activation_scale: Float_,
 90 |     ) -> "LinearMemory":
 91 |         """Return a new linear associative context-to-item memory model.
 92 | 
 93 |         Initially, in-list context units are associated with all items according to shared_support.
 94 |         They are also associated with a unique item according to item_support.
 95 |         Start-of-list and out-of-list context units receive no initial associations.
 96 | 
 97 |         To allow out-of-list contexts, set context_feature_count to `list_length + list_length + 1`.
 98 |         Otherwise use `list_length+1`.
 99 | 
100 |         Args:
101 |             item_count: the number of items in the memory model.
102 |             context_feature_count: the number of unique units in context.
103 |             item_support: initial association between an item and its own context feature
104 |             shared_support: initial association between an item and all other contextual features
105 |             activation_scale: the activation scaling factor.
106 |         """
107 |         base_memory = jnp.full((context_feature_count - 1, item_count), shared_support)
108 |         base_memory = lax.fori_loop(
109 |             0, item_count, lambda i, m: m.at[i, i].set(item_support), base_memory
110 |         )
111 |         start_list = jnp.zeros((1, item_count))
112 |         return cls(jnp.vstack((start_list, base_memory)), activation_scale)
113 | 


--------------------------------------------------------------------------------
/jaxcmr/math.py:
--------------------------------------------------------------------------------
 1 | import jax.numpy as jnp
 2 | from jax import lax
 3 | 
 4 | from jaxcmr.typing import Array, Float, Float_, Int_
 5 | 
 6 | lb = jnp.finfo(jnp.float32).eps
 7 | 
 8 | 
 9 | def power_scale(value: Float_, scale: Float_) -> Float:
10 |     """Returns value scaled by the exponent factor using logsumexp trick."""
11 |     log_activation = jnp.log(value)
12 |     return lax.cond(
13 |         jnp.logical_and(jnp.any(value != 0), scale != 1),
14 |         lambda _: jnp.exp(scale * (log_activation - jnp.max(log_activation))),
15 |         lambda _: value,
16 |         None,
17 |     )
18 | 
19 | 
20 | def exponential_primacy_decay(
21 |     study_index: Int_, primacy_scale: Float_, primacy_decay: Float_
22 | ):
23 |     """Returns the exponential primacy weighting for the specified study event.
24 | 
25 |     Args:
26 |         study_index: the index of the study event.
27 |         primacy_scale: the scale factor for primacy effect.
28 |         primacy_decay: the decay factor for primacy effect.
29 |     """
30 |     return primacy_scale * jnp.exp(-primacy_decay * study_index) + 1
31 | 
32 | 
33 | def exponential_stop_probability(
34 |     stop_probability_scale: Float_, stop_probability_growth: Float_, recall_total: Int_
35 | ):
36 |     """Returns the exponential stop probability for the specified recall event.
37 | 
38 |     Args:
39 |         stop_probability_scale: the scale factor for stop probability.
40 |         stop_probability_growth: the growth factor for stop probability.
41 |         recall_total: the total number of items recalled.
42 |     """
43 |     return stop_probability_scale * jnp.exp(recall_total * stop_probability_growth)
44 | 
45 | 
46 | def normalize_magnitude(
47 |     vector: Float[Array, " features"],
48 | ) -> Float[Array, " features"]:
49 |     """Return the input vector normalized to unit length."""
50 |     return vector / jnp.sqrt(jnp.sum(vector**2) + lb)
51 | 


--------------------------------------------------------------------------------
/jaxcmr/pnr.py:
--------------------------------------------------------------------------------
  1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/pnr.ipynb.
  2 | 
  3 | # %% auto 0
  4 | __all__ = ['fixed_pres_pnr', 'pnr', 'plot_pnr']
  5 | 
  6 | # %% ../notebooks/pnr.ipynb 2
  7 | from typing import Optional, Sequence
  8 | 
  9 | import jax.numpy as jnp
 10 | from jax import jit, vmap
 11 | from matplotlib import rcParams  # type: ignore
 12 | from matplotlib.axes import Axes
 13 | 
 14 | from .experimental.plotting import init_plot, plot_data, set_plot_labels
 15 | from .experimental.repetition import all_study_positions
 16 | from .helpers import apply_by_subject, find_max_list_length
 17 | from .typing import Array, Bool, Float, Integer
 18 | 
 19 | 
 20 | # %% ../notebooks/pnr.ipynb 4
 21 | def fixed_pres_pnr(
 22 |     recalls: Integer[Array, " trial_count recall_positions"],
 23 |     list_length: int,
 24 |     query_recall_position: int = 0,
 25 | ) -> Float[Array, " study_positions"]:
 26 |     """Returns probability of nth recall as a function of study position, assuming uniform study lists.
 27 | 
 28 |     Args:
 29 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 30 |         list_length: the length of the study list.
 31 |         query_recall_position: Which recall index (0-based) to analyze (e.g., 0 for first recall).
 32 |     """
 33 |     # Identify the item recalled at the query_recall_position for each trial.
 34 |     # Bin counts for each item number, ignoring 0 (no recall).
 35 |     # Divide by the total number of trials to get a probability.
 36 |     return jnp.bincount(
 37 |         recalls[:, query_recall_position].flatten(), length=list_length + 1
 38 |     )[1:] / len(recalls)
 39 | 
 40 | 
 41 | # %% ../notebooks/pnr.ipynb 6
 42 | def pnr(
 43 |     recalls: Integer[Array, " trial_count recall_positions"],
 44 |     presentations: Integer[Array, " trial_count study_positions"],
 45 |     list_length: int,
 46 |     size: int = 3,
 47 |     query_recall_position: int = 0,
 48 | ) -> Float[Array, " study_positions"]:
 49 |     """Returns probability of nth recall as a function of study position, allowing item repetitions.
 50 | 
 51 |     Args:
 52 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 53 |         presentations: trial by study position array of presented items. 1-indexed.
 54 |         list_length: the length of the study list.
 55 |         size: maximum number of study positions an item can be presented at.
 56 |         query_recall_position: Which recall index (0-based) to analyze (e.g., 0 for first recall).
 57 |     """
 58 |     # expanded_recalls: (trial_count, recall_positions, size) array
 59 |     # where each recalled item is mapped to all its possible study positions.
 60 |     expanded_recalls = vmap(
 61 |         vmap(all_study_positions, in_axes=(0, None, None)), in_axes=(0, 0, None)
 62 |     )(recalls, presentations, size)
 63 |     return fixed_pres_pnr(expanded_recalls, list_length, query_recall_position)
 64 | 
 65 | 
 66 | # %% ../notebooks/pnr.ipynb 8
 67 | def plot_pnr(
 68 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
 69 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
 70 |     query_recall_position: int = 0,
 71 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
 72 |     color_cycle: Optional[list[str]] = None,
 73 |     labels: Optional[Sequence[str]] = None,
 74 |     contrast_name: Optional[str] = None,
 75 |     axis: Optional[Axes] = None,
 76 |     size: int = 3,
 77 | ) -> Axes:
 78 |     """Returns Axes object with plotted probability of nth recall for given datasets and trial masks.
 79 | 
 80 |     Args:
 81 |         datasets: Datasets containing trial data to be plotted.
 82 |         trial_masks: Masks to filter trials in datasets.
 83 |         query_recall_position: Which recall index (0-based) to plot (e.g., 0 for first recall).
 84 |         distances: Unused, included for compatibility with other plotting functions.
 85 |         color_cycle: List of colors for plotting each dataset.
 86 |         labels: Names for each dataset for legend, optional.
 87 |         contrast_name: Name of contrast for legend labeling, optional.
 88 |         axis: Existing matplotlib Axes to plot on, optional.
 89 |         size: Maximum number of study positions an item can be presented at.
 90 |     """
 91 |     axis = init_plot(axis)
 92 | 
 93 |     if color_cycle is None:
 94 |         color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
 95 | 
 96 |     if labels is None:
 97 |         labels = [""] * len(datasets)
 98 | 
 99 |     # Convert single dict or single mask to list
100 |     if isinstance(datasets, dict):
101 |         datasets = [datasets]
102 |     if isinstance(trial_masks, jnp.ndarray):
103 |         trial_masks = [trial_masks]
104 | 
105 |     # Determine x-axis length
106 |     max_list_length = find_max_list_length(datasets, trial_masks)
107 | 
108 |     # For each dataset, apply the pnr function by subject, gather results, and plot
109 |     for data_index, data in enumerate(datasets):
110 |         subject_values = jnp.vstack(
111 |             apply_by_subject(
112 |                 data,
113 |                 trial_masks[data_index],
114 |                 jit(pnr, static_argnames=("size", "list_length")),
115 |                 size,
116 |                 query_recall_position,
117 |             )
118 |         )
119 | 
120 |         color = color_cycle.pop(0)
121 |         plot_data(
122 |             axis,
123 |             jnp.arange(max_list_length, dtype=int) + 1,
124 |             subject_values,
125 |             labels[data_index],
126 |             color,
127 |         )
128 | 
129 |     set_plot_labels(axis, "Study Position", "Probability of Nth Recall", contrast_name)
130 |     return axis
131 | 


--------------------------------------------------------------------------------
/jaxcmr/repcrp.py:
--------------------------------------------------------------------------------
  1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/repcrp.ipynb.
  2 | 
  3 | # %% auto 0
  4 | __all__ = ['set_false_at_index', 'RepCRPTabulation', 'tabulate_trial', 'repcrp', 'plot_rep_crp', 'plot_difference_rep_crp',
  5 |            'plot_first_rep_crp', 'plot_second_rep_crp']
  6 | 
  7 | # %% ../notebooks/repcrp.ipynb 3
  8 | from typing import Optional, Sequence
  9 | 
 10 | from jax import jit, lax, vmap
 11 | from jax import numpy as jnp
 12 | from matplotlib import rcParams  # type: ignore
 13 | from matplotlib.axes import Axes
 14 | from simple_pytree import Pytree
 15 | 
 16 | from .experimental.plotting import init_plot, plot_data, set_plot_labels
 17 | from .experimental.repetition import all_study_positions
 18 | from .helpers import apply_by_subject
 19 | from .typing import Array, Bool, Float, Int_, Integer
 20 | 
 21 | # %% ../notebooks/repcrp.ipynb 4
 22 | def set_false_at_index(vec: Bool[Array, " positions"], i: Int_):
 23 |     return lax.cond(i, lambda: (vec.at[i - 1].set(False), None), lambda: (vec, None))
 24 | 
 25 | 
 26 | class RepCRPTabulation(Pytree):
 27 |     "A tabulation of transitions between items during recall of a study list."
 28 | 
 29 |     def __init__(
 30 |         self,
 31 |         presentation: Integer[Array, " study_events"],
 32 |         first_recall: Int_,
 33 |         min_lag: int = 4,
 34 |         size: int = 3,
 35 |     ):
 36 |         self.size = size
 37 |         self.min_lag = min_lag
 38 |         self.list_length = presentation.size
 39 |         self.lag_range = self.list_length - 1
 40 |         self.all_positions = jnp.arange(1, self.list_length + 1, dtype=int)
 41 |         self.base_lags = jnp.zeros(self.lag_range * 2 + 1, dtype=int)
 42 |         self.item_study_positions = lax.map(
 43 |             lambda i: all_study_positions(i, presentation, size),
 44 |             self.all_positions,
 45 |         )
 46 | 
 47 |         self.actual_lags = jnp.zeros((size, self.lag_range * 2 + 1), dtype=int)
 48 |         self.avail_lags = jnp.zeros((size, self.lag_range * 2 + 1), dtype=int)
 49 | 
 50 |         self.previous_positions = self.item_study_positions[first_recall - 1]
 51 |         self.avail_recalls = jnp.ones(self.list_length, dtype=bool)
 52 |         self.avail_recalls = self.available_recalls_after(first_recall)
 53 | 
 54 |     # for updating avail_recalls: study positions still available for retrieval
 55 |     def available_recalls_after(self, recall: Int_) -> Bool[Array, " positions"]:
 56 |         "Update the study positions available to retrieve after a transition."
 57 |         study_positions = self.item_study_positions[recall - 1]
 58 |         return lax.scan(set_false_at_index, self.avail_recalls, study_positions)[0]
 59 | 
 60 |     # for updating actual_lags: lag-transitions actually made from the previous item
 61 |     def lags_from_previous(self, pos: Int_) -> Bool[Array, " size positions"]:
 62 |         "Identify the lag(s) from the study position(s) of the previous item."
 63 | 
 64 |         def f(prev):
 65 |             return lax.cond(
 66 |                 (pos * prev) == 0,
 67 |                 lambda: self.base_lags,
 68 |                 lambda: self.base_lags.at[pos - prev + self.lag_range].add(1),
 69 |             )
 70 | 
 71 |         # modified to not sum over prev_recall axis before returning
 72 |         return lax.map(f, self.previous_positions).astype(bool)
 73 | 
 74 |     def tabulate_actual_lags(self, recall: Int_) -> Integer[Array, " lags"]:
 75 |         "Tabulate the actual transition after a transition."
 76 |         recall_study_positions = self.item_study_positions[recall - 1]
 77 | 
 78 |         # modified to sum over current_recall axis while still separating over prev_recall
 79 |         new_lags = lax.map(self.lags_from_previous, recall_study_positions).astype(bool)
 80 |         return self.actual_lags + new_lags.sum(0)
 81 | 
 82 |     # for updating avail_lags: lag-transitions available from the previous item
 83 |     def available_lags_from(self, pos: Int_) -> Bool[Array, " lags"]:
 84 |         "Identify recallable lag transitions from the specified study position."
 85 |         return lax.cond(
 86 |             pos == 0,
 87 |             lambda: self.base_lags,
 88 |             lambda: self.base_lags.at[self.all_positions - pos + self.lag_range].add(
 89 |                 self.avail_recalls
 90 |             ),
 91 |         )
 92 | 
 93 |     def tabulate_available_lags(self) -> Integer[Array, " lags"]:
 94 |         "Tabulate available transitions after a transition."
 95 | 
 96 |         # modified to not sum over prev_recall axis before summing
 97 |         new_lags = lax.map(self.available_lags_from, self.previous_positions)
 98 |         return self.avail_lags + new_lags.astype(bool)
 99 | 
100 |     # unifying tabulation of actual/avail lags, previous positions, and avail recalls
101 |     def should_tabulate(self) -> Bool:
102 |         "Only consider transitions from item with at least two spaced-out study positions"
103 |         return len(self.previous_positions) > 1 and self.previous_positions[
104 |             -1
105 |         ] - self.previous_positions[-2] >= (self.min_lag + 1)
106 | 
107 |     def conditional_tabulate(self, recall: Int_) -> "RepCRPTabulation":
108 |         "Only tabulate actual and possible lags if the additional condition is met."
109 |         return lax.cond(
110 |             self.should_tabulate(),
111 |             lambda: self.replace(
112 |                 previous_positions=self.item_study_positions[recall - 1],
113 |                 avail_recalls=self.available_recalls_after(recall),
114 |                 actual_lags=self.tabulate_actual_lags(recall),
115 |                 avail_lags=self.tabulate_available_lags(),
116 |             ),
117 |             lambda: self.replace(
118 |                 previous_positions=self.item_study_positions[recall - 1],
119 |                 avail_recalls=self.available_recalls_after(recall),
120 |             ),
121 |         )
122 | 
123 |     def tabulate(self, recall: Int_) -> "RepCRPTabulation":
124 |         "Tabulate actual and possible serial lags of current from previous item."
125 |         return lax.cond(
126 |             recall,
127 |             lambda: self.conditional_tabulate(recall),
128 |             lambda: self,
129 |         )
130 | 
131 | # %% ../notebooks/repcrp.ipynb 6
132 | def tabulate_trial(
133 |     trial: Integer[Array, " recall_events"],
134 |     presentation: Integer[Array, " study_events"],
135 |     min_lag=4,
136 |     size: int = 2,
137 | ) -> tuple[Float[Array, " lags"], Float[Array, " lags"]]:
138 |     init = RepCRPTabulation(presentation, trial[0], min_lag, size)
139 |     tab = lax.fori_loop(1, trial.size, lambda i, t: t.tabulate(trial[i]), init)
140 |     return tab.actual_lags, tab.avail_lags
141 | 
142 | # %% ../notebooks/repcrp.ipynb 8
143 | def repcrp(
144 |     trials: Integer[Array, "trials recall_events"],
145 |     presentation: Integer[Array, "trials study_events"],
146 |     list_length: int,
147 |     min_lag=4,
148 |     size: int = 2,
149 | ) -> Float[Array, " lags"]:
150 |     """Returns lag-CRP centered around each study position of repeated items across trials. 
151 | 
152 |     Args:
153 |         trials: Recall events for each trial.
154 |         presentation: Study events for each trial.
155 |         list_length: Number of study events in each trial; unused.
156 |         min_lag: Minimum amount of study positions between two presentations of an item.
157 |         size: Maximum number of study positions an item can be presented
158 |     """
159 |     actual, possible = vmap(tabulate_trial, in_axes=(0, 0, None, None))(
160 |         trials, presentation, min_lag, size
161 |     )
162 |     return actual.sum(0) / possible.sum(0)
163 | 
164 | # %% ../notebooks/repcrp.ipynb 10
165 | def plot_rep_crp(
166 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
167 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
168 |     max_lag: int = 5,
169 |     min_lag: int = 4,
170 |     size: int = 2,
171 |     repetition_index: Optional[int] = None,
172 |     color_cycle: Optional[list[str]] = None,
173 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
174 |     labels: Optional[Sequence[str]] = None,
175 |     contrast_name: Optional[str] = None,
176 |     axis: Optional[Axes] = None,
177 | ) -> Axes:
178 |     """Returns Axes object with plotted prob of repetition lag-crp for datasets and trial masks.
179 | 
180 |     Args:
181 |         datasets: Datasets containing trial data to be plotted.
182 |         trial_masks: Masks to filter trials in datasets.
183 |         max_lag: Maximum number of study positions an item can be presented at.
184 |         min_lag: Minimum amount of study positions between two presentations of an item.
185 |         size: Maximum number of study positions an item can be presented at.
186 |         color_cycle: List of colors for plotting each dataset.
187 |         distances: Unused, included for compatibility with other plotting functions.
188 |         labels: Names for each dataset for legend, optional.
189 |         contrast_name: Name of contrast for legend labeling, optional.
190 |         axis: Existing matplotlib Axes to plot on, optional.
191 |     """
192 | 
193 |     axis = init_plot(axis)
194 | 
195 |     if color_cycle is None:
196 |         color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
197 | 
198 |     if labels is None:
199 |         labels = [""] * len(datasets)
200 | 
201 |     if isinstance(datasets, dict):
202 |         datasets = [datasets]
203 | 
204 |     if isinstance(trial_masks, jnp.ndarray):
205 |         trial_masks = [trial_masks]
206 | 
207 |     if isinstance(repetition_index, int):
208 |         repetition_indices = [repetition_index]
209 |     else:
210 |         repetition_indices = list(range(size))
211 | 
212 |     lag_interval = jnp.arange(-max_lag, max_lag + 1, dtype=int)
213 | 
214 |     for data_index, data in enumerate(datasets):
215 |         lag_range = jnp.max(data["listLength"][trial_masks[data_index]]) - 1
216 |         subject_values = apply_by_subject(
217 |             data,
218 |             trial_masks[data_index],
219 |             jit(repcrp, static_argnames=("size",)),
220 |             min_lag,
221 |             size,
222 |         )
223 | 
224 |         for repetition_index in repetition_indices:
225 |             repetition_subject_values = jnp.vstack(
226 |                 [each[repetition_index] for each in subject_values]
227 |             )[:, lag_range - max_lag : lag_range + max_lag + 1]
228 | 
229 |             color = color_cycle.pop(0)
230 |             plot_data(
231 |                 axis,
232 |                 lag_interval,
233 |                 repetition_subject_values,
234 |                 str(repetition_index + 1),
235 |                 color,
236 |             )
237 | 
238 |     set_plot_labels(axis, "Lag", "Conditional Resp. Prob.", contrast_name)
239 |     return axis
240 | 
241 | 
242 | # %% ../notebooks/repcrp.ipynb 11
243 | def plot_difference_rep_crp(
244 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
245 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
246 |     max_lag: int = 5,
247 |     min_lag: int = 4,
248 |     size: int = 2,
249 |     color_cycle: Optional[list[str]] = None,
250 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
251 |     labels: Optional[Sequence[str]] = None,
252 |     contrast_name: Optional[str] = None,
253 |     axis: Optional[Axes] = None,
254 | ) -> Axes:
255 |     """Returns Axes object with plotted prob of repetition lag-crp for datasets and trial masks.
256 | 
257 |     Args:
258 |         datasets: Datasets containing trial data to be plotted.
259 |         trial_masks: Masks to filter trials in datasets.
260 |         max_lag: Maximum number of study positions an item can be presented at.
261 |         min_lag: Minimum amount of study positions between two presentations of an item.
262 |         size: Maximum number of study positions an item can be presented at.
263 |         color_cycle: List of colors for plotting each dataset.
264 |         distances: Unused, included for compatibility with other plotting functions.
265 |         labels: Names for each dataset for legend, optional.
266 |         contrast_name: Name of contrast for legend labeling, optional.
267 |         axis: Existing matplotlib Axes to plot on, optional.
268 |     """
269 | 
270 |     axis = init_plot(axis)
271 | 
272 |     if color_cycle is None:
273 |         color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
274 | 
275 |     if labels is None:
276 |         labels = [""] * len(datasets)
277 | 
278 |     if isinstance(datasets, dict):
279 |         datasets = [datasets]
280 | 
281 |     if isinstance(trial_masks, jnp.ndarray):
282 |         trial_masks = [trial_masks]
283 | 
284 |     lag_interval = jnp.arange(-max_lag, max_lag + 1, dtype=int)
285 | 
286 |     for data_index, data in enumerate(datasets):
287 |         lag_range = jnp.max(data["listLength"][trial_masks[data_index]]) - 1
288 |         subject_values = apply_by_subject(
289 |             data,
290 |             trial_masks[data_index],
291 |             jit(repcrp, static_argnames=("size",)),
292 |             min_lag,
293 |             size,
294 |         )
295 | 
296 |         repetition_index = 0
297 |         repetition_subject_values = jnp.vstack(
298 |             [each[repetition_index] for each in subject_values]
299 |         )[:, lag_range - max_lag : lag_range + max_lag + 1]
300 | 
301 |         repetition_index = 1
302 |         repetition_subject_values -= jnp.vstack(
303 |             [each[repetition_index] for each in subject_values]
304 |         )[:, lag_range - max_lag : lag_range + max_lag + 1]
305 | 
306 |         color = color_cycle.pop(0)
307 |         plot_data(
308 |             axis,
309 |             lag_interval,
310 |             repetition_subject_values,
311 |             str(repetition_index + 1),
312 |             color,
313 |         )
314 | 
315 |     set_plot_labels(axis, "Lag", "Diff. Conditional Resp. Prob.", contrast_name)
316 |     return axis
317 | 
318 | 
319 | # %% ../notebooks/repcrp.ipynb 12
320 | def plot_first_rep_crp(
321 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
322 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
323 |     max_lag: int = 5,
324 |     min_lag: int = 4,
325 |     size: int = 2,
326 |     color_cycle: Optional[list[str]] = None,
327 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
328 |     labels: Optional[Sequence[str]] = None,
329 |     contrast_name: Optional[str] = None,
330 |     axis: Optional[Axes] = None,
331 | ) -> Axes:
332 |     "Convenience function to plot first repetition CRP."
333 |     return plot_rep_crp(
334 |         datasets,
335 |         trial_masks,
336 |         max_lag,
337 |         min_lag,
338 |         size,
339 |         repetition_index=0, # first repetition
340 |         color_cycle=color_cycle,
341 |         distances=distances,
342 |         labels=labels,
343 |         contrast_name=contrast_name,
344 |         axis=axis,
345 |     )
346 | 
347 | def plot_second_rep_crp(
348 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
349 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
350 |     max_lag: int = 5,
351 |     min_lag: int = 4,
352 |     size: int = 2,
353 |     color_cycle: Optional[list[str]] = None,
354 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
355 |     labels: Optional[Sequence[str]] = None,
356 |     contrast_name: Optional[str] = None,
357 |     axis: Optional[Axes] = None,
358 | ) -> Axes:
359 |     "Convenience function to plot first repetition CRP."
360 |     return plot_rep_crp(
361 |         datasets,
362 |         trial_masks,
363 |         max_lag,
364 |         min_lag,
365 |         size,
366 |         repetition_index=1, # first repetition
367 |         color_cycle=color_cycle,
368 |         distances=distances,
369 |         labels=labels,
370 |         contrast_name=contrast_name,
371 |         axis=axis,
372 |     )
373 | 


--------------------------------------------------------------------------------
/jaxcmr/simulation.py:
--------------------------------------------------------------------------------
  1 | from typing import Optional, Type, Mapping, Sequence
  2 | 
  3 | import numpy as np
  4 | from jax import lax, random, vmap
  5 | from jax import numpy as jnp
  6 | from jax.tree_util import tree_map
  7 | 
  8 | from jaxcmr.typing import (
  9 |     Array,
 10 |     Bool,
 11 |     Float,
 12 |     Float_,
 13 |     Int_,
 14 |     Integer,
 15 |     MemorySearchModelFactory,
 16 |     MemorySearch,
 17 |     PRNGKeyArray,
 18 | )
 19 | 
 20 | 
 21 | def segment_by_index(vector, index_vector) -> tuple[list[np.ndarray], np.ndarray]:
 22 |     """Segments a vector by index vector."""
 23 |     unique_indices, first_indices = np.unique(index_vector, return_index=True)
 24 |     unique_indices = unique_indices[np.argsort(first_indices)]
 25 |     return [vector[index_vector == idx] for idx in unique_indices], unique_indices
 26 | 
 27 | 
 28 | def item_to_study_positions(
 29 |     item: Int_,
 30 |     presentation: Integer[Array, " list_length"],
 31 |     size: int,
 32 | ):
 33 |     """Return the one-indexed study positions of an item in a 1D presentation sequence.
 34 | 
 35 |     Args:
 36 |         item: the item index.
 37 |         presentation: the 1D presentation sequence.
 38 |         size: number of non-zero entries to return.
 39 |     """
 40 |     return lax.cond(
 41 |         item == 0,
 42 |         lambda: jnp.zeros(size, dtype=int),
 43 |         lambda: jnp.nonzero(presentation == item, size=size, fill_value=-1)[0] + 1,
 44 |     )
 45 | 
 46 | # %%
 47 | 
 48 | def single_free_recall(
 49 |     model: MemorySearch, rng: PRNGKeyArray
 50 | ) -> tuple[MemorySearch, Integer[Array, ""]]:
 51 |     """Return model state and choice after performing a free recall event.
 52 | 
 53 |     Args:
 54 |         model: the current memory search model, after starting retrieval.
 55 |         rng: key for random number generation.
 56 |     """
 57 |     p_all = model.outcome_probabilities()
 58 |     choice = random.choice(rng, p_all.shape[0], p=p_all)
 59 |     return model.retrieve(choice), choice
 60 | 
 61 | 
 62 | def maybe_free_recall(model, rng):
 63 |     """Return model state and choice after performing a free recall event if the model is active.
 64 | 
 65 |     Args:
 66 |         model: the current memory search model, after starting retrieval.
 67 |         rng: key for random number generation.
 68 |     """
 69 |     return lax.cond(
 70 |         model.is_active,
 71 |         single_free_recall,
 72 |         lambda m, _: (m, 0),
 73 |         model,
 74 |         rng,
 75 |     )
 76 | 
 77 | 
 78 | def simulate_free_recall(
 79 |     model: MemorySearch, list_length: int, rng: PRNGKeyArray
 80 | ) -> tuple[MemorySearch, Integer[Array, ""] | PRNGKeyArray]:
 81 |     """Return model state and choices after performing free recall events until termination.
 82 | 
 83 |     Args:
 84 |         model: the current memory search model, after starting retrieval.
 85 |         list_length: the length of the study and recall sequences.
 86 |         rng: key for random number generation.
 87 |     """
 88 |     return lax.scan(maybe_free_recall, model, random.split(rng, list_length))
 89 | 
 90 | 
 91 | def simulate_study_and_free_recall(
 92 |     model: MemorySearch, present: Integer[Array, " study_events"], rng: PRNGKeyArray
 93 | ) -> tuple[MemorySearch, Integer[Array, ""]]:
 94 |     """Return model state and choices after simulating a trial.
 95 | 
 96 |     Args:
 97 |         model: the current memory search model.
 98 |         present: the indices of the items to present (1-indexed).
 99 |         rng: key for random number generation.
100 |     """
101 |     model = lax.fori_loop(0, present.size, lambda i, m: m.experience(present[i]), model)
102 |     model = model.start_retrieving()
103 |     return simulate_free_recall(model, present.size, rng)
104 | 
105 | 
106 | class MemorySearchSimulator:
107 |     def __init__(
108 |         self,
109 |         model_factory: Type[MemorySearchModelFactory],
110 |         dataset: dict[str, Integer[Array, " trials ?"]],
111 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
112 |     ) -> None:
113 |         """Initialize the factory with the specified trials and trial data."""
114 |         self.factory = model_factory(dataset, connections)
115 |         self.create_model = self.factory.create_model
116 |         self.present_lists = jnp.array(dataset["pres_itemnos"])
117 | 
118 |     def simulate_trial(
119 |         self,
120 |         trial_index: Integer[Array, ""],
121 |         parameters: Mapping[str, Float_],
122 |         rng: Integer[Array, " rng"],
123 |     ) -> tuple[MemorySearch, Integer[Array, ""]]:
124 |         present = self.present_lists[trial_index]
125 |         model = self.create_model(trial_index, parameters)
126 |         return simulate_study_and_free_recall(model, present, rng)
127 | 
128 |     def present_and_simulate_trials(
129 |         self,
130 |         trial_indices: Integer[Array, " trials"],
131 |         parameters: Mapping[str, Float_],
132 |         rng: Integer[Array, " rng"],
133 |     ) -> Integer[Array, " trials recall_events"]:
134 |         return vmap(self.simulate_trial, in_axes=(0, None, 0))(
135 |             trial_indices, parameters, random.split(rng, trial_indices.size)
136 |         )[1]
137 | 
138 | #%%
139 | 
140 | def preallocate_for_h5_dataset(
141 |     data: dict, trial_mask: Bool[Array, " trial_count"], experiment_count: int
142 | ) -> dict:
143 |     """Pre-allocates dictionary of numpy arrays based on trial mask and experiment count.
144 | 
145 |     Arrays are allocated for each key in the input data.
146 |     For 'recalls', the array is initialized with zeros.
147 | 
148 |     Args:
149 |         data: Dictionary containing dataset arrays.
150 |         trial_mask: Boolean array to select trials.
151 |         experiment_count: Number of times to replicate each array.
152 | 
153 |     Returns:
154 |         Dictionary with same keys as `data`; each is an array replicated by `experiment_count`.
155 |     """
156 |     """Pre-allocate a dictionary of arrays for a given trial mask."""
157 |     return tree_map(
158 |         lambda x: jnp.repeat(x[trial_mask], experiment_count, axis=0),
159 |         data,
160 |     )
161 | 
162 | 
163 | def simulate_h5_from_h5(
164 |     model_factory: Type[MemorySearchModelFactory],
165 |     dataset: dict[str, Integer[Array, " trials ?"]],
166 |     connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
167 |     parameters: dict[str, Float[Array, " subject_count"]],
168 |     trial_mask: Bool[Array, " trial_count"],
169 |     experiment_count: int,
170 |     rng: PRNGKeyArray,
171 |     size=3,
172 | ) -> dict[str, Integer[Array, " trials ?"]]:
173 |     """
174 |     Simulates dataset from existing dataset using a memory search model parameterized by subject.
175 |     
176 |     Args:
177 |         model_factory: Factory class for creating memory search model instances.
178 |         dataset: Original H5 dataset containing trial data.
179 |         connections: Optional connectivity matrix between items in the word pool.
180 |         parameters: Dictionary of simulation parameters, parameterized per subject.
181 |         trial_mask: Boolean array specifying which trials to simulate.
182 |         experiment_count: Number of simulation iterations per trial.
183 |         rng: PRNGKeyArray for random number generation.
184 |         size: Maximum number of study positions to return for each item during reindexing.
185 |     """
186 | 
187 |     sim_h5 = preallocate_for_h5_dataset(dataset, trial_mask, experiment_count)
188 |     simulator = MemorySearchSimulator(model_factory, sim_h5, connections)
189 |     subjects, _ = jnp.unique(dataset["subject"][trial_mask], return_counts=True)
190 |     trial_indices, _ = segment_by_index(
191 |         jnp.arange(sim_h5["recalls"].shape[0], dtype=int), sim_h5["subject"].flatten()
192 |     )
193 | 
194 |     # Handle parameter sampling if the number of subjects in parameters doesn't match
195 |     if len(parameters["subject"]) != len(trial_indices):
196 |         rng, rng_iter = random.split(rng)
197 |         shuffled_indices = random.choice(
198 |             rng_iter,
199 |             len(parameters["subject"]),
200 |             shape=(len(trial_indices),),
201 |             replace=True,
202 |         )
203 |         sim_parameters = {key: parameters[key][shuffled_indices] for key in parameters}
204 |     else:
205 |         sim_parameters = parameters
206 | 
207 |     reordering = jnp.concatenate(trial_indices)
208 |     for key in sim_h5:
209 |         sim_h5[key] = sim_h5[key][reordering]
210 | 
211 |     # Run simulations for each subject in a single call
212 |     rngs = random.split(rng, len(subjects))
213 |     recalls = [
214 |         simulator.present_and_simulate_trials(
215 |             jnp.array(trials),
216 |             {key: sim_parameters[key][subject] for key in sim_parameters},
217 |             rng_key,
218 |         )
219 |         for subject, trials, rng_key in zip(subjects, trial_indices, rngs)
220 |     ]
221 | 
222 |     sim_h5["recalls"] = jnp.concatenate(recalls)
223 | 
224 |     # Reindex item positions
225 |     reindex_fn = vmap(
226 |         vmap(item_to_study_positions, in_axes=(0, None, None)), in_axes=(0, 0, None)
227 |     )
228 |     sim_h5["recalls"] = reindex_fn(sim_h5["recalls"], sim_h5["pres_itemnos"], size)
229 |     sim_h5["recalls"] = sim_h5["recalls"][:, :, 0]
230 |     return sim_h5
231 | 
232 | def parameter_shifted_simulate_h5_from_h5(
233 |     model_factory: Type[MemorySearchModelFactory],
234 |     dataset: dict[str, Integer[Array, " trials ?"]],
235 |     connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
236 |     parameters: dict[str, Float[Array, " subject_count"]],
237 |     trial_mask: Bool[Array, " trial_count"],
238 |     experiment_count: int,
239 |     varied_parameter: str,
240 |     parameter_values: Sequence[float],
241 |     rng: PRNGKeyArray,
242 |     size=3,
243 | ) -> list[dict[str, Integer[Array, " trials ?"]]]:
244 |     """
245 |     Simulates multiple H5 datasets by systematically varying a specified parameter, using the updated 
246 |     simulate_h5_from_h5 implementation.
247 | 
248 |     For each value in `parameter_values`, this function creates a shifted parameters dictionary by 
249 |     overwriting the entire array for `varied_parameter` with the given value. It then invokes 
250 |     simulate_h5_from_h5— which handles dataset preallocation, parameter sampling, trial reordering, 
251 |     and item reindexing— to generate a simulated H5 dataset for that parameter setting.
252 | 
253 |     Args:
254 |         model_factory: Factory class for creating memory search model instances.
255 |         dataset: Original H5 dataset containing trial data.
256 |         connections: Optional connectivity matrix between items in the word pool.
257 |         parameters: Dictionary of simulation parameters, parameterized per subject.
258 |         trial_mask: Boolean array specifying which trials to simulate.
259 |         experiment_count: Number of simulation iterations per trial.
260 |         varied_parameter: The parameter key to be varied across simulations.
261 |         parameter_values: Sequence of values to assign to the varied parameter.
262 |         rng: PRNGKeyArray for random number generation.
263 |         size: Maximum number of study positions to return for each item during reindexing.
264 | 
265 |     Returns:
266 |         A list of H5-like datasets (dictionaries), each corresponding to simulation results generated 
267 |         with a different value for the varied parameter.
268 |     """
269 |     sim_h5s = []
270 |     for parameter_value in parameter_values:
271 |         rng, rng_split = random.split(rng)
272 |         shifted_parameters = {
273 |             **parameters,
274 |             varied_parameter: parameters[varied_parameter].at[:].set(parameter_value)
275 |         }
276 |         sim_h5 = simulate_h5_from_h5(
277 |             model_factory,
278 |             dataset,
279 |             connections,
280 |             shifted_parameters,
281 |             trial_mask,
282 |             experiment_count,
283 |             rng_split,
284 |             size,
285 |         )
286 |         sim_h5s.append(sim_h5)
287 |     return sim_h5s
288 | 
289 | 


--------------------------------------------------------------------------------
/jaxcmr/spc.py:
--------------------------------------------------------------------------------
  1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/spc.ipynb.
  2 | 
  3 | # %% auto 0
  4 | __all__ = ['fixed_pres_spc', 'spc', 'plot_spc']
  5 | 
  6 | # %% ../notebooks/spc.ipynb 3
  7 | import jax.numpy as jnp
  8 | from jax import vmap, jit
  9 | 
 10 | from typing import Optional, Sequence
 11 | from .typing import Array, Float, Integer, Bool
 12 | from .experimental.repetition import all_study_positions
 13 | from .experimental.plotting import init_plot, plot_data, set_plot_labels
 14 | from .helpers import apply_by_subject, find_max_list_length
 15 | 
 16 | from matplotlib.axes import Axes
 17 | from matplotlib import rcParams  # type: ignore
 18 | 
 19 | 
 20 | # %% ../notebooks/spc.ipynb 5
 21 | def fixed_pres_spc(
 22 |     recalls: Integer[Array, " trial_count recall_positions"], list_length: int
 23 | ) -> Float[Array, " study_positions"]:
 24 |     """Returns recall rate as a function of study position, assuming uniform study lists.
 25 | 
 26 |     Args:
 27 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 28 |         list_length: the length of the study list.
 29 |     """
 30 |     return jnp.bincount(recalls.flatten(), length=list_length + 1)[1:] / len(recalls)
 31 | 
 32 | # %% ../notebooks/spc.ipynb 8
 33 | def spc(
 34 |     recalls: Integer[Array, " trial_count recall_positions"],
 35 |     presentations: Integer[Array, " trial_count study_positions"],
 36 |     list_length: int,
 37 |     size: int = 3,
 38 | ) -> Float[Array, " study_positions"]:
 39 |     """Returns recall rate as a function of study position.
 40 | 
 41 |     Args:
 42 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 43 |         presentations: trial by study position array of presented items. 1-indexed.
 44 |         list_length: the length of the study list.
 45 |         size: maximum number of study positions an item can be presented at.
 46 |     """
 47 |     expanded_recalls = vmap(
 48 |         vmap(all_study_positions, in_axes=(0, None, None)), in_axes=(0, 0, None)
 49 |     )(recalls, presentations, size)
 50 | 
 51 |     counts = jnp.bincount(expanded_recalls.flatten(), length=list_length + 1)[1:]
 52 |     return counts / len(expanded_recalls)
 53 | 
 54 | # %% ../notebooks/spc.ipynb 10
 55 | def plot_spc(
 56 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
 57 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
 58 |     distances: Optional[Float[Array, "word_count word_count"]] = None,
 59 |     color_cycle: Optional[list[str]] = None,
 60 |     labels: Optional[Sequence[str]] = None,
 61 |     contrast_name: Optional[str] = None,
 62 |     axis: Optional[Axes] = None,
 63 |     size: int = 3,
 64 | ) -> Axes:
 65 |     """Returns Axes object with plotted serial position curve for datasets and trial masks.
 66 | 
 67 |     Args:
 68 |         datasets: Datasets containing trial data to be plotted.
 69 |         trial_masks: Masks to filter trials in datasets.
 70 |         color_cycle: List of colors for plotting each dataset.
 71 |         distances: Unused, included for compatibility with other plotting functions.
 72 |         labels: Names for each dataset for legend, optional.
 73 |         contrast_name: Name of contrast for legend labeling, optional.
 74 |         axis: Existing matplotlib Axes to plot on, optional.
 75 |         size: Maximum number of study positions an item can be presented at.
 76 |     """
 77 |     axis = init_plot(axis)
 78 | 
 79 |     if color_cycle is None:
 80 |         color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
 81 | 
 82 |     if labels is None:
 83 |         labels = [""] * len(datasets)
 84 | 
 85 |     if isinstance(datasets, dict):
 86 |         datasets = [datasets]
 87 | 
 88 |     if isinstance(trial_masks, jnp.ndarray):
 89 |         trial_masks = [trial_masks]
 90 | 
 91 |     max_list_length = find_max_list_length(datasets, trial_masks)
 92 |     for data_index, data in enumerate(datasets):
 93 |         subject_values = jnp.vstack(
 94 |             apply_by_subject(
 95 |                 data,
 96 |                 trial_masks[data_index],
 97 |                 jit(spc, static_argnames=("size", "list_length")),
 98 |                 size,
 99 |             )
100 |         )
101 | 
102 |         color = color_cycle.pop(0)
103 |         plot_data(
104 |             axis,
105 |             jnp.arange(max_list_length, dtype=int) + 1,
106 |             subject_values,
107 |             labels[data_index],
108 |             color,
109 |         )
110 | 
111 |     set_plot_labels(axis, "Study Position", "Recall Rate", contrast_name)
112 |     return axis
113 | 


--------------------------------------------------------------------------------
/jaxcmr/srac.py:
--------------------------------------------------------------------------------
  1 | # AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/srac.ipynb.
  2 | 
  3 | # %% auto 0
  4 | __all__ = ['fixed_pres_srac', 'srac', 'plot_srac']
  5 | 
  6 | # %% ../notebooks/srac.ipynb 2
  7 | from typing import Optional, Sequence
  8 | 
  9 | import jax.numpy as jnp
 10 | from jax import jit
 11 | from matplotlib import rcParams  # type: ignore
 12 | from matplotlib.axes import Axes
 13 | 
 14 | from .experimental.plotting import init_plot, plot_data, set_plot_labels
 15 | from .helpers import apply_by_subject, find_max_list_length
 16 | from .typing import Array, Bool, Float, Integer
 17 | 
 18 | # %% ../notebooks/srac.ipynb 4
 19 | def fixed_pres_srac(
 20 |     recalls: Integer[Array, " trial_count recall_positions"], list_length: int
 21 | ) -> Float[Array, " study_positions"]:
 22 |     """
 23 |     Returns position-specific recall accuracy, assuming uniform study lists.
 24 | 
 25 |     Args:
 26 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 27 |         list_length: the number of studied items in each trial.
 28 |     """
 29 |     return (recalls[:, :list_length] == jnp.arange(1, list_length + 1)).mean(axis=0)
 30 | 
 31 | # %% ../notebooks/srac.ipynb 7
 32 | def srac(
 33 |     recalls: Integer[Array, " trial_count recall_positions"],
 34 |     presentations: Integer[Array, " trial_count study_positions"],
 35 |     list_length: int,
 36 | ) -> Float[Array, " study_positions"]:
 37 |     """
 38 |     Returns position-specific recall accuracy for possibly non-uniform or repeated study lists.
 39 | 
 40 |     Args:
 41 |         recalls: trial by recall position array of recalled items. 1-indexed; 0 for no recall.
 42 |         presentations: trial by study position array of presented items. 1-indexed.
 43 |         list_length: the number of studied items in each trial.
 44 |     """
 45 |     return (recalls[:, :list_length] == presentations[:, :list_length]).mean(axis=0)
 46 | 
 47 | # %% ../notebooks/srac.ipynb 9
 48 | def plot_srac(
 49 |     datasets: Sequence[dict[str, jnp.ndarray]] | dict[str, jnp.ndarray],
 50 |     trial_masks: Sequence[Bool[Array, " trial_count"]] | Bool[Array, " trial_count"],
 51 |     color_cycle: Optional[list[str]] = None,
 52 |     labels: Optional[Sequence[str]] = None,
 53 |     contrast_name: Optional[str] = None,
 54 |     axis: Optional[Axes] = None,
 55 | ) -> Axes:
 56 |     """
 57 |     Plots serial recall accuracy curve for one or more datasets.
 58 | 
 59 |     Args:
 60 |         datasets: Datasets containing trial data to be plotted.
 61 |         trial_masks: Masks to filter trials in datasets.
 62 |         color_cycle: List of colors for plotting each dataset.
 63 |         distances: Unused, included for compatibility with other plotting functions.
 64 |         labels: Names for each dataset for legend, optional.
 65 |         contrast_name: Name of contrast for legend labeling, optional.
 66 |         axis: Existing matplotlib Axes to plot on, optional.
 67 | 
 68 |     Returns:
 69 |         The matplotlib Axes object containing the plot.
 70 |     """
 71 |     axis = init_plot(axis)
 72 | 
 73 |     if color_cycle is None:
 74 |         color_cycle = [c["color"] for c in rcParams["axes.prop_cycle"]]
 75 | 
 76 |     if isinstance(datasets, dict):
 77 |         datasets = [datasets]
 78 | 
 79 |     if isinstance(trial_masks, jnp.ndarray):
 80 |         trial_masks = [trial_masks]
 81 | 
 82 |     if labels is None:
 83 |         labels = ["" for _ in datasets]
 84 | 
 85 |     # Identify the largest list length across datasets, so we can plot consistently
 86 |     max_list_length = find_max_list_length(datasets, trial_masks)
 87 | 
 88 |     for data_index, data_dict in enumerate(datasets):
 89 |         # We'll apply the accurate_spc function to each subject, then stack
 90 |         subject_values = jnp.vstack(
 91 |             apply_by_subject(
 92 |                 data_dict,
 93 |                 trial_masks[data_index],
 94 |                 jit(srac, static_argnames=("list_length",)),
 95 |             )
 96 |         )
 97 | 
 98 |         # Plot
 99 |         color = color_cycle.pop(0)
100 |         xvals = jnp.arange(max_list_length) + 1
101 |         plot_data(axis, xvals, subject_values, labels[data_index], color)
102 | 
103 |     set_plot_labels(axis, "Study Position", "Serial Recall Accuracy", contrast_name)
104 |     return axis
105 | 


--------------------------------------------------------------------------------
/jaxcmr/state_analysis.py:
--------------------------------------------------------------------------------
 1 | import numpy as np
 2 | import matplotlib.pyplot as plt
 3 | import seaborn as sns
 4 | 
 5 | __all__ = ["matrix_heatmap", "instance_memory_heatmap"]
 6 | 
 7 | 
 8 | def matrix_heatmap(
 9 |     matrix,
10 |     figsize=(15, 15),
11 |     axis=None,
12 |     label_font_size=32,
13 |     annot_font_size=14,
14 |     print_threshold=.005,
15 |     title="",
16 | ):
17 |     """Plots an array of model states as a value-annotated heatmap with an arbitrary title. Omits annotations for cells
18 |     where values are effectively zero.
19 | 
20 |     Args:
21 |         matrix: an array of model states; columns representing unique feature indices and rows identifying unique update indices
22 |         title: a title for the generated plot,
23 |         label_font_size: font size for the axis labels
24 |         annot_font_size: font size for the annotations within each cell
25 |         axis: an existing matplotlib axis (optional)
26 | 
27 |     Returns:
28 |         (fig, axis): the figure and axis objects for the generated heatmap
29 |     """
30 | 
31 |     if matrix.ndim == 1:
32 |         matrix = np.expand_dims(matrix, axis=0)
33 | 
34 |     if axis is None:
35 |         fig, axis = plt.subplots(figsize=figsize)
36 |     else:
37 |         fig = axis.figure
38 | 
39 |     annot = np.array(
40 |         [
41 |             ["" if -print_threshold < val < print_threshold else f"{val:.2f}" for val in row]
42 |             for row in matrix
43 |         ]
44 |     )
45 | 
46 |     sns.heatmap(
47 |         matrix,
48 |         annot=annot,
49 |         fmt="",
50 |         annot_kws={"size": annot_font_size},
51 |         linewidths=0.5,
52 |         ax=axis,
53 |         cbar=True
54 |     )
55 | 
56 |     axis.set_xlabel("Feature Index", fontsize=label_font_size)
57 |     axis.set_ylabel("Update Index", fontsize=label_font_size)
58 |     axis.set_title(title, fontsize=label_font_size)
59 | 
60 |     return fig, axis
61 | 
62 | 
63 | def instance_memory_heatmap(
64 |     memory_state,
65 |     pre_experimental_size=0,
66 |     include_inputs=True,
67 |     include_outputs=True,
68 |     include_preexperimental=True,
69 |     title="Memory Traces",
70 | ):
71 |     assert (
72 |         include_inputs or include_outputs
73 |     ), "At least one of inputs or outputs must be included"
74 |     memory_shape = np.shape(memory_state)
75 |     fig_size = list(reversed(memory_shape))
76 |     plotted_memory = memory_state.copy()
77 | 
78 |     if not include_inputs:
79 |         fig_size[0] /= 2  # type: ignore
80 |         plotted_memory = plotted_memory[:, int(memory_shape[1] / 2) :]
81 |         title = f"Output {title}"
82 | 
83 |     if not include_outputs:
84 |         fig_size[0] /= 2  # type: ignore
85 |         plotted_memory = plotted_memory[:, : int(memory_shape[1] / 2)]
86 |         title = f"Input {title}"
87 | 
88 |     if not include_preexperimental:
89 |         fig_size[1] -= pre_experimental_size
90 |         plotted_memory = plotted_memory[int(pre_experimental_size) :]
91 |         title = f"Experimental {title}"
92 | 
93 |     return matrix_heatmap(plotted_memory, fig_size, title=title)


--------------------------------------------------------------------------------
/jaxcmr/summarize.py:
--------------------------------------------------------------------------------
  1 | import json
  2 | from typing import Mapping, Optional
  3 | 
  4 | import numpy as np
  5 | import pandas as pd
  6 | from jax import numpy as jnp
  7 | from jax.tree_util import tree_map
  8 | from scipy.stats import t, ttest_rel
  9 | 
 10 | from jaxcmr.typing import Array, Float, MemorySearchCreateFn
 11 | 
 12 | __all__ = [
 13 |     "bound_params",
 14 |     "load_opt_params",
 15 |     "validate_params",
 16 |     "calculate_ci",
 17 |     "summarize_parameters",
 18 | ]
 19 | 
 20 | 
 21 | def bound_params(
 22 |     params: Mapping[str, Float[Array, " popsize"]], bounds: Mapping[str, list[float]]
 23 | ) -> dict[str, Float[Array, " popsize"]]:
 24 |     """Return parameters scaled within bounds"""
 25 |     return tree_map(
 26 |         lambda param, bound: jnp.minimum(jnp.maximum(bound[0], param), bound[1]),
 27 |         params,
 28 |         bounds,
 29 |     )
 30 | 
 31 | 
 32 | def load_opt_params(base_param_path: str):
 33 |     """Load the base parameters and bounds for the optimization.
 34 | 
 35 |     Args:
 36 |         base_param_path: Path to the base parameters file.
 37 |     """
 38 |     with open(base_param_path) as f:
 39 |         fit_config = json.load(f)
 40 |     base_params = fit_config["fixed"].copy()
 41 |     param_bounds = fit_config["free"].copy()
 42 |     return {"base": base_params, "bounds": param_bounds}
 43 | 
 44 | 
 45 | def validate_params(
 46 |     loss_fn, model_init: MemorySearchCreateFn, trials, list_arg, opt_params
 47 | ):
 48 |     """Validate the bounds of the optimization parameters."""
 49 |     base_params, base_bounds = opt_params
 50 |     test_params = base_params.copy()
 51 |     for key, key_bounds in base_bounds.items():
 52 |         test_params[key] = key_bounds[0]
 53 |     loss_fn(model_init, list_arg, trials, test_params)
 54 |     for key, key_bounds in base_bounds.items():
 55 |         test_params[key] = key_bounds[1]
 56 |     loss_fn(model_init, list_arg, trials, test_params)
 57 | 
 58 | 
 59 | def calculate_ci(data: list[float], confidence=0.95) -> float:
 60 |     """Returns the confidence interval for a list of values.
 61 | 
 62 |     Args:
 63 |         data (list[float]): Values to calculate the confidence interval for.
 64 |         confidence (float, optional): The confidence level for the interval. Defaults to 0.95.
 65 |     """
 66 |     assert len(data) > 1
 67 |     n = len(data)
 68 |     stderr = np.std(np.array(data), ddof=1) / np.sqrt(n)
 69 |     return stderr * t.ppf((1 + confidence) / 2.0, n - 1)
 70 | 
 71 | 
 72 | def add_summary_lines(
 73 |     md_table: str,
 74 |     errors: list[list[float]],
 75 |     label: str,
 76 |     include_std=False,
 77 |     include_ci=False,
 78 | ) -> str:
 79 |     """Returns markdown table added lines for mean and errors of the values.
 80 | 
 81 |     Args:
 82 |         md_table: markdown table to add summary lines to.
 83 |         errors: values to summarize.
 84 |         label: label for the summary lines.
 85 |     """
 86 |     # Add a line for the mean
 87 |     md_table += f"| {label.replace('_', ' ')} "
 88 |     if include_std:
 89 |         md_table += "| mean "
 90 |     for variant_values in errors:
 91 |         if np.isnan(np.mean(variant_values)):
 92 |             md_table += "| "
 93 |         elif include_ci:
 94 |             md_table += f"| {np.mean(variant_values):.2f} +/- {calculate_ci(variant_values):.2f} "
 95 |         else:
 96 |             md_table += f"| {np.mean(variant_values):.2f} "
 97 |     md_table += "|\n"
 98 | 
 99 |     # Add a line for the standard deviation
100 |     if include_std:
101 |         md_table += "| | std "
102 |         for variant_values in errors:
103 |             if np.isnan(np.std(variant_values)):
104 |                 md_table += "| "
105 |                 continue
106 |             md_table += f"| {np.std(variant_values):.2f} "
107 |         md_table += "|\n"
108 | 
109 |     # Add a line for the confidence interval, but just if label is 'fitness'
110 |     # if label != "fitness":
111 |     #     return md_table
112 |     # md_table += "| | ci "
113 |     # for variant_values in errors:
114 |     #     md_table += f"| +/- {calculate_ci(variant_values):.2f} "
115 |     # md_table += "|\n"
116 |     return md_table
117 | 
118 | 
119 | def summarize_parameters(
120 |     model_data: list[dict],
121 |     query_parameters: Optional[list[str]] = None,
122 |     include_std=False,
123 |     include_ci=False,
124 | ):
125 |     """Returns markdown table summarizing the parameters across model variants.
126 | 
127 |     Computes the mean and confidence interval for each parameter across all subjects for each
128 |     model variant, with an option to specify which parameters to include in the table and t
129 |     their order.
130 | 
131 |     Args:
132 |     model_data : list[dict[str, dict[str, list]]]
133 |         A list of dictionaries with with dictionaries list values.
134 |         Each list corresponds to a model or fitting variant.
135 |         inner list is p
136 |     query_parameters : list[str], optional
137 |     """
138 |     # Extract model names from the first entry of each model variant list
139 |     model_names = [model_data[i]["name"] for i in range(len(model_data))]
140 | 
141 |     # identify query parameters; by default, is all unique fixed params across model variants
142 |     if query_parameters is None:
143 |         query_parameters = list(
144 |             set().union(*[entry["fixed"].keys() for entry in model_data])
145 |         )
146 | 
147 |     if include_std:
148 |         md_table = (
149 |             "| | | " + " | ".join([n.replace("_", " ") for n in model_names]) + " |\n"
150 |         )
151 |     else:
152 |         md_table = (
153 |             "| | " + " | ".join([n.replace("_", " ") for n in model_names]) + " |\n"
154 |         )
155 |     md_table += "|---" + ("|---" * (len(model_data) + 1)) + "|\n"
156 | 
157 |     # likelihood entry first
158 |     values = [variant_data["fitness"] for variant_data in model_data]
159 |     md_table = add_summary_lines(
160 |         md_table, values, "fitness", include_std=include_std, include_ci=include_ci
161 |     )
162 | 
163 |     # Compute the mean and confidence interval for params for each model variant
164 |     for param in query_parameters:
165 |         values = []
166 |         for variant_data in model_data:
167 |             try:
168 |                 values.append(variant_data["fits"][param])
169 |             except KeyError:
170 |                 values.append([np.nan for _ in range(len(variant_data["fitness"]))])
171 |         md_table = add_summary_lines(
172 |             md_table, values, param, include_std=include_std, include_ci=include_ci
173 |         )
174 | 
175 |     return md_table
176 | 
177 | 
178 | def generate_t_p_matrices(results: list[dict]) -> tuple[pd.DataFrame, pd.DataFrame]:
179 |     """Returns matrices of t-values and p-values from paired t-tests on model fitness results.
180 | 
181 |     Args:
182 |     - results: dicts containing `name` and subjectwise `fitness` data for each model.
183 |     """
184 |     # Extract model names
185 |     model_names = [model["name"] for model in results]
186 |     num_models = len(model_names)
187 | 
188 |     # Initialize matrices for t-values and 'less' p-values
189 |     t_values = np.zeros((num_models, num_models))
190 |     p_values_less = np.zeros((num_models, num_models))
191 | 
192 |     # Populate the matrices with t and p values from the paired t-tests
193 |     for i, model_a in enumerate(results):
194 |         for j, model_b in enumerate(results):
195 |             t, p_less = ttest_rel(
196 |                 model_a["fitness"], model_b["fitness"], alternative="less"
197 |             )
198 |             t_values[i, j] = t
199 |             p_values_less[i, j] = p_less
200 | 
201 |     # Convert matrices to pandas DataFrames, replacing NaNs with empty strings
202 |     df_t = pd.DataFrame(t_values, index=model_names, columns=model_names).replace(
203 |         np.nan, ""
204 |     )
205 |     df_p = pd.DataFrame(p_values_less, index=model_names, columns=model_names).replace(
206 |         np.nan, ""
207 |     )
208 | 
209 |     return df_t, df_p
210 | 
211 | 
212 | def calculate_aic_weights(results: list[dict]) -> pd.DataFrame:
213 |     """
214 |     Calculates the Akaike Information Criterion weights for a list of models.
215 | 
216 |     Parameters:
217 |     - results (list): A list of dictionaries, each with 'name', 'fitness', and 'free' (parameters).
218 | 
219 |     Returns:
220 |     - DataFrame: A pandas DataFrame with a row for each model and their AICw scores.
221 |     """
222 |     aics = []
223 |     names = []
224 | 
225 |     # Calculate AIC for each model
226 |     for model in results:
227 |         k = len(model["free"])  # number of parameters
228 |         log_likelihood = sum(model["fitness"])  # assuming fitness is log-likelihood
229 |         aic = 2 * k + 2 * log_likelihood
230 |         aics.append(aic)
231 |         names.append(model["name"])
232 | 
233 |     # Convert AICs to AIC weights
234 |     aics = np.array(aics)
235 |     min_aic = np.min(aics)
236 |     delta_aic = aics - min_aic
237 |     weights = np.exp(-0.5 * delta_aic)
238 |     aic_weights = weights / np.sum(weights)
239 | 
240 |     # Create DataFrame
241 |     df = pd.DataFrame({"Model": names, "AICw": aic_weights})
242 |     return df.sort_values(by="AICw", ascending=False)
243 | 
244 | 
245 | def winner_comparison_matrix(results: list[dict]) -> pd.DataFrame:
246 |     """Returns matrix of fractions of fitness in row model < in model j.
247 | 
248 |     Args:
249 |         - results: dicts containing each containing 'name' and 'fitness' data for each model.
250 |     """
251 |     # Extract model names
252 |     model_names = [model["name"] for model in results]
253 |     num_models = len(model_names)
254 | 
255 |     # Initialize matrix for comparison results
256 |     comparison_matrix = np.zeros((num_models, num_models))
257 | 
258 |     # Populate the matrix with comparison fractions
259 |     for i, model_a in enumerate(results):
260 |         for j, model_b in enumerate(results):
261 |             if i != j:
262 |                 # Calculate the fraction of fitness values in model_a that are lower than those in model_b
263 |                 comparison_scores = np.array(model_a["fitness"]) < np.array(
264 |                     model_b["fitness"]
265 |                 )
266 |                 comparison_fraction = np.mean(comparison_scores)
267 |                 comparison_matrix[i, j] = comparison_fraction
268 |             else:
269 |                 # Set diagonal to NaN for clarity, since self-comparison does not make sense here
270 |                 comparison_matrix[i, j] = np.nan
271 | 
272 |     return pd.DataFrame(comparison_matrix, index=model_names, columns=model_names)
273 | 


--------------------------------------------------------------------------------
/jaxcmr/typing.py:
--------------------------------------------------------------------------------
  1 | from typing import (
  2 |     Any,
  3 |     Callable,
  4 |     Mapping,
  5 |     NotRequired,
  6 |     Optional,
  7 |     Protocol,
  8 |     Type,
  9 |     TypedDict,
 10 |     runtime_checkable,
 11 | )
 12 | 
 13 | import numpy as np
 14 | from jaxtyping import Array, ArrayLike, Bool, Float, Integer, PRNGKeyArray, Real, Shaped
 15 | 
 16 | Float_ = Float[Array, ""] | float | int
 17 | Int_ = Integer[Array, ""] | int
 18 | Bool_ = Bool[Array, ""] | bool
 19 | 
 20 | __all__ = [
 21 |     "Array",
 22 |     "ArrayLike",
 23 |     "Bool",
 24 |     "Float",
 25 |     "Float_",
 26 |     "Int_",
 27 |     "Integer",
 28 |     "Real",
 29 |     "Shaped",
 30 |     "PRNGKeyArray",
 31 |     "MemorySearch",
 32 |     "MemorySearchCreateFn",
 33 |     "MemorySearchModelFactory",
 34 |     "Memory",
 35 |     "Context",
 36 |     "LossFnGenerator",
 37 |     "FitResult",
 38 |     "FittingAlgorithm",
 39 | ]
 40 | 
 41 | test = 0
 42 | 
 43 | 
 44 | @runtime_checkable
 45 | class MemorySearch(Protocol):
 46 |     """A model of memory search.
 47 | 
 48 |     Attributes:
 49 |         item_count: the number of item slots reserved in the model.
 50 |         is_active: indicates whether the model is active or not.
 51 |     """
 52 | 
 53 |     item_count: int
 54 |     is_active: Bool[Array, ""]
 55 | 
 56 |     def experience(self, choice: Int_) -> "MemorySearch":
 57 |         """Returns model after experiencing the specified study item.
 58 | 
 59 |         Args:
 60 |             choice: the index of the item to experience (1-indexed). 0 is ignored.
 61 |         """
 62 |         ...
 63 | 
 64 |     def start_retrieving(self) -> "MemorySearch":
 65 |         """Returns model after transitioning from study to retrieval mode."""
 66 |         ...
 67 | 
 68 |     def retrieve(self, choice: Int_) -> "MemorySearch":
 69 |         """Return model after simulating retrieval of the specified item or stopping.
 70 | 
 71 |         Args:
 72 |             choice: the index of the item to retrieve (1-indexed). 0 terminates retrieval.
 73 | 
 74 |         """
 75 |         ...
 76 | 
 77 |     def activations(self) -> Float[Array, " item_count"]:
 78 |         """Returns relative support for retrieval of each item given model state"""
 79 |         ...
 80 | 
 81 |     def outcome_probability(self, choice: Int_) -> Float[Array, ""]:
 82 |         """Return probability of the specified retrieval event.
 83 | 
 84 |         Args:
 85 |             choice: the index of the item to retrieve (1-indexed) or 0 to stop.
 86 |         """
 87 |         ...
 88 | 
 89 |     def outcome_probabilities(self) -> Float[Array, " recall_outcomes"]:
 90 |         """Return probabilities of all possible retrieval events."""
 91 |         ...
 92 | 
 93 | 
 94 | @runtime_checkable
 95 | class MemorySearchCreateFn(Protocol):
 96 |     """A factory for creating memory search models."""
 97 | 
 98 |     def __call__(
 99 |         self,
100 |         list_length: int,
101 |         parameters: Mapping[str, Float_],
102 |     ) -> MemorySearch:
103 |         """Create a new memory search model with the specified parameters."""
104 |         ...
105 | 
106 | 
107 | @runtime_checkable
108 | class MemorySearchModelFactory(Protocol):
109 |     def __init__(
110 |         self,
111 |         dataset: dict[str, Integer[Array, " trials ?"]],
112 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
113 |     ) -> None:
114 |         """Initialize the factory with the specified trials and trial data."""
115 |         ...
116 | 
117 |     def create_model(
118 |         self,
119 |         trial_index: Integer[Array, ""],
120 |         parameters: Mapping[str, Float_],
121 |     ) -> MemorySearch:
122 |         """Create a new memory search model with the specified parameters for the specified trial."""
123 |         ...
124 | 
125 | 
126 | @runtime_checkable
127 | class Memory(Protocol):
128 |     state: Float[Array, " input_size output_size"]
129 | 
130 |     @property
131 |     def input_size(self) -> int:
132 |         "The size of the input feature space."
133 |         ...
134 | 
135 |     @property
136 |     def output_size(self) -> int:
137 |         "The size of the output feature space."
138 |         ...
139 | 
140 |     def associate(
141 |         self,
142 |         in_pattern: Float[Array, " input_size"],
143 |         out_pattern: Float[Array, " output_size"],
144 |         learning_rate: Float_,
145 |     ) -> "Memory":
146 |         """Return the updated memory after associating input and output patterns.
147 | 
148 |         Args:
149 |             memory: the current memory model.
150 |             input_pattern: a feature pattern for an input.
151 |             out_pattern: a feature pattern for an output.
152 |             learning_rate: the learning rate parameter.
153 |         """
154 |         ...
155 | 
156 |     def probe(
157 |         self,
158 |         in_pattern: Float[Array, " input_size"],
159 |     ) -> Float[Array, " output_size"]:
160 |         """Return the output pattern associated with the input pattern in memory.
161 | 
162 |         Args:
163 |             memory: the current memory state.
164 |             in_pattern: the input feature pattern.
165 |             activation_scale: the activation scaling factor.
166 |         """
167 |         ...
168 | 
169 | 
170 | @runtime_checkable
171 | class Context(Protocol):
172 |     """Context representation for memory search models.
173 | 
174 |     Attributes:
175 |         state: the current state of the context.
176 |         initial_state: the initial state of the context.
177 |     """
178 | 
179 |     state: Float[Array, " context_feature_units"]
180 |     initial_state: Float[Array, " context_feature_units"]
181 |     size: int
182 | 
183 |     def integrate(
184 |         self,
185 |         context_input: Float[Array, " context_feature_units"],
186 |         drift_rate: Float_,
187 |     ) -> "Context":
188 |         """Returns context after integrating input representation.
189 | 
190 |         Args:
191 |             context_input: the input representation to be integrated into the contextual state.
192 |             drift_rate: The drift rate parameter.
193 |         """
194 |         ...
195 | 
196 | 
197 | @runtime_checkable
198 | class LossFnGenerator(Protocol):
199 |     """Generates loss function for model fitting."""
200 | 
201 |     def __init__(
202 |         self,
203 |         model_factory: Type[MemorySearchModelFactory],
204 |         dataset: dict[str, Integer[Array, " trials ?"]],
205 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
206 |     ) -> None:
207 |         """Initialize the factory with the specified trials and trial data."""
208 | 
209 |     def __call__(
210 |         self,
211 |         trial_indices: Integer[Array, " trials"],
212 |         base_params: Mapping[str, Float_],
213 |         free_params: Mapping[str, list[float]],
214 |     ) -> Callable[[np.ndarray], Float[Array, ""]]:
215 |         """Return the loss value for the specified model parameters."""
216 |         ...
217 | 
218 | 
219 | class FitResult(TypedDict):
220 |     """Typed dict describing the results of a fitting procedure."""
221 | 
222 |     fixed: dict[str, float]
223 |     """Dictionary of fixed parameters and their values."""
224 | 
225 |     free: dict[str, list[float]]
226 |     """Dictionary of free parameters and their [lower_bound, upper_bound] or similar spec."""
227 | 
228 |     fitness: list[float]
229 |     """List of one or more fitness values (e.g., for single-fit or per-subject fits)."""
230 | 
231 |     fits: dict[str, list[float]]
232 |     """Dictionary of parameter names -> optimized values (one or many)."""
233 | 
234 |     hyperparameters: dict[str, Any]
235 |     """Dictionary of hyperparameter names and their values used during fitting."""
236 | 
237 |     fit_time: float
238 |     """Total time (in seconds) taken to perform the fitting."""
239 | 
240 | 
241 | class RecallDataset(TypedDict):
242 |     """
243 |     A typed dictionary representing a dataset for free or serial recall experiments.
244 |     Each key maps to a 2D integer array of shape (n_trials, ?).
245 |     Rows correspond to trials; columns vary by field.
246 |     Zeros are used to indicate unused or padding entries, with values starting from 1.
247 | 
248 |     Required fields:
249 |         - subject:       Subject IDs (one per trial).
250 |         - listLength:    The length of the list presented in each trial.
251 |         - pres_itemids:  Cross-list item IDs presented in each trial
252 |                          (points to a global word pool).
253 |         - pres_itemnos:  Within-list item numbers (1-based indices; 0 indicates padding).
254 |         - rec_itemids:   Cross-list item IDs corresponding to items recalled.
255 |         - recalls:       Within-list item numbers for recalled items
256 |                          (1-based indices; 0 indicates padding).
257 | 
258 |     Optional fields:
259 |         - You can add as many as needed, with `NotRequired[...]`.
260 |     """
261 | 
262 |     # REQUIRED FIELDS
263 | 
264 |     subject: Integer[Array, "n_trials 1"]
265 |     """Subject ID for each trial (shape: [n_trials, 1])."""
266 | 
267 |     listLength: Integer[Array, "n_trials 1"]
268 |     """List length for each trial (shape: [n_trials, 1])."""
269 | 
270 |     pres_itemnos: Integer[Array, "n_trials num_presented"]
271 |     """Per-trial within-list item numbers (shape: [n_trials, num_presented]).
272 |     1-based indices with 0 for unused/padding entries."""
273 | 
274 |     recalls: Integer[Array, "n_trials num_recalled"]
275 |     """Within-list item numbers for recalled items (shape: [n_trials, num_recalled]).
276 |     1-based indices with 0 for unused/padding entries."""
277 | 
278 |     # OPTIONAL FIELDS, REQUIRED FOR SEMANTIC ANALYSIS
279 |     pres_itemids: Integer[Array, "n_trials num_presented"]
280 |     """Per-trial cross-list item IDs (shape: [n_trials, num_presented]). 
281 |     These IDs reference a global word pool and may repeat across trials."""
282 | 
283 |     rec_itemids: NotRequired[Integer[Array, "n_trials num_recalled"]]
284 |     """Cross-list item IDs for recalled items (shape: [n_trials, num_recalled])."""
285 | 
286 |     # OPTIONAL FIELDS, MISC
287 |     irt: NotRequired[Integer[Array, "n_trials num_recalled"]]
288 |     """Item response times for recalled items (shape: [n_trials, num_recalled])."""
289 | 
290 |     session: NotRequired[Integer[Array, "n_trials 1"]]
291 |     """Session IDs for each trial (shape: [n_trials, 1])."""
292 | 
293 |     listtype: NotRequired[Integer[Array, "n_trials 1"]]
294 |     """List type for each trial (shape: [n_trials, 1])."""
295 | 
296 |     list_type: NotRequired[Integer[Array, "n_trials 1"]]
297 |     """List type for each trial (shape: [n_trials, 1])."""
298 | 
299 | @runtime_checkable
300 | class FittingAlgorithm(Protocol):
301 |     """Protocol describing a fitting algorithm for memory search models.
302 | 
303 |     Returned dicts should contain the following keys:
304 |         - 'fixed': dict of fixed parameters and their values
305 |         - 'free': dict of free parameters and their parameter bounds
306 |         - 'fitness': fitness value(s) of the optimized parameters
307 |         - 'fits': dictionary of free parameters and their optimized value(s)
308 |     """
309 | 
310 |     def __init__(
311 |         self,
312 |         dataset: dict[str, Integer[Array, " trials ?"]],
313 |         connections: Optional[Integer[Array, " word_pool_items word_pool_items"]],
314 |         base_params: Mapping[str, Float_],
315 |         model_factory: Type["MemorySearchModelFactory"],
316 |         loss_fn_generator: Type["LossFnGenerator"],
317 |         hyperparams: Optional[dict[str, Any]] = None,
318 |     ):
319 |         """
320 |         Configure the fitting algorithm.
321 | 
322 |         Args:
323 |             dataset: The dataset containing trial data (including 'subject').
324 |             connections: Optional connectivity matrix.
325 |             base_params: A dictionary of parameters that are held fixed.
326 |             model_factory: Class implementing MemorySearchModelFactory.
327 |             loss_fn_generator: Class implementing LossFnGenerator.
328 |             hyperparams: Optional dictionary of hyperparameters for the fitting routine.
329 |                 May include 'bounds' (dict[str, list[float]]) and other keys
330 |                 like 'num_steps', 'pop_size', etc.
331 |         """
332 |         ...
333 | 
334 |     def single_fit(
335 |         self,
336 |         trial_mask: Bool[Array, " trials"],
337 |     ) -> FitResult:
338 |         """Returns result of fitting the model to the trials specified by the mask."""
339 |         ...
340 | 
341 |     def fit_to_subjects(
342 |         self,
343 |         trial_mask: Bool[Array, " trials"],
344 |     ) -> FitResult:
345 |         """Returns result of fitting the model separately to each subject present in the dataset."""
346 |         ...
347 | 
348 |     def fit(
349 |         self,
350 |         trial_mask: Bool[Array, " trials"],
351 |         fit_to_subjects: bool = True,
352 |     ) -> FitResult:
353 |         """Convenience wrapper for either single-fit or subject-by-subject fitting."""
354 |         ...
355 | 


--------------------------------------------------------------------------------

Here's a start at literate programming implementation of parameter shifting simulation using nbdev. I exported a jupyter notebook into a py:percent script. Can you make it into a fully fledged literate programming notebook instead of just a sequence of code cells? Would be nice if you could clean up its structure some too. 

```python
# %%
# %load_ext autoreload
# %autoreload 2

# %%
# params for this run
run_tag = "Parameter_Shifting"

model_name = "CMR"
model_factory_path = "jaxcmr.cmr.BaseCMRFactory"
fit_result_path = (
    "projects/cru_to_cmr/HealeyKahana2014_BaseCMR_benchmark.json"
)

# data params
data_name = "HealeyKahana2014"
data_query = "data['listtype'] == -1"
data_path = "data/HealeyKahana2014.h5"
target_directory = "figures/shifting/"
connection_path = "data/peers-all-mpnet-base-v2.npy"

# simulation params
experiment_count = 50

# analysis params
analysis_paths = [
    # "compmempy.analyses.repcrp.plot_first_rep_crp",
    # "compmempy.analyses.repcrp.plot_second_rep_crp",
    # "compmempy.analyses.repcrp.plot_difference_rep_crp",
    # "compmempy.analyses.rpl.plot_spacing",
    "jaxcmr.spc.plot_spc",
    "jaxcmr.crp.plot_crp",
    "jaxcmr.pnr.plot_pnr"
    # "compmempy.analyses.distance_crp.plot_distance_crp",
]

# %%
from jaxcmr.helpers import generate_trial_mask, load_data, import_from_string, format_floats
from jaxcmr.experimental.array import to_numba_typed_dict
import os
import json
import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from jaxcmr.simulation import parameter_shifted_simulate_h5_from_h5
from jax import random
from matplotlib import rcParams  # type: ignore


data = load_data(data_path)
trial_mask = generate_trial_mask(data, data_query)
model_factory = import_from_string(model_factory_path)
analyses = [import_from_string(path) for path in analysis_paths]

# embeddings = np.load(connection_path)
# connections = compute_similarity_matrix(embeddings)
max_size = np.max(data["pres_itemnos"])
connections = jnp.zeros((max_size, max_size))

with open(fit_result_path, "r") as f:
    fit_result = json.load(f)
    if "subject" not in fit_result["fits"]:
        fit_result["fits"]["subject"] = fit_result["subject"]

if not os.path.exists(target_directory):
    os.makedirs(target_directory)

# %%

bounds = {
    "encoding_drift_rate": [2.220446049250313e-16, 0.9999999999999998],
    "start_drift_rate": [2.220446049250313e-16, 0.9999999999999998],
    "recall_drift_rate": [2.220446049250313e-16, 0.9999999999999998],
    "shared_support": [2.220446049250313e-16, 99.9999999999999998],
    "item_support": [2.220446049250313e-16, 9.9999999999999998],
    "learning_rate": [2.220446049250313e-16, 0.9999999999999998],
    "primacy_scale": [2.220446049250313e-16, 99.9999999999999998],
    "primacy_decay": [2.220446049250313e-16, 99.9999999999999998],
    "stop_probability_scale": [2.220446049250313e-16, 0.9999999999999998],
    "stop_probability_growth": [2.220446049250313e-16, 9.9999999999999998],
    "choice_sensitivity": [2.220446049250313e-16, 99.9999999999999998],
    # "encoding_drift_decrease": [2.220446049250313e-16, 0.9999999999999998],
}

rng = random.PRNGKey(0)

for shifted_parameter, (min_value, max_value) in bounds.items():

    color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
    considered_values = jnp.linspace(start=min_value, stop=max_value, num=len(color_cycle))[:-1]
    rng, rng_iter = random.split(rng)
    sim = parameter_shifted_simulate_h5_from_h5(
        model_factory,
        data,
        connections,
        {key: jnp.array(val) for key, val in fit_result["fits"].items()},
        trial_mask,
        experiment_count,
        shifted_parameter,
        considered_values,
        rng_iter,
    )
    sim = [
        to_numba_typed_dict({key: np.array(val) for key, val in each.items()})
        for each in sim
    ]


    for analysis in analyses:
        figure_str = f"{model_name}_{shifted_parameter.title()}_{run_tag}_{analysis.__name__[5:]}_{data_name}.png"
        print(f"{target_directory}{figure_str}")
        color_cycle = [each["color"] for each in rcParams["axes.prop_cycle"]]
        cmap = plt.get_cmap('viridis')
        color_cycle = [cmap(i) for i in np.linspace(0, 1, len(considered_values))]
        color_cycle = [mcolors.rgb2hex(c) for c in color_cycle]
        _trial_mask = generate_trial_mask(sim[0], data_query)
        for i in range(len(sim)):
            sim[i]["subject"] *= 0

        axis = analysis(
            datasets=sim,
            trial_masks=[_trial_mask] * len(considered_values),
            color_cycle=color_cycle,
            distances=1 - connections,
            axis=plt.gca(),
            labels=format_floats(considered_values, 1),
            contrast_name=shifted_parameter,
            # handle_repetitions=handle_repetitions[i],

        )

        axis.tick_params(labelsize=14)
        axis.set_xlabel(axis.get_xlabel(), fontsize=16)
        axis.set_ylabel(axis.get_ylabel(), fontsize=16)
        axis.legend(loc="upper left", bbox_to_anchor=(1, 1))
        # axis.set_title(
        #     f"{data_name}_{model_name}_{shifted_parameter.title()}".replace("_", " ")
        # )
        # axis.set_title(shifted_parameter.replace("_", " ").title())

        
        plt.savefig(f"{target_directory}{figure_str}", bbox_inches="tight", dpi=600)
        plt.show()
```