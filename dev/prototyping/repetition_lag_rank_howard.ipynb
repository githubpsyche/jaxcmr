{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.DISABLE_JIT = True\n",
    "\n",
    "import numpy as np\n",
    "from psifr.stats import percentile_rank\n",
    "from compmempy.helpers.transforming_arrays import njit_apply_along_axis\n",
    "from compmempy.helpers.handling_data import item_to_study_positions, apply_by_subject\n",
    "from compmempy.helpers.loading_data import to_numba_typed_dict\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: reference implementation of lag-rank analysis in simplest case\n",
    "\n",
    "def lag_rank(trials: np.ndarray, list_length: int) -> float:\n",
    "    \"\"\"Summarize the tendency to group together nearby items by running a lag rank analysis.\n",
    "\n",
    "    For each recall, this determines the absolute lag of all remaining items available for recall and then calculates their percentile rank. Then the rank of the actual transition made is taken, scaled to vary between 0 (furthest item chosen) and 1 (nearest item chosen). Chance clustering will be 0.5; clustering above that value is evidence of a temporal contiguity effect.\n",
    "\n",
    "    Args:\n",
    "        trials (np.ndarray): rows represent trials and columns represents a recall position.\n",
    "            Nonzero identify study index of recalled items.\n",
    "        list_length (int): The number of item presentations in each trial.\n",
    "    \"\"\"\n",
    "    terminus = np.sum(trials != 0, axis=1)  # determine where each trial ends\n",
    "    actual_ranks = []\n",
    "\n",
    "    for trial_index in range(len(trials)):\n",
    "        possible_items = np.arange(list_length) + 1\n",
    "        previous_item = 0\n",
    "\n",
    "        for recall_index in range(terminus[trial_index]):\n",
    "            current_item = trials[trial_index, recall_index]\n",
    "            if recall_index > 0:\n",
    "                actual_lag = np.abs(current_item - previous_item)\n",
    "                possible_lags = np.abs(possible_items - previous_item)\n",
    "                actual_ranks.append(1 - percentile_rank(actual_lag, possible_lags))\n",
    "\n",
    "            previous_item = current_item\n",
    "            possible_items = possible_items[possible_items != previous_item]\n",
    "\n",
    "    return float(np.nanmean(actual_ranks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepetitionLagRank:\n",
    "    \"\"\"\n",
    "    Generalize lag-rank analyses to account for multiple presentations of each item, but exclusively consider transitions from repeated items and separately measure rank percentiles relative to each presentation index of the repeated items.\n",
    "\n",
    "    Assess lag-rank using the nearest study position to the reference study position used for tabulating lags. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, presentations: np.ndarray, max_repetitions: int = 2, min_lag: int = 4\n",
    "    ):\n",
    "        \"Pre-allocate arrays for lag-CRP tabulations.\"\n",
    "        list_length = np.max(np.sum(presentations != 0, axis=1))\n",
    "        self.lag_range = list_length - 1\n",
    "        self.min_lag = min_lag\n",
    "        self.max_repetitions = max_repetitions\n",
    "        self.actual_ranks = [[] for _ in range(max_repetitions)]\n",
    "\n",
    "    def should_tabulate(\n",
    "        self,\n",
    "        prev_study_positions: np.ndarray,\n",
    "    ) -> bool:\n",
    "        \"\"\"Only consider transitions from item with at least two spaced-out study positions\"\"\"\n",
    "        return len(prev_study_positions) > 1 and prev_study_positions[\n",
    "            -1\n",
    "        ] - prev_study_positions[-2] >= (self.min_lag + 1)\n",
    "\n",
    "    def tabulate_lags(\n",
    "        self,\n",
    "        previous_item: int,\n",
    "        current_item: int,\n",
    "        possible_items: np.ndarray,\n",
    "        item_study_positions: list[np.ndarray],\n",
    "    ):\n",
    "        \"Tabulate actual and possible serial lags of current from previous item.\"\n",
    "\n",
    "        prev_study_positions = item_study_positions[previous_item - 1]\n",
    "        current_study_positions = item_study_positions[current_item - 1]\n",
    "        for repetition_index, prev_study_position in enumerate(prev_study_positions):\n",
    "\n",
    "            # first track the minimum lags of the actual transition made\n",
    "            actual_lag = np.inf\n",
    "            for current_study_position in current_study_positions:\n",
    "                lag = np.abs(current_study_position - prev_study_position)\n",
    "                actual_lag = np.minimum(lag, actual_lag)\n",
    "\n",
    "            # then track minimum lags for each possible transition\n",
    "            possible_lags = []\n",
    "            for item in possible_items:\n",
    "                possible_lag = np.inf\n",
    "                possible_study_positions = item_study_positions[item - 1]\n",
    "                for possible_study_position in possible_study_positions:\n",
    "                    lag = np.abs(possible_study_position - prev_study_position)\n",
    "                    possible_lag = np.minimum(lag, possible_lag)\n",
    "                possible_lags.append(possible_lag)\n",
    "\n",
    "            percent_rank = 1 - percentile_rank(actual_lag, possible_lags)\n",
    "            # assert not np.isnan(percent_rank)\n",
    "            self.actual_ranks[repetition_index].append(percent_rank)\n",
    "\n",
    "    def tabulate_over_transitions(\n",
    "        self, trials: np.ndarray, presentations: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"Tabulate actual and possible lag transitions over a set of trials\"\n",
    "\n",
    "        terminus = np.sum(trials != 0, axis=1)\n",
    "\n",
    "        for trial_index in range(len(trials)):\n",
    "            presentation = presentations[trial_index]\n",
    "            item_count = np.max(presentation)\n",
    "            possible_items = np.arange(item_count) + 1\n",
    "            item_study_positions = njit_apply_along_axis(\n",
    "                item_to_study_positions, possible_items, presentation\n",
    "            )\n",
    "            previous_item = 0\n",
    "\n",
    "            for recall_index in range(terminus[trial_index]):\n",
    "                current_item = presentation[trials[trial_index, recall_index] - 1]\n",
    "                if recall_index > 0 and self.should_tabulate(\n",
    "                    item_study_positions[previous_item - 1],\n",
    "                ):\n",
    "                    self.tabulate_lags(\n",
    "                        previous_item,\n",
    "                        current_item,\n",
    "                        possible_items,\n",
    "                        item_study_positions,\n",
    "                    )\n",
    "\n",
    "                previous_item = current_item\n",
    "                possible_items = possible_items[possible_items != previous_item]\n",
    "\n",
    "        return np.nanmean(self.actual_ranks, axis=1)\n",
    "    \n",
    "def replagrank(\n",
    "    trials: np.ndarray, \n",
    "    presentations: np.ndarray,\n",
    "    list_length: int,\n",
    "    max_repetitions: int = 2,\n",
    "    min_lag: int = 4,\n",
    ") -> np.ndarray:\n",
    "    \"Summarize the tendency to group together nearby items by running a lag rank analysis.\"\n",
    "    lag_rank = RepetitionLagRank(presentations, max_repetitions, min_lag)\n",
    "    scores = lag_rank.tabulate_over_transitions(trials, presentations)\n",
    "    # return scores[0] - scores[1]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"HowardKahana2005\"\n",
    "data_path = \"data/HowardKahana2005.h5\"\n",
    "data_query = \"data['condition'] == 1\"\n",
    "\n",
    "data = to_numba_typed_dict({key: np.array(value) for key, value in load_data(data_path).items()})\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "\n",
    "max_repetitions = 3\n",
    "min_lag = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55458256, 0.55935317, 0.57284621],\n",
       "       [0.54180173, 0.59890606, 0.57060684],\n",
       "       [0.53393008, 0.55153121, 0.53496213],\n",
       "       [0.57366132, 0.63184187, 0.60833569],\n",
       "       [0.81606475, 0.7471299 , 0.65207017],\n",
       "       [0.59370536, 0.63382783, 0.62046825],\n",
       "       [0.56615354, 0.69242874, 0.67287036],\n",
       "       [0.58799843, 0.65033669, 0.70159359],\n",
       "       [0.65934723, 0.63793931, 0.63931203],\n",
       "       [0.5338701 , 0.59965742, 0.62899263],\n",
       "       [0.57808731, 0.62113122, 0.61312485],\n",
       "       [0.58809324, 0.61893685, 0.63010304],\n",
       "       [0.66350915, 0.64017319, 0.61181625],\n",
       "       [0.54788143, 0.58375505, 0.5761218 ],\n",
       "       [0.58420092, 0.59557518, 0.61315352],\n",
       "       [0.54054236, 0.58148119, 0.64388459],\n",
       "       [0.65721444, 0.64702842, 0.68813806],\n",
       "       [0.5530729 , 0.64742351, 0.60206152],\n",
       "       [0.53660104, 0.59874394, 0.57247216],\n",
       "       [0.52473799, 0.54955018, 0.61529558],\n",
       "       [0.70289568, 0.71807535, 0.68180693],\n",
       "       [0.59832981, 0.51317758, 0.55687888],\n",
       "       [0.57025241, 0.62370414, 0.57735659],\n",
       "       [0.45440874, 0.43530984, 0.4953402 ],\n",
       "       [0.57092866, 0.57147805, 0.57880318],\n",
       "       [0.52180849, 0.51171794, 0.48190661],\n",
       "       [0.57339638, 0.60193973, 0.59400288],\n",
       "       [0.5258892 , 0.56052442, 0.62285872],\n",
       "       [0.56610243, 0.4765133 , 0.50312161],\n",
       "       [0.5951985 , 0.57224376, 0.55762706],\n",
       "       [0.62735722, 0.57236724, 0.54816227],\n",
       "       [0.64508869, 0.66837728, 0.62389608],\n",
       "       [0.56636675, 0.55707567, 0.5011274 ],\n",
       "       [0.64250727, 0.70615202, 0.69357691],\n",
       "       [0.53051909, 0.58024319, 0.585744  ],\n",
       "       [0.70912135, 0.67660852, 0.59925694],\n",
       "       [0.59454183, 0.59896037, 0.57279881],\n",
       "       [0.6117724 , 0.62941208, 0.57829309],\n",
       "       [0.53703909, 0.55747328, 0.63185823],\n",
       "       [0.53731393, 0.51735181, 0.54962787],\n",
       "       [0.58068436, 0.54370152, 0.57813287],\n",
       "       [0.56864542, 0.57701484, 0.54422983],\n",
       "       [0.57843252, 0.6240097 , 0.63883205],\n",
       "       [0.54871564, 0.49525195, 0.48508345],\n",
       "       [0.58940128, 0.60578622, 0.61153163],\n",
       "       [0.58024328, 0.5389512 , 0.56423059],\n",
       "       [0.57520759, 0.57734075, 0.59870892],\n",
       "       [0.54018556, 0.52827883, 0.48313794],\n",
       "       [0.57730403, 0.59647282, 0.57986429],\n",
       "       [0.53275572, 0.52877221, 0.54237952],\n",
       "       [0.63682275, 0.63592068, 0.6386255 ],\n",
       "       [0.54223971, 0.54130842, 0.59277863],\n",
       "       [0.53272947, 0.5249297 , 0.54011252],\n",
       "       [0.49941578, 0.55191867, 0.57784818],\n",
       "       [0.59152072, 0.62507518, 0.64091714],\n",
       "       [0.53234216, 0.61222777, 0.59584325],\n",
       "       [0.59609019, 0.5866973 , 0.57065999],\n",
       "       [0.55405709, 0.54240449, 0.5194937 ],\n",
       "       [0.66608952, 0.67168465, 0.64860478],\n",
       "       [0.60585999, 0.60619934, 0.5478742 ],\n",
       "       [0.66608952, 0.67168465, 0.64860478],\n",
       "       [0.61242804, 0.64062523, 0.68391313],\n",
       "       [0.6659635 , 0.63520918, 0.65217003],\n",
       "       [0.63561098, 0.63290246, 0.56128793],\n",
       "       [0.65158926, 0.66085076, 0.63565832],\n",
       "       [0.66276498, 0.64813004, 0.61664807]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(apply_by_subject(\n",
    "    data,\n",
    "    trial_mask,\n",
    "    replagrank,\n",
    "    max_repetitions,\n",
    "    min_lag,\n",
    "))\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -1.0248325864793413\n",
      "Two-tailed P-value: 0.8453794324837075\n",
      "There is no statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.5869558009074269\n",
      "Mean Second Sample: 0.5939309807352077\n",
      "Standard Error First Sample: 0.0071141390595325935\n",
      "Standard Error Second Sample: 0.00646714770370032\n",
      "Mean Difference: -0.00697517982778077\n",
      "Standard Error Difference: 0.006754406482273385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 2]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2, alternative=\"greater\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"Two-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of CMR Fitted to Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| | | Cond34LohnasKahana2014 BaseCMR Model Fitting |\n",
       "|---|---|---|\n",
       "| fitness | mean | 817.31 +/- 70.82 |\n",
       "| | std | 203.20 |\n",
       "| item support | mean | 14.62 +/- 7.60 |\n",
       "| | std | 21.82 |\n",
       "| recall drift rate | mean | 0.91 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| learning rate | mean | 0.47 +/- 0.08 |\n",
       "| | std | 0.23 |\n",
       "| stop probability scale | mean | 0.02 +/- 0.01 |\n",
       "| | std | 0.02 |\n",
       "| semantic scale | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| stop probability growth | mean | 0.23 +/- 0.03 |\n",
       "| | std | 0.10 |\n",
       "| choice sensitivity | mean | 29.08 +/- 12.52 |\n",
       "| | std | 35.92 |\n",
       "| mcf trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| shared support | mean | 8.95 +/- 5.74 |\n",
       "| | std | 16.47 |\n",
       "| primacy decay | mean | 20.74 +/- 10.79 |\n",
       "| | std | 30.97 |\n",
       "| primacy scale | mean | 11.76 +/- 8.89 |\n",
       "| | std | 25.52 |\n",
       "| start drift rate | mean | 0.61 +/- 0.11 |\n",
       "| | std | 0.32 |\n",
       "| mfc trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| mfc choice sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| encoding drift rate | mean | 0.71 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| semantic choice sensitivity | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jaxcmr_research.helpers.hdf5 import simulate_h5_from_h5\n",
    "from jax import random\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data\n",
    "from jaxcmr_research.helpers.misc import summarize_parameters, import_from_string\n",
    "import numpy as np\n",
    "from jaxcmr_research.helpers.array import compute_similarity_matrix\n",
    "from jax import numpy as jnp\n",
    "import json\n",
    "from IPython.display import Markdown  # type: ignore\n",
    "\n",
    "data_name = \"Cond34LohnasKahana2014\"\n",
    "data_path = \"data/LohnasKahana2014.h5\"\n",
    "data_query = \"data['list_type'] >= 3\"\n",
    "connection_path = \"data/peers-all-mpnet-base-v2.npy\"\n",
    "experiment_count = 100\n",
    "seed = 0\n",
    "fit_result_path = (\n",
    "    \"notebooks/Model_Fitting/Cond34LohnasKahana2014_BaseCMR_Model_Fitting.json\"\n",
    ")\n",
    "\n",
    "\n",
    "data = load_data(data_path)\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "embeddings = np.load(connection_path)\n",
    "connections = compute_similarity_matrix(embeddings) # unused here\n",
    "model_factory_path = \"jaxcmr_research.cmr.BaseCMRFactory\"\n",
    "model_factory = import_from_string(model_factory_path)\n",
    "with open(fit_result_path, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    if \"subject\" not in results[\"fits\"]:\n",
    "        results[\"fits\"][\"subject\"] = results[\"subject\"]\n",
    "\n",
    "\n",
    "Markdown(\n",
    "    summarize_parameters([results], None, include_std=True, include_ci=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(seed)\n",
    "rng, rng_iter = random.split(rng)\n",
    "sim = simulate_h5_from_h5(\n",
    "    model_factory=model_factory,\n",
    "    dataset=data,\n",
    "    connections=connections,\n",
    "    parameters={key: jnp.array(val) for key, val in results[\"fits\"].items()},\n",
    "    trial_mask=trial_mask,\n",
    "    experiment_count=experiment_count,\n",
    "    rng=rng_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75887673, 0.7302167 ],\n",
       "       [0.69681988, 0.67291635],\n",
       "       [0.70955402, 0.7050469 ],\n",
       "       [0.66869269, 0.65605559],\n",
       "       [0.66450965, 0.64945109],\n",
       "       [0.69580354, 0.71301576],\n",
       "       [0.64725494, 0.65425818],\n",
       "       [0.62592041, 0.63509403],\n",
       "       [0.72164041, 0.71636556],\n",
       "       [0.64914463, 0.66624445],\n",
       "       [0.80162998, 0.74868433],\n",
       "       [0.65573286, 0.65381038],\n",
       "       [0.62425521, 0.61936696],\n",
       "       [0.77323767, 0.72573166],\n",
       "       [0.64427101, 0.63551851],\n",
       "       [0.7450803 , 0.72672831],\n",
       "       [0.78015708, 0.75350908],\n",
       "       [0.76767412, 0.74310528],\n",
       "       [0.64175197, 0.64298237],\n",
       "       [0.6487697 , 0.64219973],\n",
       "       [0.63852844, 0.64632898],\n",
       "       [0.63352686, 0.66216156],\n",
       "       [0.49074815, 0.50246911],\n",
       "       [0.67416181, 0.67416415],\n",
       "       [0.72535477, 0.74980616],\n",
       "       [0.65746941, 0.65224424],\n",
       "       [0.78740716, 0.74231676],\n",
       "       [0.62743774, 0.66551034],\n",
       "       [0.64185827, 0.65641555],\n",
       "       [0.73360079, 0.73135758],\n",
       "       [0.68911805, 0.64958527],\n",
       "       [0.75720051, 0.75354283],\n",
       "       [0.70907691, 0.69936759],\n",
       "       [0.6342391 , 0.61895271],\n",
       "       [0.67657392, 0.6799025 ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(apply_by_subject(\n",
    "    sim,\n",
    "    generate_trial_mask(sim, data_query),\n",
    "    replagrank,\n",
    "    max_repetitions,\n",
    "    min_lag,\n",
    "))\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 1.7770109995486576\n",
      "Two-tailed P-value: 0.08451454111037006\n",
      "There is no statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.6856308203240852\n",
      "Mean Second Sample: 0.6792693290738305\n",
      "Standard Error First Sample: 0.010549645458655145\n",
      "Standard Error Second Sample: 0.008753099626656921\n",
      "Mean Difference: 0.006361491250254665\n",
      "Standard Error Difference: 0.0035283710856964506\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Ensure the data has exactly two columns\n",
    "if data.shape[1] != 2:\n",
    "    raise ValueError(\"Data must have exactly two columns\")\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 1]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"Two-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxcmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
