{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.DISABLE_JIT = True\n",
    "\n",
    "import numpy as np\n",
    "from psifr.stats import percentile_rank\n",
    "from compmempy.helpers.transforming_arrays import njit_apply_along_axis\n",
    "from compmempy.helpers.handling_data import item_to_study_positions, apply_by_subject\n",
    "from compmempy.helpers.loading_data import to_numba_typed_dict\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: reference implementation of lag-rank analysis in simplest case\n",
    "\n",
    "def lag_rank(trials: np.ndarray, list_length: int) -> float:\n",
    "    \"\"\"Summarize the tendency to group together nearby items by running a lag rank analysis.\n",
    "\n",
    "    For each recall, this determines the absolute lag of all remaining items available for recall and then calculates their percentile rank. Then the rank of the actual transition made is taken, scaled to vary between 0 (furthest item chosen) and 1 (nearest item chosen). Chance clustering will be 0.5; clustering above that value is evidence of a temporal contiguity effect.\n",
    "\n",
    "    Args:\n",
    "        trials (np.ndarray): rows represent trials and columns represents a recall position.\n",
    "            Nonzero identify study index of recalled items.\n",
    "        list_length (int): The number of item presentations in each trial.\n",
    "    \"\"\"\n",
    "    terminus = np.sum(trials != 0, axis=1)  # determine where each trial ends\n",
    "    actual_ranks = []\n",
    "\n",
    "    for trial_index in range(len(trials)):\n",
    "        possible_items = np.arange(list_length) + 1\n",
    "        previous_item = 0\n",
    "\n",
    "        for recall_index in range(terminus[trial_index]):\n",
    "            current_item = trials[trial_index, recall_index]\n",
    "            if recall_index > 0:\n",
    "                actual_lag = np.abs(current_item - previous_item)\n",
    "                possible_lags = np.abs(possible_items - previous_item)\n",
    "                actual_ranks.append(1 - percentile_rank(actual_lag, possible_lags))\n",
    "\n",
    "            previous_item = current_item\n",
    "            possible_items = possible_items[possible_items != previous_item]\n",
    "\n",
    "    return float(np.nanmean(actual_ranks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepetitionLagRank:\n",
    "    \"\"\"\n",
    "    Generalize lag-rank analyses to account for multiple presentations of each item, but exclusively consider transitions from repeated items and separately measure rank percentiles relative to each presentation index of the repeated items.\n",
    "\n",
    "    Assess lag-rank using the nearest study position to the reference study position used for tabulating lags. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self, presentations: np.ndarray, max_repetitions: int = 2, min_lag: int = 4\n",
    "    ):\n",
    "        \"Pre-allocate arrays for lag-CRP tabulations.\"\n",
    "        list_length = np.max(np.sum(presentations != 0, axis=1))\n",
    "        self.lag_range = list_length - 1\n",
    "        self.min_lag = min_lag\n",
    "        self.max_repetitions = max_repetitions\n",
    "        self.actual_ranks = [[] for _ in range(max_repetitions)]\n",
    "\n",
    "    def should_tabulate(\n",
    "        self,\n",
    "        prev_study_positions: np.ndarray,\n",
    "    ) -> bool:\n",
    "        \"\"\"Only consider transitions from item with at least two spaced-out study positions\"\"\"\n",
    "        return len(prev_study_positions) > 1 and prev_study_positions[\n",
    "            -1\n",
    "        ] - prev_study_positions[-2] >= (self.min_lag + 1)\n",
    "\n",
    "    def tabulate_lags(\n",
    "        self,\n",
    "        previous_item: int,\n",
    "        current_item: int,\n",
    "        possible_items: np.ndarray,\n",
    "        item_study_positions: list[np.ndarray],\n",
    "    ):\n",
    "        \"Tabulate actual and possible serial lags of current from previous item.\"\n",
    "\n",
    "        prev_study_positions = item_study_positions[previous_item - 1]\n",
    "        current_study_positions = item_study_positions[current_item - 1]\n",
    "        for repetition_index, prev_study_position in enumerate(prev_study_positions):\n",
    "\n",
    "            # first track the minimum lags of the actual transition made\n",
    "            actual_lag = np.inf\n",
    "            for current_study_position in current_study_positions:\n",
    "                lag = np.abs(current_study_position - prev_study_position)\n",
    "                actual_lag = np.minimum(lag, actual_lag)\n",
    "\n",
    "            # then track minimum lags for each possible transition\n",
    "            possible_lags = []\n",
    "            for item in possible_items:\n",
    "                possible_lag = np.inf\n",
    "                possible_study_positions = item_study_positions[item - 1]\n",
    "                for possible_study_position in possible_study_positions:\n",
    "                    lag = np.abs(possible_study_position - prev_study_position)\n",
    "                    possible_lag = np.minimum(lag, possible_lag)\n",
    "                possible_lags.append(possible_lag)\n",
    "\n",
    "            percent_rank = 1 - percentile_rank(actual_lag, possible_lags)\n",
    "            # assert not np.isnan(percent_rank)\n",
    "            self.actual_ranks[repetition_index].append(percent_rank)\n",
    "\n",
    "    def tabulate_over_transitions(\n",
    "        self, trials: np.ndarray, presentations: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"Tabulate actual and possible lag transitions over a set of trials\"\n",
    "\n",
    "        terminus = np.sum(trials != 0, axis=1)\n",
    "\n",
    "        for trial_index in range(len(trials)):\n",
    "            presentation = presentations[trial_index]\n",
    "            item_count = np.max(presentation)\n",
    "            possible_items = np.arange(item_count) + 1\n",
    "            item_study_positions = njit_apply_along_axis(\n",
    "                item_to_study_positions, possible_items, presentation\n",
    "            )\n",
    "            previous_item = 0\n",
    "\n",
    "            for recall_index in range(terminus[trial_index]):\n",
    "                current_item = presentation[trials[trial_index, recall_index] - 1]\n",
    "                if recall_index > 0 and self.should_tabulate(\n",
    "                    item_study_positions[previous_item - 1],\n",
    "                ):\n",
    "                    self.tabulate_lags(\n",
    "                        previous_item,\n",
    "                        current_item,\n",
    "                        possible_items,\n",
    "                        item_study_positions,\n",
    "                    )\n",
    "\n",
    "                previous_item = current_item\n",
    "                possible_items = possible_items[possible_items != previous_item]\n",
    "\n",
    "        return np.nanmean(self.actual_ranks, axis=1)\n",
    "    \n",
    "def replagrank(\n",
    "    trials: np.ndarray,\n",
    "    presentations: np.ndarray,\n",
    "    list_length: int,\n",
    "    max_repetitions: int = 2,\n",
    "    min_lag: int = 4,\n",
    ") -> np.ndarray:\n",
    "    \"Summarize the tendency to group together nearby items by running a lag rank analysis.\"\n",
    "    lag_rank = RepetitionLagRank(presentations, max_repetitions, min_lag)\n",
    "    scores = lag_rank.tabulate_over_transitions(trials, presentations)\n",
    "    # return scores[0] - scores[1]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"LohnasKahana2014\"\n",
    "data_path = \"data/LohnasKahana2014.h5\"\n",
    "data_query = \"data['list_type'] >= 3\"\n",
    "\n",
    "data = to_numba_typed_dict({key: np.array(value) for key, value in load_data(data_path).items()})\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "\n",
    "max_repetitions = 2\n",
    "min_lag = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81361003, 0.7295366 ],\n",
       "       [0.72933177, 0.62998377],\n",
       "       [0.69780963, 0.68558622],\n",
       "       [0.6556871 , 0.64951138],\n",
       "       [0.64238469, 0.60224452],\n",
       "       [0.72728644, 0.72827337],\n",
       "       [0.57413907, 0.66251383],\n",
       "       [0.59982331, 0.59598916],\n",
       "       [0.74370313, 0.68040249],\n",
       "       [0.69351671, 0.66671052],\n",
       "       [0.85971342, 0.71705638],\n",
       "       [0.63179574, 0.63085404],\n",
       "       [0.61232839, 0.60385234],\n",
       "       [0.78649566, 0.72635669],\n",
       "       [0.66599945, 0.60788552],\n",
       "       [0.76838349, 0.6924549 ],\n",
       "       [0.82239387, 0.75002738],\n",
       "       [0.79654307, 0.72084637],\n",
       "       [0.60051983, 0.62020085],\n",
       "       [0.68834796, 0.6614504 ],\n",
       "       [0.60875966, 0.58214981],\n",
       "       [0.59232637, 0.63117534],\n",
       "       [0.60970931, 0.59833962],\n",
       "       [0.70855475, 0.65347738],\n",
       "       [0.69587753, 0.77728374],\n",
       "       [0.6418735 , 0.61501097],\n",
       "       [0.81724153, 0.7786852 ],\n",
       "       [0.6340587 , 0.64829903],\n",
       "       [0.60041945, 0.60581126],\n",
       "       [0.75633806, 0.72031164],\n",
       "       [0.65403545, 0.60550774],\n",
       "       [0.83477521, 0.75605316],\n",
       "       [0.65839664, 0.65519922],\n",
       "       [0.582935  , 0.59736899],\n",
       "       [0.65533257, 0.6113879 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(apply_by_subject(\n",
    "    data,\n",
    "    trial_mask,\n",
    "    replagrank,\n",
    "    max_repetitions,\n",
    "    min_lag,\n",
    "))\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:00, 514.28it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaxcmr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepetition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dataset\n\u001b[1;32m      3\u001b[0m ctrl_data \u001b[38;5;241m=\u001b[39m control_dataset(to_numba_typed_dict({key: np\u001b[38;5;241m.\u001b[39marray(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_type\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] == 4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_type\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ctrl_subject_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mapply_by_subject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctrl_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecalls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplagrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m ctrl_subject_values\n",
      "File \u001b[0;32m~/compmempy/compmempy/helpers/handling_data.py:213\u001b[0m, in \u001b[0;36mapply_by_subject\u001b[0;34m(data, trial_mask, func, *args)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(subject_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 213\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubject_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresentations\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubject_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mreplagrank\u001b[0;34m(trials, presentations, list_length, max_repetitions, min_lag)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the tendency to group together nearby items by running a lag rank analysis.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m lag_rank \u001b[38;5;241m=\u001b[39m RepetitionLagRank(presentations, max_repetitions, min_lag)\n\u001b[0;32m--> 103\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mlag_rank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabulate_over_transitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresentations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# return scores[0] - scores[1]\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m, in \u001b[0;36mRepetitionLagRank.tabulate_over_transitions\u001b[0;34m(self, trials, presentations)\u001b[0m\n\u001b[1;32m     78\u001b[0m current_item \u001b[38;5;241m=\u001b[39m presentation[trials[trial_index, recall_index] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recall_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_tabulate(\n\u001b[1;32m     80\u001b[0m     item_study_positions[previous_item \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     81\u001b[0m ):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabulate_lags\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprevious_item\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_item\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpossible_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_study_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m previous_item \u001b[38;5;241m=\u001b[39m current_item\n\u001b[1;32m     90\u001b[0m possible_items \u001b[38;5;241m=\u001b[39m possible_items[possible_items \u001b[38;5;241m!=\u001b[39m previous_item]\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mRepetitionLagRank.tabulate_lags\u001b[0;34m(self, previous_item, current_item, possible_items, item_study_positions)\u001b[0m\n\u001b[1;32m     54\u001b[0m         possible_lag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mminimum(lag, possible_lag)\n\u001b[1;32m     55\u001b[0m     possible_lags\u001b[38;5;241m.\u001b[39mappend(possible_lag)\n\u001b[0;32m---> 57\u001b[0m percent_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mpercentile_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_lags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# assert not np.isnan(percent_rank)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactual_ranks[repetition_index]\u001b[38;5;241m.\u001b[39mappend(percent_rank)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jaxcmr/lib/python3.10/site-packages/psifr/stats.py:43\u001b[0m, in \u001b[0;36mpercentile_rank\u001b[0;34m(actual, possible)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     42\u001b[0m rank \u001b[38;5;241m=\u001b[39m (actual_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (possible_count \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrank\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "from jaxcmr_research.helpers.repetition import control_dataset\n",
    "\n",
    "ctrl_data = control_dataset(to_numba_typed_dict({key: np.array(value) for key, value in data.items()}), \"data['list_type'] == 4\", \"data['list_type'] == 1\", 100)\n",
    "\n",
    "ctrl_subject_values = np.array(apply_by_subject(\n",
    "    ctrl_data,\n",
    "    np.ones(ctrl_data['recalls'].shape[0], dtype=bool),\n",
    "    replagrank,\n",
    "    max_repetitions,\n",
    "    min_lag,\n",
    "))\n",
    "\n",
    "ctrl_subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 3.4299583538057674\n",
      "One-tailed P-value: 0.000800182195425057\n",
      "There is a statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.6902984700510105\n",
      "Mean Second Sample: 0.6627942211503018\n",
      "Standard Error First Sample: 0.01371007703540283\n",
      "Standard Error Second Sample: 0.00962267158588462\n",
      "Mean Difference: 0.027504248900708624\n",
      "Standard Error Difference: 0.00790344397299532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Ensure the data has exactly two columns\n",
    "if data.shape[1] != 2:\n",
    "    raise ValueError(\"Data must have exactly two columns\")\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 1]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2, alternative=\"greater\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"One-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of CMR Fitted to Lohnas Kahana 2014 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| | | Cond34LohnasKahana2014 BaseCMR Model Fitting |\n",
       "|---|---|---|\n",
       "| fitness | mean | 817.31 +/- 70.82 |\n",
       "| | std | 203.20 |\n",
       "| recall drift rate | mean | 0.91 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| encoding drift rate | mean | 0.71 +/- 0.05 |\n",
       "| | std | 0.15 |\n",
       "| stop probability growth | mean | 0.23 +/- 0.03 |\n",
       "| | std | 0.10 |\n",
       "| learning rate | mean | 0.47 +/- 0.08 |\n",
       "| | std | 0.23 |\n",
       "| item support | mean | 14.62 +/- 7.60 |\n",
       "| | std | 21.82 |\n",
       "| semantic scale | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| mfc trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| stop probability scale | mean | 0.02 +/- 0.01 |\n",
       "| | std | 0.02 |\n",
       "| shared support | mean | 8.95 +/- 5.74 |\n",
       "| | std | 16.47 |\n",
       "| semantic choice sensitivity | mean | 0.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| primacy decay | mean | 20.74 +/- 10.79 |\n",
       "| | std | 30.97 |\n",
       "| primacy scale | mean | 11.76 +/- 8.89 |\n",
       "| | std | 25.52 |\n",
       "| start drift rate | mean | 0.61 +/- 0.11 |\n",
       "| | std | 0.32 |\n",
       "| choice sensitivity | mean | 29.08 +/- 12.52 |\n",
       "| | std | 35.92 |\n",
       "| mfc choice sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n",
       "| mcf trace sensitivity | mean | 1.00 +/- 0.00 |\n",
       "| | std | 0.00 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jaxcmr_research.helpers.hdf5 import simulate_h5_from_h5\n",
    "from jax import random\n",
    "from jaxcmr_research.helpers.hdf5 import generate_trial_mask, load_data\n",
    "from jaxcmr_research.helpers.misc import summarize_parameters, import_from_string\n",
    "import numpy as np\n",
    "from jaxcmr_research.helpers.array import compute_similarity_matrix\n",
    "from jax import numpy as jnp\n",
    "import json\n",
    "from IPython.display import Markdown  # type: ignore\n",
    "\n",
    "data_name = \"Cond34LohnasKahana2014\"\n",
    "data_path = \"data/LohnasKahana2014.h5\"\n",
    "data_query = \"data['list_type'] >= 3\"\n",
    "connection_path = \"data/peers-all-mpnet-base-v2.npy\"\n",
    "experiment_count = 1\n",
    "seed = 0\n",
    "fit_result_path = (\n",
    "    \"notebooks/Model_Fitting/Cond34LohnasKahana2014_BaseCMR_Model_Fitting.json\"\n",
    ")\n",
    "\n",
    "\n",
    "data = load_data(data_path)\n",
    "trial_mask = generate_trial_mask(data, data_query)\n",
    "embeddings = np.load(connection_path)\n",
    "connections = compute_similarity_matrix(embeddings) # unused here\n",
    "model_factory_path = \"jaxcmr_research.cmr.BaseCMRFactory\"\n",
    "model_factory = import_from_string(model_factory_path)\n",
    "with open(fit_result_path, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    if \"subject\" not in results[\"fits\"]:\n",
    "        results[\"fits\"][\"subject\"] = results[\"subject\"]\n",
    "\n",
    "\n",
    "Markdown(\n",
    "    summarize_parameters([results], None, include_std=True, include_ci=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(seed)\n",
    "rng, rng_iter = random.split(rng)\n",
    "sim = simulate_h5_from_h5(\n",
    "    model_factory=model_factory,\n",
    "    dataset=data,\n",
    "    connections=connections,\n",
    "    parameters={key: jnp.array(val) for key, val in results[\"fits\"].items()},\n",
    "    trial_mask=trial_mask,\n",
    "    experiment_count=experiment_count,\n",
    "    rng=rng_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77692096, 0.74743774],\n",
       "       [0.70638134, 0.67180049],\n",
       "       [0.75945242, 0.73465871],\n",
       "       [0.65707941, 0.64447968],\n",
       "       [0.67295288, 0.64746986],\n",
       "       [0.72448643, 0.74264282],\n",
       "       [0.75290972, 0.71364864],\n",
       "       [0.66671027, 0.69113424],\n",
       "       [0.67982723, 0.70215156],\n",
       "       [0.62979111, 0.60622805],\n",
       "       [0.78261856, 0.79155743],\n",
       "       [0.69979227, 0.6532797 ],\n",
       "       [0.61556494, 0.59404468],\n",
       "       [0.7843276 , 0.75754388],\n",
       "       [0.66870814, 0.66289579],\n",
       "       [0.72253204, 0.73960228],\n",
       "       [0.76171796, 0.7447928 ],\n",
       "       [0.75842882, 0.72868288],\n",
       "       [0.59033125, 0.63275705],\n",
       "       [0.65047042, 0.6351033 ],\n",
       "       [0.61912167, 0.610592  ],\n",
       "       [0.67443116, 0.68603807],\n",
       "       [0.48866639, 0.44945337],\n",
       "       [0.68564902, 0.69379116],\n",
       "       [0.75800599, 0.69960205],\n",
       "       [0.60612067, 0.60563539],\n",
       "       [0.84268733, 0.75617773],\n",
       "       [0.59443468, 0.69628965],\n",
       "       [0.67282773, 0.65928771],\n",
       "       [0.76414394, 0.79114156],\n",
       "       [0.70979413, 0.64219779],\n",
       "       [0.72200604, 0.74074654],\n",
       "       [0.72632703, 0.74693199],\n",
       "       [0.59961239, 0.5677514 ],\n",
       "       [0.61064621, 0.66469644]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_values = np.array(apply_by_subject(\n",
    "    sim,\n",
    "    generate_trial_mask(sim, data_query),\n",
    "    replagrank,\n",
    "    max_repetitions,\n",
    "    min_lag,\n",
    "))\n",
    "\n",
    "subject_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 1.3167208876141099\n",
      "Two-tailed P-value: 0.09836951011353126\n",
      "There is no statistically significant difference between the two paired samples at the 5% significance level.\n",
      "Mean First Sample: 0.6895850904892817\n",
      "Mean Second Sample: 0.6814926971478833\n",
      "Standard Error First Sample: 0.01219909593171219\n",
      "Standard Error Second Sample: 0.011695876197063684\n",
      "Mean Difference: 0.008092393341398618\n",
      "Standard Error Difference: 0.006057434063202858\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example data in an Nx2 array\n",
    "data = subject_values\n",
    "\n",
    "# Ensure the data has exactly two columns\n",
    "if data.shape[1] != 2:\n",
    "    raise ValueError(\"Data must have exactly two columns\")\n",
    "\n",
    "# Split the data into two related samples\n",
    "data1, data2 = data[:, 0], data[:, 1]\n",
    "\n",
    "# Perform the paired t-test\n",
    "t_statistic, p_value = ttest_rel(data1, data2, alternative=\"greater\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"Two-tailed P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the two paired samples at the 5% significance level.\")\n",
    "\n",
    "print(f\"Mean First Sample: {np.mean(data1)}\")\n",
    "print(f\"Mean Second Sample: {np.mean(data2)}\")\n",
    "print(f\"Standard Error First Sample: {np.std(data1) / np.sqrt(len(data1))}\")\n",
    "print(f\"Standard Error Second Sample: {np.std(data2) / np.sqrt(len(data2))}\")\n",
    "print(f\"Mean Difference: {np.mean(data1 - data2)}\")\n",
    "print(f\"Standard Error Difference: {np.std(data1 - data2) / np.sqrt(len(data1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxcmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
